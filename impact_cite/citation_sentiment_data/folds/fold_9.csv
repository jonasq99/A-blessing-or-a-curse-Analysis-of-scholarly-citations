citation,label
"~F ~c ~R ~cR (2) ~\]~) continue explanations, we begin by mentioning the 'Xtrgct' tool by Smadja (Smadja, 1993)",0
"The focus of much of the automatic sentiment analysis research is on identifying the affect bearing words (words with emotional content) and on measurement approaches for sentiment (Turney & Littman, 2003; Pang & Lee, 2004; Wilson et al. , 2005)",0
"The senses are: 1 material from cellulose 2 report 3 publication 4 medium for writing 5 scientific 6 publishing firm 7 physical object inventory is suitable for which application, other than cross-lingual applications where the inventory can be determined from parallel data (Carpuat and Wu, 2007; Chan et al., 2007)",0
"Its size is compatible to (Turney and Littman, 2002)",0
"(2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10way jackknifing to generate tags for the training set",0
"Data and Parameters To facilitate comparison with previous work, we trained our models on sections 2-21 of the WSJ section of the Penn tree-bank (Marcus et al. , 1993)",0
"(2006) and Jansche (2005), who discuss maximum expected F-score training of decision trees and logistic regression models",0
"The statistical approach involves the following: alignment of bilingual texts at the sentence level nsing statistical techniques (e.g. Brown, Lai and Mercer (1991), Gale and Church (1993), Chen (1993), and Kay and RSscheisen (1993)), statistical machine translation models (e.g. Brown, Cooke, Pietra, Pietra et al",0
"We implement this algorithm using the perceptron framework, as it can be easily modified for structured prediction while preserving convergence guarantees (Daume III and Marcu, 2005; Snyder and Barzilay, 2007)",0
"Most of the annotation approaches tackling these issues, however, are aimed at performing classifications at either the document level (Pang et al. , 2002; Turney, 2002), or the sentence or word level (Wiebe et al. , 2004; Yu and Hatzivassiloglou, 2003)",0
"Accuracy on sentiment classification in other domains exceeds 80% (Turney, 2002)",0
"Thus, Collins (2002a) also proposed an averaged perceptron, where the nal weight vector is 1Collins(2002a)alsoprovidedproofthatguaranteedgood learning for the non-separable case",0
"Our use of Gibbs sampling follows from its increasing use in Bayesian inference problems in NLP (Finkel et al., 2006; Johnson et al., 2007b)",0
"We wish to minimize this error function, so we select  accordingly: argmin  summationdisplay a E(a)(a, (argmax a p(a, f|e))) (4) Maximizing performance for all of the weights at once is not computationally tractable, but (Och, 2003) has described an efficient one-dimensional search for a similar problem",1
"Much previous work on unsupervised grammar induction has used gold-standard partof-speech tags (Smith and Eisner, 2006b; Klein and Manning, 2004; Klein and Manning, 2002)",0
"We augment Collins head-driven model 2 (Collins, 1997) to incorporate a semantic label on each internal node",0
"For a detailed introduction to IBM translation models, please see (Brown et al. , 1993)",0
"Results in terms of word-error-rate (WER) and BLEU score (Papineni et al. , 2002) are reported in Table 4 for those sentences that contain at least one unknown word",0
"(2004; 2005), but its performance was worse than our centroid baseline",0
"Our graphical representation has two advantages over previous work (Ding et al., 2008): unifying sentence relations and incorporating question interactions",1
"This situation is very similar to that involved in training HMM text taggers, where joint probabilities are computed that a particular word corresponds to a particular part-ofspeech, and the rest of the words in the sentence are also generated (e.g. \[Cutting et al. , 1992\])",0
"In this paper we report case-insensitive Bleu scores (Papineni et al., 2002), unless otherwise stated, calculated with the NIST tool, and caseinsensitive Meteor-ranking scores, without WordNet (Agarwal and Lavie, 2008)",0
"For this reason, paraphrase poses a great challenge for many Natural Language Processing (NLP) tasks, just as ambiguity does, notably in text summarization and NL generation (Barzilay and Lee, 2003; Pang et al. , 2003)",0
"Consequently, semi-supervised learning, which combines both labeled and unlabeled data, has been applied to some NLP tasks such as word sense disambiguation (Yarowsky, 1995; Pham et al. , 2005), classification (Blum and Mitchell, 1998; Thorsten, 1999), clustering (Basu et al. , 2004), named entity classification (Collins and Singer, 1999), and parsing (Sarkar, 2001)",0
"V B N P  J J R ( a ) ( b ) V 2 V 1 V 2 ' V 1 ' V P V B N P w ill b e J J R  Figure 1: Two different binarizations (a) and (b) of the same SCFG rule distinguished by the solid lines and dashed lines                         ( W e  h o p e  t h e  s i t u a t i o n  w i l l  b e  b e t t e r  . )           N P        J J R     d e c o d i n g m a t c h  8 7 4  r u l e s m a t c h  6 2  r u l e s c o m p e t i n g  e d g e s :  8 0 1 c o m p e t i n g  e d g e s :  5 7 Figure 2: Edge competitions caused by different binarizations  The edge competition problem for SMT decoding is not addressed in previous work (Zhang et al., 2006; Huang, 2007) in which each SCFG rule is binarized in a fixed way",1
"A first family of libraries was based on a word alignment A, produced using the Refined method described in (Och and Ney, 2003) (combination of two IBM-Viterbi alignments): we call these the A libraries",0
"In this paper, we employed the Chinese word segmentation tool (Wu et al. , 2006) that achieved about 0.93-0.96 recall/precision rates in the SIGHAN-3 word segmentation task (Levow, 2006)",0
"Movie-review dataset consists of positive and negative reviews from the Internet Movie Database (IMDb) archive (Pang et al., 2002)",0
"As a learning algorithm for our classification model, we used Maximum Entropy (Berger et al. , 1996)",0
"There are only a few successful studies, such as (Ando and Zhang, 2005) for chunking and (McClosky et al., 2006a; McClosky et al., 2006b) on constituency parsing",1
"138 2 Rule Generation We start with phrase translations on the parallel training data using the techniques and implementation described in (Koehn et al. , 2003a)",0
"Large treebanks are available for major languages, however these are often based on a speci c text type or genre, e.g. nancial newspaper text (the Penn-II Treebank (Marcus et al. , 1993))",0
"1 Introduction Conditional Maximum Entropy (CME) modeling has received a great amount of attention within natural language processing community for the past decade (e.g. , Berger et al. , 1996; Reynar and Ratnaparkhi, 1997; Koeling, 2000; Malouf, 2002; Zhou et al. , 2003; Riezler and Vasserman, 2004)",0
"They provide pairs of phrases that are used to construct a large set of potential translations for each input sentence, along with feature values associated with each phrase pair that are used to select the best translation from this set.1 The most widely used method for building phrase translation tables (Koehn et al. , 2003) selects, from a word alignment of a parallel bilingual training corpus, all pairs of phrases (up to a given length) that are consistent with the alignment",1
"Collins (1997)s parser and its reimplementation and extension by Bikel (2002) have by now been applied to a variety of languages: English (Collins, 1999), Czech (Collins et al. , 1999), German (Dubey and Keller, 2003), Spanish (Cowan and Collins, 2005), French (Arun and Keller, 2005), Chinese (Bikel, 2002) and, according to Dan Bikels web page, Arabic",0
"3 MaxEnt Model and Features 3.1 MaxEnt Model for NOR The principle of maximum entropy (MaxEnt) model is that given a collection of facts, choose a model consistent with all the facts, but otherwise as uniform as possible (Berger et al., 1996)",0
"In prior research, ILP was used as a postprocessing step to remove redundancy and make other global decisions about parameters (McDonald, 2007; Marciniak and Strube, 2005; Clarke and Lapata, 2007)",0
"Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005)",1
"5 Discussion and Future Work The work in this paper substantially differs from previous work in SMT based on the noisy channel approach presented in (Brown et al. , 1993)",0
"2.2.2 ENGLISH TRAINING DATA For training in the English experiments, we used WSJ (Marcus et al. , 1993)",0
DeRose 1988; Cutting et al 1992; Merialdo 1994,0
"Brown, (Brown et al. , 1992) uses the same bigrams and by means of a greedy algorithm forms the hierarchical clusters of words",0
"For this purpose, we adopt the view of the ITG constraints as a bilingual grammar as, e.g., in (Wu, 1997)",0
"Semi-supervised conditional random fields (CRFs) based on a minimum entropy regularizer (SS-CRF-MER) have been proposed in (Jiao et al. , 2006)",0
"Compared with clean parallel corpora such as """"Hansard"""" (Brown et al. 1993), which consists of 505 French-English translations of political debates in the Canadian parliament, texts from the web are far more diverse and noisy",1
",.~.eqmvalent ot duty in a parallel French text, the correct sense of the Enghsh word is identified These studies exploit th~s lnformatmn m order to gather co-occurrence data for the different senses, which ts then used to dtsamb~guate new texts In related work, Dywk (1998) used patterns of translational relatmns in an EnghshNorwegian paralle ! corpus (ENPC, Oslo Umverslty) to define semantic propemes such as synonymy, ambtgmty, vagueness, and semantic helds and suggested a derivation otsemantic representations for signs (eg, lexemes), captunng semantm relatmnshlps such as hyponymy etc, fiom such translatmnal relatmns Recently, Resnlk and Yarowsky (1997) suggested that fol the purposes ot WSD, the different senses of a wo~d could be detelmlned by considering only sense d~stmctmns that are lextcahzed cross-hngmstlcally In particular, they propose that some set of target languages be ~dent~fied, and that the sense d~stmctmns to be considered for language processing appllcatmns and evaluatmn be restricted to those that are reahzed lexlcally in some minimum subset of those languages This idea would seem to p~ovtde an answer, at least m part, to the problem of determining different senses of a word mtumvely, one assumes that ff another language lexlcahzes a word m two or more ways, there must be a conceptual monvatmn If we look at enough languages, we would be likely to fred the s~gmficant lexlcal differences that dehmtt different senses of a word However, th~s suggestmn raises several questions Fo~ instance, ~t ~s well known that many amb~gumes are preserved across languages (for example, the French tntdrYt and the Enghsh interest), especmlly languages that are relatively closely related Assuming this problem can be overcome, should differences found m closely related languages be given lesser (or greater) weight than those found m more distantly related languages 9 More generally, which languages should be considered for this exermse 9 All languages 9 Closely related languages9 Languages from different language famlhes '~ A mixture of the two 9 How many languages, and of which types, would be """"enough"""" to provide adequate lnfotmanon tot this purpose~ There ts also the questmn ot the crlterm that would be used to estabhsh that a sense distinction is """"lexlcahzed cross-hngu~stmally"""" How consistent must the d~stlnCtlOn be 9 Does it mean that two concepts are expressed by mutually non-lntetchangeable lexmal items in some slgmficant number ot other languages, or need tt only be the case that the option ot a different lexlcahzatlon exists m a certain percentage of cases 9 Another conslderatmn ts where the cross-hngual mformatlon to answer these questmns would come from Using bdmgual dictionaries would be extremely tedmus and error-prone, g~ven the substantial d~vergence among d~ctlonanes in terms of the kinds and degree of sense dlstmctmns they make Resmk and Yalowsky (1997) suggest EutoWordNet (Vossen, 1998) as a possible somce of mformatmn, but, given that EuroWordNet ts pttmatdy a lexmon and not a corpus, ~t is subject to many of the same objections as for bl-hngual dictionaries An alternative would be to gather the reformation from parallel, ahgned corpma Unlike bilingual and muttt-hngual dictionaries, translatmn eqmvalents xn parallel texts a~e determined by experienced translatols, who evaluate each instance ot a word's use m context rather than as a part of the meta-hngmst~c actlvlty of classifying senses for mclusmn in a dictionary However, at present very few parallel ahgned corpora exist The vast majority ot these are bl-texts, mvolwng only two languages, one of which is very often English Ideally, a serious 53 evaluation of Resnik and Yarowsky's proposal would include parallel texts m languages from several different language families, and, to maximally ensure that the word m question is used in the exact same sense across languages, ~t would be preferable that the same text were used over all languages in the study The only currently avadable parallel corpora for more than two languages are Olwell's Nmeteen Eighty-Four (Erjavec and Ide, 1998), Plato's Repubhc (Erjavec, et al, 1998), the MULTEXT Journal .o/ the Commt.~ston corpus (Ide and V6roms, 1994), and the Bible (Resnlk, et al, m press) It is likely that these corpora do not provide enough appropriate data to reliably determine sense distinctions Also, ~t Is not clear how the lexlcahzatlon of sense distractions across languages Is affected by genre, domain, style, etc Thls paper attempts to provide some prehmlnary answers to the questions outhned above, In order to eventually determine the degree to which the use of parallel data ts vmble to determine sense distinctions, and, ff so, the ways in which th~s reformation might be used Given the lack of lalge parallel texts across multiple languages, the study is necessarily hmlted, however, close exammanon of a small sample of parallel data can, as a first step, provide the basis and dlrectmn for more extensive studies 1 Methodology I have conducted a small study using parallel, aligned versmns ot George Orwell's Nineteen Etghtv-Fo,lr (Euavec and Ide, 1998)m five languages Enghsh, Slovene, Estonian, Romanlan, and Czech I The study therefole Involves languages from four language families The O~well parallel corpus also includes vers|ons o) Ntneteen-E~gho Four m Hungarian, Bulgarmn, Latwan, Llthuaman, Se~bmn, and Russmn (Germanic, Slavic, Fmno-Ugrec, and Romance), two languages from the same family (Czech and Slovene), as well as one non-Indo-European language (Estoman) Nmeteen Eighty-Four Is a text of about 100,000 words, translated directly from the original English m each of the other languages The parallel versions of the text are sentence-aligned to the English and tagged for part of speech Although Nineteen Eighty-Four is a work of fiction, Orwell's prose IS not highly stylized and, as such, it provides a reasonable sample ot modern, ordinary language that ~s not tied to a given topic or sub-domain (such as newspapers, technical reports, etc ) Furthermore, the translations of the text seem to be relatively faithful to the original for instance, over 95% ot the sentence alignments in the full pmallel corpus of seven languages are one-to-one (Prlest-Dorman, et al, 1997) Nine ambiguous English words were considered hard, head, country, hne, promise, shght, seize, scrap, float The first four were chosen because they have been used in other dlsamb~guatlon studies, the latter five were chosen from among the words used m the Senseval dlsamblguatlon exercise (Kllgamff and Palmer, forthcoming) In all cases, the study was necessarily hmlted to words that occurred frequently enough in the Orwell text to warrant consideration F~ve hundred forty-two sentences conta|nmg an occurrence or occurrences (Including morphological variants) of each of the nine words were extracted from the Enghsh text, together w~th the parallel sentences m which they occur m the texts ot the four comparison languages (Czech, Estonian, Romantan, Slovene) As Walks and Stevenson (1998) have pointed out, pa~t-of-speech tagging accomplishes a good portion of the work ot semantic dlsamb~guatmn, therefore occmrences of wolds that appemed in the data in more than 54 one part of speech were grouped separately 2 The Enghsh occurrences were then grouped usmg the sense distinctions m WordNet, (version 1 6) \[Miller et al, 1990, Fellbaum, 1998\]) The sense categonzatmn was performed by the author and two student assistants, results from the three were compared and a final, mutually agreeable set of sense assignments was estabhshed For each of the four comparison languages, the corpus of sense-grouped parallel sentences were sent to a llngmst and natl,ve speaker of the comparison language The hngmsts were asked to provide the lexlcal item m each parallel sentence that corresponds to the ambiguous Enghsh word If inflected, they were asked to provide both the inflected form and the root form In addttmn, the lmgmsts were asked to indicate the type of translatmn, according to the dtstmctmns given m Table 1 For over 85% of the Enghsh word occurrences (corresponding to types 1 and 2 m Table 1), a specific lexlcal item or items could be identified as the translation equivalent for the corresponding Enghsh word For comparison purposes, each translanon equivalent was represented by ~ts lemma (or the lemma of the toot form in the case of derivatives) and associated w~th the WordNet sense to which it corresponds In order to determine the degree to which the assigned sense dlstlncttons correspond to translation eqmvalents, a coherence index ( Cl) was computed that measures how often each pmr of senses is translated usmg the same word as well as the consistency with which a g~ven se,ls,z ~s translated with the same word ~ Note that the z The adJective and adverb senses of hard are consadeied together because the distinction is not consistent across the translations used m the study Note that the CI ~s similar to semanuc entropy (Melamed, 1997) However, Melamed computes CIs do not determine whether or not a sense dtstmctton can be lextcahzed in the target language, but only the degree to whmh they are lexicahzed differently m the translated text However, tt can be assumed that the CIs provide a measure of the tendency to lex~cahze different WordNet senses differently, which can m turn be seen as an mdtcatmn of the degree to which the distraction ts vahd For each ambiguous word, the CI Is computed for each pair of senses, as follows S<q t> Cl(sqS, ) = '=1 m rnrt where @ n ~s the number of comparison languages under consideration, nl~q and m,, are the nt~mber of occurrences olsense sqand sense s~ m the Enghsh corpus, respectively, including occurrences that have no idenufiable translation, s<~ ~>m ts the number of times that senses q and r are translated by the same lex~cal Item m language t, i e, x=y t ~tJan ~( q ), r~oan~( r ) The CI ts a value between 0 and 1, computed by examining clusters of occurrences translated by the same word In the othel languages If sense and sense ) are consistently translated w~th the same wo~d in each comparison language, then Cl(s, s~) = 1, if they are translated with a different word m every occurrence, Cl(s, ~) = 0 In general, the CI for pans of different senses provides an index of thmr relatedness, t e, the greater the value of Cl(s, sj), the more frequently occurrences of-sense t and sense j are translated with the same lextcal item When t = j, we entropy tOl wold types, lather than word senses 55 obtain a measure of the coherence of a ~lven sense Type Meaning 1 A slngle lexlcal Item is used to translate the En@izsh equivalent (possibly a 2 The English word is translated by a phrase of two or more words or a compound, meaning as the slngle English word 3 The En@izsh word is not lexzcalized in the translation 4 A pronoun is substituted for the English word In the translation An English phrase contalnmng the ambiguous word Is translated by a single language which has a broader or more specific meanlng, or by a phrase in whl corresponding to the English word Is not explicltl~ lexlcallzed Table 1 Translation types and their trequencles % dizen whl%h h 6% 6% 6% of s p same Word # Description hard 1 1 difficult 2 head i i i 1 Table 2 1 2 _meta~horlcally hard _\] 3 not yielding to pressure, 1 4 very strong or ~lgorous, ar 2 I wlth force or vigor (adv) 3 earnestly, intently (adv) i_ ~art of the body  3 intellect 4 _r~le_!r, ch,%ef 7 front, front part WoldNet senses ot hard and head CIs were also computed for each language individually as well as for different language groupings Romaman, Czech, and Estonian (three different language families) Czech and Slovene (same family), Romaman, Czech, Slovene (Indo-European, and Estonian (nonIndo-European) To better visualize the relationship between senses, a hierarchical clustering algorithm was applied to the CI data to generate trees reflecting sense proximity 4 Finally, in order to determine the degree to which the linguistic relaUon between languages may affect coherence, a correlation was run among CIs for all pairs of the four target languages Fol example, Table 2 gives the senses of hard and head that occurred in the data s The CI data .s 'sobS' hard and head are given in Tables 3 and 4 ~uous CIs measuring the aff, mty of a sense with itself--that is, the tendency for all occurrences of that sense to be translated wlth the same word--show that all of the s,x senses of ha,d have greatel internal consistency tfian athmty with other senses, with senses 1 1 (""""dlff|cult"""" CI = 56) and 13 (,'not soft,, ci = 63) registenng the h,ghest internal consistency 6 The same holds true for three of the four senses of head, while the CI for senses 1 3 (""""Intellect"""") and 1 1 (""""part of the body"""") is higher than the CI for 1 3/1 3 WordNet Sense 2 1 2 3 1 4 1 3 1 1 1 2 21 23 1 4 13 0 50 o 13 i ool 0 O0 0 25 i O0 0 04 0 50 0 17 0 56 0 19 0 00 0 00 0 00 0 00 0 00 0 25 0 21 Table 3 CIs for hard I i 12 0,,63 0 00 0 50 2 Results Although the data sample is small, It gives some insight into ways m which a larger sample might contribute to sense discrimination 4 Developed by Andleas Stolcke Results tor all words m the study are avadable at http//www cs vassar edu/~~de/wsd/cross-hng html 6 Senses 2 3 and 1 4 have CIs ot 1 because each ot these senses exists m a single occurrence m the corpus, and have theretote been dlscarded horn consideration ot CIs to~ individual senses We a~e currently mvesugatmg the use oI the Kappa staUst~c (Carletta, 1996) to normahze these sparse data 56 WordNet Sense 1 1 1 3 1 4 1 7 1 1 0 69 1 3 0 53 0 45 1 4 0 12 0 07, 0 50 1 7 0 40 0 001 0 00 1 00 Table 4 CIs for head Figure 2 shows the sense clusters for hard generated from the CI data 7 The senses fall into two mare clusters, w~th the two most internally consistent senses (1 1 and 1 3) at the deepest level of each ot the respecuve groups The two adverbml forms 8 are placed in separate groups, leflectmg thmr semantic proximity to the different adjecuval meanings of hard The clusters for head (Figure 2) stmdarly show two dlstmct groupings, each anchored in the two senses with the h~ghest internal consistency and the lowest mutual CI (""""part of the body"""" (1 1) and """"ruler, chief"""" (1 4)) The h~erarchtes apparent m the cluster graphs make intuitive sense Structured hke dictmnary enmes, the clusters for hard and head might appeal as m F~gure 1 This ts not dissimilar to actual dlctLonary entries for hard and head, for example, the enmes for hard in four differently constructed dlctmnanes ( Colhns Enghsh (CED), Longman's (LDOCE), OxJotd Advanced Learner's (OALD), and COBUILD) all hst the """"'d~fficult"""" and """"not soft"""" senses first and second, whmh, since most dictionaries hst the most common Ol frequently used senses hrst, reflects the gross dlwslon apparent m the clusters Beyond this, ~t ~s difficult to assess the 7 Foi the purposes ot the cluster analys~s, CIs of l 00 resulting from a single occurrrence were normahzed to 5 8 Because ~oot to, ms were used m the analysis, no dzstlncUon m UanslaUon eqmvalents was made tor part ot speech correspondence between the senses In the dictionary entries and the clusters The remamlng WordNet senses are scattered at various places within the entries or, m some cases, split across various senses The h~erarchlcal relatmns apparent m the clusters are not reflected m the d~cttonary enmes, smce the senses are for the most part presented in flat, hnear hsts However, It is interesting to note that the first five senses of hard In the COBUILD d~cuonary, which is the only d~cttonary in the group constructed on the bas~s of colpus examples 9 and presents senses m ruder of frequency, correspond to hve of the six WordNet senses in thls study WordNet's """"metaphorically hard"""" is spread over multiple senses in the COB UILD, as it.is In the other d~ctlonarles HARD HEAD I 1 dlfflcult 2 vlgorously II 1 a not soft b strong 2 a earnestly b metaphorlcally hard I 1 a part of the body b zntellect 2 front, front part II ruler, chlef Flgme 1 Clusteis tol hard and head suuctured as dlcuonary entt ~es The results tor dlftment language groupings show that the tendency to lextcahze senses differently is not aftected by language d~stance (Table 5) In fact, the mean CI fol Estonian, the only non-Indo-European language m the study, ~s lower than that for any other group, mdmatmg that WordNet sense dtstmctmns are slightly less hkely to be lexlcahzed differently m Estonian 9 Edmons ot the LDOCE (1987 vexsmn) and OALD (1985 version) dictlonalles consulted m this study ple-date edmons ol those same d~ctlonanes based on colpus evidence 57 Correlations of CIs for each language pair (Table 5) also show no relationship between the degree to which sense d~stmcuons are lexlcahzed differently and language distance This is contrary to results obtained by Resmk and Yarowsky (subm,tted), who, using a memc slmdar to the one used in this study, found that that non-Indo-European languages tended to lexlcallze English sense d~stmctlons more than Indo-European languages, especially at finergrained levels However, their translation data was generated by native speakers presented with Isolated sentences in English, who were asked to provide the translation for a given word In the sentence It is not clear how this data compares to translations generated by trained translators working with full context Lanquaqe qroup Averaqe CI ALL 0 27 RO/ES/SL 0 28 SL/CS 0 28 RO/SL/CS 0 27 ES 0 26 Table 5 Average CI values Lanqs Hard Country Llne Head Ave ES/CS 0 86 0 72 0 68 0 69 0 74 RO/SL 0 73 0 78 0 68 1 00 0 80 RO/CS 0 83 0 66 0 67 0 72 0 72 SL/CS 0 88 0 51 0 72 0 71 0 71 RO/ES 0 97 0 26 0 70 0 98 0 73 ES/SL 0 73 0 59 0 90 0 99 0 80 Table 6 CI correlauon tor the tour target languages I -I I  I I m~nlmum dlstance = 0 249399 m~nlmum d~stance = 0 434856 mlnlmum dlstance = 0 555158 mlnlmum dlstance = 0 602972 m~nlmum dlstance = 0 761327 I  >21 I  >ii I  >23 l  >13 l  >14 I  >12 (13) (23) (12) (1,4) (ii) (21) (1412) (2313) ( 2 3 1 3 1 4 1 2 ) ( 2 111 ) Figure 2 Cluster tree and distance measures tor the sm senses of hard I  >14 -i I  > i i I--- 1 J  > i 3 I  >17 mlnlmum dlstance = 0 441022 mlnlmum dlstance = 0 619052 mln~mum dlstance = 0 723157 (13) (ll) (17) (1113) (111317) (14) F,gure 3 Cluster tree and dmtance measures tot the tout senses ot head 58 Conclusion The small sample m this study suggests that cross-hngual lexlcahzat~on can be used to define and structure sense d~stmct~ons The cluster graphs above provide mformat~on about relations among WordNet senses that could be used, for example, to determine the granularity of sense differences, whtch m turn could be used in tasks such as machine translatton, mtormaUon retrieval, etc For example, it is hkely that as sense dtstmcttons become finer, the degree of error ~s less severe Resmk and Yarowsky (1997) suggest that confusing freer-grained sense dtstmctlons should be penahzed less severely than confusing grosser d~stmct~ons when evaluatmg the performance of sense dtsambtguatt0n systems The clusters also provide insight into the lexlcallzatlon of sense dtstmcttons related by various semantic relations (metonymy, meronymy, etc ) across languages, for instance, the """"part of the body"""" and """"intellect"""" senses of head are lex~cahzed with the same ~tem a s~gnlficant portion of the t~me across all languages, reformation that could be used m machine translatton In addtt~on, cluster data such as that presented here could be used m lexicography, to determine a mole detaded hierarchy of relations among senses in dtct~onary entries It is less clear how cross-hngual reformation can be used to determine sense d~st~nctlons independent of a pre-deflned set, such as the WordNet senses used here In an effort to explore how thts mlght be done, I have used the small sample from thts study to create word groupmgs from """"back translations"""" (l e, additional translations m the original language ot the translations m the target language) and developed a metric that uses th~s mformatton to determine relatedness between occurrences, whtch ~s m turn used to cluster occurrences into sense groups I have also compared sets of back translations for words representing the various WordNet senses, which provtde word groups s~mdar to WordNet synsets Interestingly, there ts virtually no overlap between the WordNet synsets and word groups generated from back translations The results show, however, that sense dlstmctlons useful for natural language processing tasks such as machme translanon could potentsally be determined, ot at least influenced, by constdeHng this mformatton The automatically generated synsets themselves may also be useful m the same apphcatlons; where WordNet synsets (and ontologtes) have been used tn the past More work needs to be done on the topic of cross-hngual sense determination, utthzmg substantially larger parallel corpora that include a variety ot language types as well as texts fiom several genres This small study explores a possible methodology to apply when such resources become avatlable Acknowledgements The author would hke to gratefully acknowledge the contrtbut~on of those who provided the translatton mfotmat~on Tomaz Eua~ec (Slovene), Kadrt Muxschnek (Estonian), Vladtmlr Petkevtc (Czech), and Dan Tubs (Romanlan), as well as Dana Fleut and Darnel Khne, who helped to transcrtbe and evaluate the data Special thanks to Dan Melamed and Hlnrtch Schutze for their helpful comments 59 \[\] \[\] in \[\] in i i Hg nn i an i am References Ca~letta, Jean (1996) Assessing Agreement on Classthcatton Tasks The Kappa Stat~st~t. Computational Lmgulstlcs, 22(2), 249-254 Dagan, Ido and Ita~, Alon (1994) Wo~d sense dlsambxguat~on using a second language monohngual corpus Computattonal Ltngmsttcs, 20(4), 563-596 Dagan, Ido, Ital, Alon, and Schwall, Ulnke (1991) Two languages a~e more mformattve than one Proceedings of the 29th Annual Meettng of the Assoctatton for Computattonal Ltngutsttcs, 18-21 June 1991, Berkeley, Cahfornm, 130-137 Dyvtk, Helge (1998) Translations as Semantic Mirrors Proceedmgs of Workshop W13 Multzlmguahty in the Lextcon II, The 13th Biennial European Conference on Arttftctal lntelhgence (ECA198), Brighton, UK, 24-44 Eqavec, Tomaz and Ide, Nancy (1998) The MULTEXT-EAST Corpus Proceedlng~ of the Fltst International Conference on Language Resources and Evaluatton, 27-30 May 1998, Granada, 971-74 Erjavec, Tomaz, Lawson, Ann, and Romary, Laurent (1998) East meets West Producing Multflmgual Resources m a European Context Pioceedtngs of the Ftrst Internattonal Conference on Language Resources and Evaluation, 27-30 May 1998, Gtanada, 981-86 Fellbaum, Chttstmne (ed) (1998) WordNet An Electrontc Lexlcal Database MIT Press, Cambridge, Massachusetts Gale, Wdham A, Church, Kenneth W and Yatowsky, Davtd (1993) A method tor dlsamblguatmg word senses m a large cmpus Computers and the Humamtles, 26, 415-439, Hearst, M'attl A (1991) Noun homograph  ' dlsamblguatlon using local:'~.'0ntext m large corpora Proceedtngs of the 7th Annual Conference of the Umver~lt~ of Waterloo Centre for the New OED and Text ReaeaJch, Oxford, Umted Kingdom, 1-19 Ide, Nancy and V61oms, Jean (1998) Word sense d~samb~guat~on The state of the alt Computational Lmgut~ttc~, 24 1, 1-40 Kdgar~ttt, Adam and Palmer, Ma~tha, Eds (forthcoming) Proceedmgs ot the Senseval Word Sense D~samb~guatlon Workshop, Specml double ~ssue otComputer~ and the Humamttes, 33 4-5 Leacock, Claudia, Towell, Geoffrey and Voorhees, Ellen (1993) Corpus-based stattstlcal sense resolution Proceedtng~ of the ARPA Human Language Technology Worsl~shop, San Francisco, Morgan Kautman Melamed, I Dan (1997) Measuring Semantic Entropy ACL-SIGLEX Workshop Taggmg Tert wtth Lextcal Semanttcs Why, What, and How ~ April 4-5, 1997, Washington, D C, 41-46 Mtllet, George A, Beckwlth, Richard T Fellbaum",0
"Minimum Error Rate training (Och, 2003) over BLEU was used to optimize the weights for each of these models over the development test data",0
he translation probability can also be discriminatively trained such as in Tillmann and Zhang (2006,0
"This ITG constraint is characterized by the two forbidden structures shown in Figure 1 (Wu, 1997)",0
"1 Introduction Many different statistical tests have been proposed to measure the strength of word similarity or word association in natural language texts (Dunning, 1993; Church and Hanks, 1990; Dagan et al. , 1999)",0
"4 Relation to Previous Work There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks, including named-entity recognition, gene prediction, shallow parsing and others (Finkel et al. , 2005; Culotta et al. , 2005; Sha and Pereira, 2003)",0
"The parse trees on the English side of the bitexts were generated using a parser (Soricut, 2004) implementing the Collins parsing models (Collins, 1997)",0
"Treebank (Marcus et al., 1993), six of which are errors",0
"Some examples of language reuse include collocation analysis (Smadja, 1993), the use of entire factual sentences extracted from corpora (e.g. , """"'Toy Story' is the Academy Award winning animated film developed by Pixar~'), and summarization using sentence extraction (Paice, 1990; Kupiec et al. , 1995)",0
"In the second pass, 5-gram and 6-gram zero-cutoff stupid-backoff (Brants et al., 2007) language models estimated using 4.7 billion words of English newswire text are used to generate lattices for phrasal segmentation model rescoring",0
"We have chosen the Maximum Entropy tagger (Ratnaparkhi, 1996) for a comparison with our universal tagger, since it achieved (by a small margin) the best overall result on Slovene as reported there (86.360% on all tokens) of taggers available to us (MBT, the best overall, was not freely available to us at the time of writing)",1
"Beyond WordNet (Fellbaum, 1998), a wide range of resources has been developed and utilized, including extensions to WordNet (Moldovan and Rus, 2001; Snow et al., 2006) and resources based on automatic distributional similarity methods (Lin, 1998; Pantel and Lin, 2002)",0
"We then built separate English-to-Spanish and Spanish-to-English directed word alignments using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length 7 using the alignment template approach (Och and Ney, 2004)",0
"8.1 The Averaged Perceptron Algorithm with Separating Plane The averaged perceptron algorithm (Collins, 2002) has previously been applied to various NLP tasks (Collins, 2002; Collins, 2001) for discriminative reranking",0
"There is a large number of potentially informative features that could play a role in correctly predicting the tag of an unknown word (Ratnaparkhi, 1996; Weischedel et al. , 1993; Daelemans et al. , 1996)",0
"2(Daume III and Marcu, 2005) also presents the method using the averaged perceptron (Collins, 2002a) 3For re-ranking problems, Shen and Joshi (2004) proposed a perceptron algorithm that also uses margins",0
"Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do",0
"For our POS tagging experiments, we use 561 MEDLINE sentences (9576 words) from the Penn BioIE project (PennBioIE, 2005), a test set previously used by Blitzer et al.(2006)",0
"Then, we used the refinement technique grow-diag-final-and (Koehn et al., 2003) to all 50  50 bidirectional alignment pairs",0
"BLEU: Automatic evaluation by BLEU score (Papineni et al. , 2002)",0
"The phrases in the translations were located using standard phrase extraction techniques (Koehn et al., 2003)",0
"As discussed in (Titov and Henderson, 2007), undirected graphical models do not seem to be suitable for history-based parsing models",0
"Automatic NE transliteration is an important component in many cross-language applications, such as Cross-Lingual Information Retrieval (CLIR) and Machine Translation(MT) (Hermjakob et al., 2008; Klementiev and Roth, 2006a; Meng et al., 2001; Knight and Graehl, 1998)",0
"3A hypergraph is analogous to a parse forest (Huang and Chiang, 2007)",0
"2.2 Phrase-based Chinese-to-English MT The MT system used in this paper is Moses, a stateof-the-art phrase-based system (Koehn et al., 2003)",1
"Therefore, we also carried out evaluations using the NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), WER (Hunt, 1989), PER (Tillmann et al., 1997) and TER (Snover et al., 2005) machine translation evaluation techniques",0
"Experiments are presented in table 1, using BLEU (Papineni et al., 2001) and METEOR5 (Banerjee and Lavie, 2005), and we also show the length ratio (ratio of hypothesized tokens to reference tokens)",0
"Arguably the most widely used is the mutual information (Hindle, 1990; Church and Hanks, 1990; Dagan et al. , 1995; Luk, 1995; D. Lin, 1998a)",1
"Usually the IBM Model 1, developed in the statistical machine translation field (Brown et al., 1993), is used to construct translation models for retrieval purposes in practice",0
"We show translation results in terms of the automatic BLEU evaluation metric (Papineni et al. , 2002) on the MT03 Arabic-English DARPA evaluation test set consisting of a212a89a212a89a87 sentences with a98a89a212a161a213a89a214a89a215 Arabic words with a95 reference translations",0
"Several non-linear objective functions, such as F-score for text classification (Gao et al. , 2003), and BLEU-score and some other evaluation measures for statistical machine translation (Och, 2003), have been introduced with reference to the framework of MCE criterion training",0
"The second voting model is a maximum entropy model (Jaynes, 1978), since Klein and Manning (2002) found that this model yielded higher accuracy than naive Bayes in a subsequent comparison of WSD performance",0
"In the field of parsing, McDonald and Nivre (2007) compared parsing errors between graphbased and transition-based parsers",0
"Since the use of cluster of machines is not always practical, (Talbot and Osborne, 2007b; Talbot and Osborne, 2007a) showed a randomized data structure called Bloom filter, that can be used to construct space efficient language models 513 for SMT",1
"Empirical evaluations using two standard summarization metricsthe Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004)show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in Pyramid score, and outperforms our best-performing non-sequential model by 3.9%",0
"Either pruning (Stolcke, 1998; Church et al., 2007) or lossy randomizing approaches (Talbot and Brants, 2008) may result in a compact representation for the application run-time",0
"Making such an assumption is reasonable since POS taggers that can achieve accuracy of 96% are readily available to assign POS to unrestricted English sentences (Brill, 1992; Cutting et al. , 1992)",0
"We show that our semi-supervised approach yields improvements for fixed datasets by performing parsing experiments on the Penn Treebank (Marcus et al., 1993) and Prague Dependency Treebank (Hajic, 1998; Hajic et al., 2001) (see Sections 4.1 and 4.3)",0
"The bigram translation probability t2(f|f,e) specifies the likelihood that target word f is to follow f in a phrase generated by source word e. 170 2.1 Properties of the Model and Prior Work The formulation of the WtoP alignment model was motivated by both the HMM word alignment model (Vogel et al. , 1996) and IBM Model-4 with the goal of building on the strengths of each",0
ne of the most notable examples is Yarowskys (1995) bootstrapping algorithm for word sense disambiguatio,1
"2.4 Reordering Reordering features take many forms in MT. In phrase-based systems, reordering is accomplished both within phrase pairs (local reordering) as well as through distance-based distortion models (Koehn et al., 2003) and lexicalized reordering models (Koehn et al., 2007)",0
"The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005)",0
"1 Introduction In this paper, we study the use of so-called word trigger pairs (for short: word triggers) (Bahl et al. , 1984, Lau and Rosenfeld, 1993, Tillmann and Ney, 1996) to improve an existing language model, which is typically a trigram model in combination with a cache component (Ney and Essen, 1994)",0
"This model can be seen as an extension of the standard Maximum Entropy Markov Model (MEMM, see (Ratnaparkhi, 1996)) with an extra dependency on the predicate label, we will henceforth refer to this model as MEMM+pred",0
.2.1 Proxy items There is a potential risk of redundancy if we represent related statistics using the log-frequency BF scheme presented in Talbot and Osborne (2007,0
"1 Introduction Chinese Word Segmentation (CWS) has been witnessed a prominent progress in the last three Bakeoffs (Sproat and Emerson, 2003), (Emerson, 2005), (Levow, 2006)",1
"While Galley (2004) describes extracting treeto-string rules from 1-best trees, Mi and Huang et al",0
"Precision and recall rates were 92.4% on the same data used in (Ramshaw and Marcus, 1995)",0
"A la Ramshaw and Marcus (1995), they represent the words as a sequence of labeled words with IOB annotations, where the B marks a word at the beginning of a chunk, I marks a word inside a chunk, and O marks those words (and punctuation) that are outside chunks",0
"Table 2: Figures about clustering algorithms Algorithm # Sentences/# Clusters S-HAC 6,23 C-HAC 2,17 QT 2,32 EM 4,16 In fact, table 2 shows that most of the clusters have less than 6 sentences which leads to question the results presented by (Barzilay & Lee, 2003) who only keep the clusters that contain more than 10 sentences",1
"255 Meteor (Banerjee and Lavie, 2005), Precision and Recall (Melamed et al. , 2003), and other such automatic metrics may also be affected to a greater or lesser degree because they are all quite rough measures of translation similarity, and have inexact models of allowable variation in translation",0
"An early exception to this was (Collins, 1997) itself, where Model 2 used function tags during the training process for heuristics to identify arguments (e.g. , the TMP tag on the NP in Figure 1 disqualifies the NP-TMP from being treated as an argument)",0
"They have been successfully applied to accent restoration, word"""" sense disambiguation 209 and homograph disambiguation (Yarowsky, 1994; 1995; 1996)",1
"Recently, a number of machine learning approaches have been proposed (Zettlemoyer and Collins, 2005; Mooney, 2007)",0
"Chu-Carroll and Carpenter (1999) describe a method of disambiguation, where disambiguation questions are dynamically constructed on the basis of an analysis of the differences among the closest routing destination vectors",0
"A tree sequence to string rule  174 A tree-sequence to string translation rule in a forest is a triple <L, R, A>, where L is the tree sequence in source language, R is the string containing words and variables in target language, and A is the alignment between the leaf nodes of L and R. This definition is similar to that of (Liu et al. 2007, Zhang et al. 2008a) except our treesequence is defined in forest",0
"Section 4 describes the online training procedure and compares it to the well known perceptron training algorithm (Collins, 2002)",0
"Similar to the work of Barzilay and Lee (2003), who have applied paraphrase generation techniques to comparable corpora consisting of different newspaper articles about the same event, we are currently attempting to solve the data sparseness problem by extending our approach to non-parallel corpora",0
"3 Semantic Representation 3.1 The Need for Dependencies Perhaps the most common representation of text for assessing content is Bag-Of-Words or Bag-of-NGrams (Papineni et al. , 2002)",0
"The other approach is to estimate a single score or likelihood of a translation with rich features, for example, with the maximum entropy (MaxEnt) method as in (Carpuat and Wu, 2007; Ittycheriah and Roukos, 2007; He et al., 2008)",0
"The model scaling factors are optimized with respect to some evaluation criterion (Och, 2003)",0
"However, by exploiting the fact that the underlying scores assigned to competing hypotheses, w(e,h,f), vary linearly w.r.t. changes in the weight vector, w, Och (2003) proposed a strategy for finding the global minimum along any given search direction",0
"For example, aspects of a digital camera could include picture quality, battery life, size, color, value, etc. Finding such aspects is a challenging research problem that has been addressed in a number of ways (Hu and Liu, 2004b; Gamon et al., 2005; Carenini et al., 2005; Zhuang et al., 2006; Branavan et al., 2008; Blair-Goldensohn et al., 2008; Titov and McDonald, 2008b; Titov and McDonald, 2008a)",0
"By associating natural language with concepts as they are entered into a knowledge A Model Of Semantic Analysis All of the following discussion is based on a model of semantic analysis similar to that proposed in (Hobbs, 1985)",0
"We use evaluations similar to those used before (Rapp, 2002; Pado and Lapata, 2007; Baroni et al., 2008, among others)",0
"Following recent research about disambiguation models on linguistic grammars (Abney, 1997; Johnson et al. , 1999; Riezler et al. , 2002; Clark and Curran, 2003; Miyao et al. , 2003; Malouf and van Noord, 2004), we apply a log-linear model or maximum entropy model (Berger et al. , 1996) on HPSG derivations",0
"using the BLEU metric (Papineni et al., 2002)",0
"Several models were introduced for these problems, for example, the Hidden Markov Model (HMM) (Rabiner, 1989), Maximum Entropy Model (ME) (Ratnaparkhi and Adwait, 1996), and Conditional Random Fields (CRFs) (Lafferty et al., 2001)",0
"Considerations of sentence fluency are also key in sentence simplification (Siddharthan, 2003), sentence compression (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2006; McDonald, 2006; Turner and Charniak, 2005; Galley and McKeown, 2007), text re-generation for summarization (Daume III and Marcu, 2004; Barzilay and McKeown, 2005; Wan et al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007)",0
"In what concerns the evaluation process, although ROUGE (Lin, 2004) is the most common evaluation metric for the automatic evaluation of summarization, since our approach might introduce in the summary information that it is not present in the original input source, we found that a human evaluation was more adequate to assess the relevance of that additional information",1
"This tagging scheme is the IOB scheme originally put forward by Ramshaw and Marcus (Ramshaw and Marcus, 1995)",0
"Finally, we plan to apply the model to other paraphrasing tasks including fully abstractive document summarisation (Daume III and Marcu, 2002)",0
"Like (Church and Hanks, 1990), we used mutual information to measure the cohesion between two words",0
"All state-of-the-art wide-coverage parsers relax this assumption in some way, for instance by (i) changing the parser in step (3), such that the application of rules is conditioned on other steps in the derivation process (Collins, 1997; Charniak, 1997), or by (ii) enriching the nonterminal labels in step (1) with context-information (Johnson, 1998; Klein and Manning, 2003), along with suitable backtransforms in step (4)",1
"Our experimental results display that our SDB model achieves a substantial improvement over the baseline and significantly outperforms XP+ according to the BLEU metric (Papineni et al., 2002)",0
"Feature selection methods have been proposed in the maximum-entropy literature by several authors (Ratnaparkhi, Roukos, and Ward 1994; Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997; Papineni, Roukos, and Ward 1997, 1998; McCallum 2003; Zhou et al. 2003; Riezler and Vasserman 2004)",0
"1 Introduction Translations tables in Phrase-based Statistical Machine Translation (SMT) are often built on the basis of Maximum-likelihood Estimation (MLE), being one of the major limitations of this approach that the source sentence context in which phrases occur is completely ignored (Koehn et al. , 2003)",1
"One prominent constraint of the IBM word alignment models (Brown et al., 1993) is functional alignment, that is each target word is mapped onto at most one source word",1
"Equation (2) is rewritten as: )|()|()|( )|()|()|()|( 2211 21 ce colecolcolcolcol rrpcepcep crpcepcepcep = = (3) It is equal to a word translation model if we take the relation type in the collocations as an element like a word, which is similar to Model 1 in (Brown et al. , 1993)",0
"We observe that AER is loosely correlated to BLEU ( = 0.81) though the relation is weak, as observed earlier by Fraser and Marcu (2006a)",0
"??word proximity: For the web searches, Turney (2002) uses the NEAR operator and considers only those documents that contain the adjectives within a specific proximity",0
"By doing so we must emphasize that, as described in the previous section, the BLEU score was not designed to deliver satisfactory results at the sentence level (Papineni et al., 2002), and this also applies to the closely related NIST score",1
"Building heavily on the ideas of History-based parsing (Black et al. , 1993; Nivre, 2006), training the parser means essentially running the parsing algorithms in a learning mode on the data in order to gather training instances for the memory-based learner",0
"Our results agree, at least at the level of morphology, with (Leech and Eyes 1993; Marcus et al. 1993)",0
"Lexical relationships under the standard IBM models (Brown et al. , 1993) do not account for many-to-many mappings, and phrase extraction relies heavily on the accuracy of the IBM word-toword alignment",1
esearchers have mostly looked at representing words by their surrounding words (Lund and Burgess 1996) and by their syntactical contexts (Hindle 1990; Lin 1998,0
"The common types of features include contextual (Lin, 1998), co-occurrence (Yang and Callan, 2008), and syntactic dependency (Pantel and Lin, 2002; Pantel and Ravichandran, 2004)",0
"Given a set of features and a training corpus, the MaxEnt estimation process produces a model in which every feature fi has a weight i. We can compute the conditional probability as (Berger et al., 1996): p(o|h) = 1Z(h) productdisplay i ifi(h,o) (1) Z(h) = summationdisplay o productdisplay i ifi(h,o) (2) The conditional probability of the outcome is the product of the weights of all active features, normalized over the products of all the features",0
"Word alignments were generated using Model 4 (Brown et al., 1993) using the multi-threaded implementation of GIZA++ (Och and Ney, 2003; Gao and Vogel, 2008)",0
"7Following Carletta (1996), we measure agreement in Kappa, which follows the formula K = P(A)P(E)1P(E) where P(A) is observed, and P(E) expected agreement",0
"These scores are higher than those of several other parsers (e.g. Collins 1997, 99; Charniak 1997), but remain behind tim scores of Charniak (2000) who obtains 90.1% LP and 90.1% LR for sentences _< 40 words",1
"1 Introduction Parsing technology has come a long way since Charniak (1996) demonstrated that a simple treebank PCFG performs better than any other parser (with F175 accuracy) on parsing the WSJ Penn treebank (Marcus et al., 1993)",0
"(2006) effectively utilized unlabeled data to improve parsing accuracy on the standard WSJ training set, but they used a two-stage parser comprised of Charniaks lexicalized probabilistic parser with n-best parsing and a discriminative reranking parser (Charniak and Johnson, 2005), and thus it would be better categorized as co-training (McClosky et al., 2008)",0
"Maximum entropy estimation for translation of individual words dates back to Berger et al (1996), and the idea of using multi-class classifiers to sharpen predictions normally made through relative frequency estimates has been recently reintroducedundertherubricofwordsensedisambiguation and generalized to substrings (Chan et al 2007; Carpuat and Wu 2007a; Carpuat and Wu 2007b)",0
"4.5.2 BLEU on NIST MT Test Sets We use MT02 as the development set4 for minimum error rate training (MERT) (Och, 2003)",0
"There exists a variety of different metrics, e.g., word error rate, position-independent word error rate, BLEU score (Papineni et al. , 2002), NIST score (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), GTM (Turian et al. , 2003)",0
"Almost all of these measures can be grouped into one of the following three categories: a0 frequency-based measures (e.g. , based on absolute and relative co-occurrence frequencies) a0 information-theoretic measures (e.g. , mutual information, entropy) a0 statistical measures (e.g. , chi-square, t-test, log-likelihood, Dices coefficient) The corresponding metrics have been extensively discussed in the literature both in terms of their mathematical properties (Dunning, 1993; Manning and Schutze, 1999) and their suitability for the task of collocation extraction (see Evert and Krenn (2001) and Krenn and Evert (2001) for recent evaluations)",0
"We have used three different algorithms: the nearest neighbour algorithm IB1IG, which is part of the Timbl software package (Daelemans et al. , 1999), the decision tree learner IGTREE, also from Timbl, and C5.0, a commercial version of the decision tree learner C4.5 (Quinlan, 1993)",0
"Various methods (Hindle, 1990; Lin, 1998; Hagiwara et al. , 2005) have been proposed for synonym acquisition",0
"Finally, the loglikelihood ratios test (henceforth LLR) (Dunning, 1993) is applied on each set of pairs",0
"However, except for (Fraser and Marcu, 2007b), none of these advances in alignment quality has improved translation quality of a state-of-the-art system",1
"Importantly, this Bayesian approach facilitates the incorporation of sparse priors that result in a more practical distribution of tokens to lexical categories (Johnson, 2007)",0
"Table 1 shows the percentage of agreement in classifying words as compounds or non-compounds (Compound Classification Agreement, CCA) for each language and the Kappa score (Carletta, 1996) obtained from it, and the percentage of words for which also the decomposition provided was identical (Decompounding Agreement, DA)",0
"Baseline We use the Moses MT system (Koehn et al., 2007) as a baseline and closely follow the example training procedure given for the WMT-07 and WMT-08 shared tasks.4 In particular, we perform word alignment in each direction using GIZA++ (Och and Ney, 2003), apply the grow-diag-finaland heuristic for symmetrization and use a maximum phrase length of 7",0
"Some works \[Woods et al, 1972\], \[Boguraev, 1979\], \[Marcus et al. 1993\] suggested several strategies that based their 231 decision-making on the relationships existing between predicates and argumentswhat \[Katz and Fodor, 1963\] called selectional restrictions",0
"The difference in accuracy between a SVM model applied to RRR dataset (RRR-basic experiment) and the same experiment applied to TB2 dataset (TB2278 Description Accuracy Data Extra Supervision Always noun 55.0 RRR Most likely for each P 72.19 RRR Most likely for each P 72.30 TB2 Most likely for each P 81.73 FN Average human, headwords (Ratnaparkhi et al. , 1994) 88.2 RRR Average human, whole sentence (Ratnaparkhi et al. , 1994) 93.2 RRR Maximum Likelihood-based (Hindle and Rooth, 1993) 79.7 AP Maximum entropy, words (Ratnaparkhi et al. , 1994) 77.7 RRR Maximum entropy, words & classes (Ratnaparkhi et al. , 1994) 81.6 RRR Decision trees (Ratnaparkhi et al. , 1994) 77.7 RRR Transformation-Based Learning (Brill and Resnik, 1994) 81.8 WordNet Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR Maximum-Likelihood based (Collins and Brooks, 1995) 86.1 TB2 Decision trees & WSD (Stetina and Nagao, 1997) 88.1 RRR WordNet Memory-based Learning (Zavrel et al. , 1997) 84.4 RRR LexSpace Maximum entropy, unsupervised (Ratnaparkhi, 1998) 81.9 Maximum entropy, supervised (Ratnaparkhi, 1998) 83.7 RRR Neural Nets (Alegre et al. , 1999) 86.0 RRR WordNet Boosting (Abney et al. , 1999) 84.4 RRR Semi-probabilistic (Pantel and Lin, 2000) 84.31 RRR Maximum entropy, ensemble (McLauchlan, 2001) 85.5 RRR LSA SVM (Vanschoenwinkel and Manderick, 2003) 84.8 RRR Nearest-neighbor (Zhao and Lin, 2004) 86.5 RRR DWS FN dataset, w/o semantic features (FN-best-no-sem) 91.79 FN PR-WWW FN dataset, w/ semantic features (FN-best-sem) 92.85 FN PR-WWW TB2 dataset, best feature set (TB2-best) 93.62 TB2 PR-WWW Table 5: Accuracy of PP-attachment ambiguity resolution (our results in bold) basic experiment) is 2.9%",0
"We have already shown in Section 3 how to solve (a); here we avoid (b) by maximizing conditional likelihood, marginalizing out the hidden variable, denotedz: max vector summationdisplay x,y p(x,y)log summationdisplay z pvector(y,z | x) (17) This sort of conditional training with hidden variables was carried out by Koo and Collins (2005), for example, in reranking; it is related to the information bottleneck method (Tishby et al. , 1999) and contrastive estimation (Smith and Eisner, 2005)",0
"2 Bidirectional Dependency Networks When building probabilistic models for tag sequences, we often decompose the global probability of sequences using a directed graphical model (e.g. , an HMM (Brants, 2000) or a conditional Markov model (CMM) (Ratnaparkhi, 1996))",0
"Schtze, 1993) is not suited to highly skewed distributions omni-present in natural language",0
"Smadja, 1993): 1",0
"In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium?s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al. , 2003), Translation Error Rate (TER) (Snover et al. , 2006)1, and METEOR (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment",1
"Both data were extracted from the Penn Treebank Wall Street Journal (WSJ) Corpus (Marcus et al. , 1993)",0
"1 Introduction and Motivation Detecting contradictory statements is an important and challenging NLP task with a wide range of potential applications including analysis of political discourse, of scientific literature, and more (de Marneffe et al., 2008; Condoravdi et al., 2003; Harabagiu et al., 2006)",0
"Within the machine learning paradigm, IL has been incorporated as a technique for bootstrapping an extensional learning algorithm, as in (Yarowsky, 1995; Collins and Singer, 1999; Liu et al. , 2004)",0
"3.1 Exhaustive search by tree fragments This method generates all possible tree fragments rooted by each node in the source parse tree or forest, and then matches all the generated tree fragments against the source parts (left hand side) of translation rules to extract the useful rules (Zhang et al., 2008a)",0
"5 The SemCor collection (Miller et al., 1993) is a subset of the Brown Corpus and consists of 352 news articles distributed into three sets in which the nouns, verbs, adverbs, and adjectives have been manually tagged with their corresponding WordNet senses and part-of-speech tags using Brills tagger (1995)",0
"In informal experiments described elsewhere (Melamed 1995), I found that the G 2 statistic suggested by Dunning (1993) slightly outperforms 2",1
"2.5 Evaluation Minnen and Carroll (Under review) report an evaluation of the accuracy of the morphological generator with respect to the CELEX lexical database (version 2.5; Baayen et al. , 1993)",0
"In the following section, we follow the notation in (Collins, 1997)",0
"However, we do not rely on linguistic resources (Kamps and Marx, 2002) or on search engines (Turney and Littman, 2003) to determine the semantic orientation, but rather rely on econometrics for this task",0
"In addition, parsing re-ranking (Collins 2000; Riezler et al. 2002; Charniak and Johnson 2005; Huang 2008) has also been shown to be another effective technique to improve parsing performance",0
"Computational linguists have demonstrated that a words meaning is captured to some extent by the distribution of words and phrases with which it commonly co-occurs (Church & Hanks, 1990)",0
"2 Background 2.1 Phrase Table Extraction Phrasal decoders require a phrase table (Koehn et al. , 2003), which contains bilingual phrase pairs and 17 scores indicating their utility",0
"Computing the phrase translation probability is trivial in the training corpora, but lexical weighting (Koehn et al., 2003) needs lexical-level alignment",0
"Following previous work in statistical MT (Brown et al., 1993), we envision a noisy-channel model in which a language model generates English, and then a translation model transforms English trees into Chinese",0
"Statistic-based algorithms based on Belief Network(Murphy, 2001) such as Hidden-MarkovModel(HMM)(Cutting, 1992)(Thede, 1999), Lexicalized HMM(Lee, 2000) and Maximal-Entropy model(Ratnaparkhi, 1996) use the statistical information of a manually tagged corpus as background knowledge to tag new sentences",0
"Corpus-based or example-based MT (Sato and Nagao, 1990; Sumita and Iida, 1991) and statistical MT (Brown et al. , 1993) systems provide the easiest customizability, since users have only to supply a collection of source and target sentence pairs (a bilingual corpus)",1
"For scoring MT outputs, the proposed RSCM uses a score based on a translation model called IBM4 (Brown et al. , 1993) (TM-score) and a score based on a language model for the translation target language (LM-score)",0
"It also differs from previous proposals on lexical acquisition using statistical measures such as (Church et al. , 1991; Brent, 1991; Brown et al. , 1993) which either deny the prior existence of linguistic knowledge or use linguistic knowledge in ad hoc ways",1
"We use 3500 sentences from CoNLL (Tjong Kim Sang and De Meulder, 2003) as the NER data and section 20-23 of the WSJ (Marcus et al., 1993; Ramshaw and Marcus, 1995) as the POS/chunk data (8936 sentences)",0
"Automated metrics such as BLEU (Papineni et al. , 2002), RED (Akiba et al, 2001), Weighted N-gram model (WNM) (Babych, 2004), syntactic relation / semantic vector model (Rajman and Hartley, 2001) have been shown to correlate closely with scoring or ranking by different human evaluation parameters",1
"While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account",0
"Within the NLP community, n-best list ranking has been looked at carefully in parsing, extractive summarization (Barzilay et al. 1999; Hovy and Lin 1998), and machine translation (Zhang et al. 2006), to name a few",0
"The relationship between the translation model and the alignment model is given by: Pr(fJ1 jeI1) = X aJ1 Pr(fJ1 ;aJ1jeI1) (3) In this paper, we use the models IBM-1, IBM4 from (Brown et al. , 1993) and the HiddenMarkovalignmentmodel(HMM)from(Vogelet al. , 1996)",0
"Note that, since the FrameNet data does not include deep syntactic tree annotation, we processed the FrameNet data with Collins parser (Collins, 1997), consequently, the experiments on FrameNet relate to automatic syntactic parse trees",0
"As a solution, a given amount of labeled training data is divided into two distinct sets, i.e., 4/5 for estimating , and the 667 remaining 1/5 for estimating  (Suzuki et al., 2007)",0
"1 Introduction The task of sentence compression (or sentence reduction) can be defined as summarizing a single sentence by removing information from it (Jing and McKeown, 2000)",0
"4 Pattern switching The compositional translation presents problems which have been reported by (Baldwin and Tanaka, 2004; Brown et al., 1993): Fertility SWTs and MWTs are not translated by a term of a same length",0
"Like baseNP chunking(Church, 1988; Ramshaw & Marcus 1995), content chunk parsing is also a kind of shallow parsing",0
cCarthy et al. use a distributional similarity thesaurus acquired from corpus data using the method of Lin (1998) for nding the predominant sense of a word where the senses are dened by WordNe,0
"Many corpus based methods have been proposed to deal with the sense disambiguation problem when given de nition for each possible sense of a target word or a tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (Leacock et al. , 1998), and semi-supervised sense disambiguation (Yarowsky, 1995)",0
"is combined with [ ]E jiT,1+ to be aligned with [ ] F nmT,, then [ ]( ) [ ]( )ATTCNTATTr E K E i FEF jinmjinm,.Pr,P,1],[,],[ ],1[ += where K is the degree of .EiN Finally, the node translation probability is modeled as ( ) ( ) ( )tNtNlNlNNN EiFlEiFlEjFl PrPrPr  . And the text translation probability ( )EF ttPr is model using IBM model I (Brown et al 1993)",0
"Word Error Rate (WER), which penalizes the edit distance against reference translations (Su et al. , 1992) BLEU: the geometric mean of n-gram precision for the translation results found in reference translations (Papineni et al. , 2002) Translation Accuracy (ACC): subjective evaluation ranks ranging from A to D (A: perfect, B: fair, C: acceptable and D: nonsense), judged blindly by a native speaker (Sumita et al. , 1999) In contrast to WER, higher BLEU and ACC scores indicate better translations",0
"2.3 Experiment The training set for these experiments was sections 01-21 of the Penn Treebank (Marcus et al. , 1993)",0
"The MT community has developed not only an extensive literature on alignment (Brown et al., 1993; Vogel et al., 1996; Marcu and Wong, 2002; DeNero et al., 2006), but also standard, proven alignment tools such as GIZA++ (Och and Ney, 2003)",0
"More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al., 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune",0
"7 For a more detailed discussion, see Berger, Della Pietra, and Della Pietra (1996) and Ratnaparkhi (1996)",0
"Additionally, automatic evaluation of content coverage using ROUGE (Lin, 2004) was explored in 2004",0
"Several automatic sentence alignment approaches have been proposed based on sentence length (Brown et al., 1991) and lexical information (Kay and Roscheisen, 1993)",0
"303 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language While it is common in studies of collocations to omit low-frequency words and expressions from analysis, because they give rise to invalid or unrealistic statistical measures (Church and Hanks, 1990), we are able to identify higher-precision collocations by including placeholders for unique words (i.e. , the ugen-n-grams)",0
"Training Procedure Our algorithm is a modification of the perceptron ranking algorithm (Collins, 2002), which allows for joint learning across several ranking problems (Daume III and Marcu, 2005; Snyder and Barzilay, 2007)",0
"We used the heuristic combination described in (Och and Ney 2003) and extracted phrasal translation pairs from this combined alignment as described in (Koehn et al. , 2003)",0
"It has shown promise in improving the performance of many tasks such as name tagging (Miller et al. , 2004), semantic class extraction (Lin et al. , 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998)",0
"Kim and Hovy (2006) proposed a method for extracting opinion holders, topics and opinion words, in which they use semantic role labeling as an intermediate step to label opinion holders and topics",0
"We shall take HMM-based word alignment model (Vogel et al. , 1996) as an example and follow the notation of (Brown et al. , 1993)",0
"Since a handmade thesaurus is not slfitahle for machine use, and expensive to compile, automatical construction of~a thesaurus has been attempted using corpora (Hindle, 1990)",0
"A Broad-Coverage Word Sense Tagger Dekang Lin Department of Computer Science University of Manitoba Winnipeg, Manitoba, Canada R3T 2N2 lindek@cs.umanitoba.ca Previous corpus-based Word Sense Disambiguation (WSD) algorithms (Hearst, 1991; Bruce and Wiebe, 1994; Leacock et al. , 1996; Ng and Lee, 1996; Yarowsky, 1992; Yarowsky, 1995) determine the meanings of polysemous words by exploiting their local contexts",0
"We adopted an N-best hypothesis approach (Och, 2003) to train",0
"Following extraction, O-CRF applies the RESOLVER algorithm (Yates and Etzioni, 2007) to find relation synonyms, the various ways in which a relation is expressed in text",0
"(Jiang et al., 2008a; Jiang et al., 2008b)",0
"1 Introduction In statistical machine translation, output translations are evaluated by their similarity to human reference translations, where similarity is most often measured by BLEU (Papineni et al., 2002)",0
"1 Introduction Recent work in statistical machine translation (MT) has sought to overcome the limitations of phrasebased models (Marcu and Wong, 2002; Koehn et al. , 2003; Och and Ney, 2004) by making use of syntactic information",0
"2 Previous Work Other researchers have investigated the topic of automatic generation of abstracts, but the focus has been different, e.g., sentence extraction (Edmundson, 1969; Johnson et al, 1993; Kupiec et al. , 1995; Mann et al. , 1992; Teufel and Moens, 1997; Zechner, 1995), processing of structured templates (Paice and Jones, 1993), sentence compression (Hori et al. , 2002; Knight and Marcu, 2001; Grefenstette, 1998, Luhn, 1958), and generation of abstracts from multiple sources (Radev and McKeown, 1998)",0
"More specifically, a statistical word alignment model (Brown et al. , 1993) is used to acquire a bilingual lexicon consisting of NL substrings coupled with their translations in the target MRL",0
.2 Data sparseness Another facet of the general trade-off identified by Rapp (2002) pertains to how limitations in862 herent in the combination of data and cooccurrence retrieval method are manifes,0
"Methods such as (Wu, 1997), (Alshawi et al. , 2000) and (Lopez et al. , 2002) employ a synchronous parsing procedure to constrain a statistical alignment",0
"(Yamada and Knight, 2001) follow (Brown et al., 1993) in using the noisy channel model, by decomposing the translation decisions modeled by the translation model into different types, and inducing probability distributions via maximum likelihood estimation over each decision type",0
"Candidate translations are scored by a linear combination of models, weighted according to Minimum Error Rate Training or MERT (Och, 2003)",0
"We train our feature weights using max-BLEU (Och, 2003) and decode with a CKY-based decoder that supports language model scoring directly integrated into the search",0
"We use Viterbi training (Brown et al. , 1993) but neighborhood estimation (Al-Onaizan et al. , 1999; Och and Ney, 2003) or pegging (Brown et al. , 1993) could also be used",0
"production rules are typically learned from alignment structures (Wu, 1997; Zhang and Gildea, 2004; Chiang, 2007) or from alignment structures and derivation trees for the source string (Yamada and Knight, 2001; Zhang and Gildea, 2004)",0
"Since Czech is a language with relatively high degree of word-order freedom, and its sentences contain certain syntactic phenomena, such as discontinuous constituents (non-projective constructions), which cannot be straightforwardly handled using the annotation scheme of Penn Treebank (Marcus et al. , 1993; Linguistic Data Consortium, 1999), based on phrase-structure trees, we decided to adopt for the PCEDT the dependency-based annotation scheme of the Prague Dependency Treebank  PDT (Linguistic Data Consortium, 2001)",1
"A phrase-based translation model is one of the modern approaches which exploits a phrase, a contiguous sequence of words, as a unit of translation (Koehn et al. , 2003; Zens and Ney, 2003; Tillman, 2004)",0
"While theoretically sound, this approach is computationally challenging both in practice (DeNero et al., 2008) and in theory (DeNero and Klein, 2008), may suffer from reference reachability problems (DeNero et al., 2006), and in the end may lead to inferior translation quality (Koehn et al., 2003)",0
"+ truecase 20.7 (+0.4) 27.8 (+0.2) Table 2: Impact of truecasing on case-sensitive BLEU In a more integrated approach, factored translation models (Koehn and Hoang, 2007) allow us to consider grammatical coherence in form of partof-speech language models",0
"To quickly (and approximately) evaluate this phenomenon, we trained the statistical IBM wordalignment model 4 (Brown et al. , 1993),1 using the GIZA++ software (Och and Ney, 2003) for the following language pairs: ChineseEnglish, Italian English, and DutchEnglish, using the IWSLT-2006 corpus (Takezawa et al. , 2002; Paul, 2006) for the first two language pairs, and the Europarl corpus (Koehn, 2005) for the last one",0
"(Koo and Collins, 2005; Matsuzaki et al. , 2005; Riezler et al. , 2002))",0
"3.2 The parsers The parsers that we chose to evaluate are the C&C CCG parser (Clark and Curran, 2007), the Enju HPSG parser (Miyao and Tsujii, 2005), the RASP parser (Briscoe et al., 2006), the Stanford parser (Klein and Manning, 2003), and the DCU postprocessor of PTB parsers (Cahill et al., 2004), based on LFG and applied to the output of the Charniak and Johnson reranking parser",0
"Specifically, aspect rating as an interesting topic has also been widely studied (Titov and McDonald, 2008a; Snyder and Barzilay, 2007; Goldberg and Zhu, 2006)",0
"Minimizing risk has been shown to improve performance for MT (Kumar and Byrne, 2004), as well as other language processing tasks (Goodman, 1996; Goel and Byrne, 2000; Kumar and Byrne, 2002; Titov and Henderson, 2006; Smith and Smith, 2007)",1
"6.2 Experimental Settings We utilize a maximum entropy (ME) model (Berger et al., 1996) to design the basic classifier for WSD and TC tasks",0
"The IBM model 1 (Brown et al. , 1993) is used to find an initial estimate of the translation probabilities",0
"The learning algorithm used is the IB1 algorithm (Aha et al. , 1991) with k = 5, i.e. classification based on 5 nearest neighbors.4 Distances are measured using the modified value difference metric (MVDM) (Stanfill and Waltz, 1986; Cost and Salzberg, 1993) for instances with a frequency of at least 3 (and the simple overlap metric otherwise), and classification is based on distance weighted class voting with inverse distance weighting (Dudani, 1976)",0
"3 Quasi-Synchronous Grammar For a formal description of QG, we recommend Smith and Eisner (2006)",1
"Mincuts have been used 4As of this writing, WordNet is available for more than 40 world languages (http://www.globalwordnet.org) Figure 2: Semi-supervised classification using mincuts in semi-supervised learning for various tasks, including document level sentiment analysis (Pang and Lee, 2004)",0
"Previous workthe generative models described in Collins (1996) and the earlier version of these models described in Collins (1997)conditioned on punctuation as surface features of the string, treating it quite differently from lexical items",0
"The SemEval-2010 task we present here builds on thework ofNakov (Nakovand Hearst, 2006; Nakov, 2007; Nakov, 2008b), where NCs are paraphrased by combinations of verbs and prepositions",0
"Output sequence optimization Rather than basing classifications only on model parameters estimated from co-occurrences between input and output symbols employed for maximizing the likelihood of point-wise single-label predictions at the output level, classifier output may be augmented by an optimization over the output sequence as a whole using optimization techniques such as beam searching in the space of a conditional markov models output (Ratnaparkhi, 1996) or hidden markov models (Skut and Brants, 1998)",0
"Since that time, however, increasingly large amounts of language model training data have become available ranging from approximately one billion words (the Gigaword corpora from the Linguistic Data Consortium) to trillions of words (Brants et al., 2007)",0
"The system described in (Bean and Riloff, 1999) also makes use of syntactic heuristics",0
"corpus (Garside et al. , 1987), the Penn Treebank (Marcus et al. , 1993), the SUSANNE corpus (Sampson, 1995), the Spoken English Corpus (Taylor and Knowles, 1988), the Oxford Psycholinguistic Database (Quinlan, 1992), and the """"Computer-Usable"""" version of the Oxford Advanced Learner's Dictionary of Current English (OALDCE; Mitton, 1.9.92)",0
"Successful discriminative parsers have used generative models to reduce training time and raise accuracy above generative baselines (Collins & Roark, 2004; Henderson, 2004; Taskar et al. , 2004)",0
"Again the best result was obtained with IOB1 (F~=I =92.37) which is an imI)rovement of the best reported F,~=1 rate for this data set ((Ramshaw and Marcus, 1995): 92.03)",1
"One important application of bitext maps is the construction of translation lexicons (Dagan et al. , 1993) and, as discussed, translation lexicons are an important information source for bitext mapping",0
"The most popular non-data-splitting methods for predicting test set cross-entropy (or likelihood) are AIC and variants such as AICc, quasi-AIC (QAIC), and QAICc (Akaike, 1973; Hurvich and Tsai, 1989; Lebreton et al., 1992)",0
"After a brief period following the introduction of generally accepted and widely used metrics, BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), when it seemed that this persistent problem has finally been solved, the researchers active in the field of machine translation (MT) started to express their worries that although these metrics are simple, fast and able to provide consistent results for a particular system during its development, they are not sufficiently reliable for the comparison of different systems or different language pairs",1
"Idiom 0 0 1 1 0 2 V. Doubt 3 0 4 0 0 7 Total A 294 160 546 39 1 1,040 In order to measure the agreement in a more precise way, we used the Kappa statistic (Siegel and Castellan 1988), recently proposed by Carletta as a measure of agreement for discourse analysis (Carletta 1996)",0
"2.2 Perceptron-based training To tune the parameters w of the model, we use the averaged perceptron algorithm (Collins, 2002) because of its efficiency and past success on various NLP tasks (Collins and Roark, 2004; Roark et al. , 2004)",1
"3 The Learning Architecture The synchronous derivations described above are modelled with an Incremental Sigmoid Belief Network (ISBN) (Titov and Henderson, 2007a)",0
"The standard split of the corpus into training (sections 222, 9,753 sentences), validation (section 24, 321 sentences), and testing (section 23, 603 sentences) was performed.2 As in (Henderson, 2003; Turian and Melamed, 2006) we used a publicly available tagger (Ratnaparkhi, 1996) to provide the part-of-speech tag for each word in the sentence",0
"Turney also reported good result without domain customization (Turney, 2002)",1
"(Cahill et al. , 2004b) measure annotation quality in terms of precision and recall against manually constructed, gold-standard f-structures for 105 randomly selected trees from section 23 of the WSJ section of Penn-II",0
"The number of weights wi is 3 plus the number of source languages, and they are trained using minimum error-rate training (MERT) to maximize the BLEU score (Och, 2003) on a development set",0
"ROUGE-L (Lin, 2004) This measure evaluates summaries by longest common subsequence (LCS) defined by Equation 4",0
"The translation quality is evaluated by BLEU metric (Papineni et al., 2002), as calculated by mteval-v11b.pl with case-insensitive matching of n-grams, where n =4",0
"However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited",0
"7Our decoder lacks certain features shown to be beneficial to synchronous grammar decoding, in particular rule binarisation (Zhang et al., 2006)",0
"Specifically, we explore the statistical term weighting features of the word generation model with Support Vector machine (SVM), faithfully reproducing previous work as closely as possible (Pang et al., 2002)",0
"Unconstrained CL corresponds exactly to a conditional maximum entropy model (Berger et al. , 1996; Lafferty et al. , 2001)",0
"(2007), we introduced the Movie Review Polarity Dataset Enriched with Annotator Rationales.8 It is based on the dataset of Pang and Lee (2004),9 which consists of 1000 positive and 1000 negative movie reviews, tokenized and divided into 10 folds (F0F9)",0
"During the SRC stage, a Maximum entropy (Berger et al., 1996) classifier is used to predict the probabilities of a word in the sentence Language No-duplicated-roles Catalan arg0-agt, arg0-cau, arg1-pat, arg2-atr, arg2-loc Chinese A0, A1, A2, A3, A4, A5, Czech ACT, ADDR, CRIT, LOC, PAT, DIR3, COND English A0, A1, A2, A3, A4, A5, German A0, A1, A2, A3, A4, A5, Japanese DE, GA, TMP, WO Spanish arg0-agt, arg0-cau, arg1-pat, arg1-tem, arg2-atr, arg2-loc, arg2-null, arg4-des, argL-null, argMcau, argM-ext, argM-fin Table 1: No-duplicated-roles for different languages to be each semantic role",0
"2A maximum-entropy-based part of speech tagger was used (Ratnaparkhi, 1996) without the adaptation to the biomedical domain",0
"However, Dunning (1993) pointed out that for the purpose of corpus statistics, where the sparseness of data is an important issue, it is better to use the log-likelihood ratio",0
"4 Information Base 4.1 Text Corpus Text corpora are essential to statistical modeling, in developing formal theories of the grammars, investigating prosodic phenomena in speech, and evaluating or comparing the adequacy of parsing models (Marcus et al. , 1993)",0
"96 Research on DA classification initially focused on two-party conversational speech (Mast et al. , 1996; Stolcke et al. , 1998; Shriberg et al. , 1998) and, more recently, has extended to multi-party audio recordings like the ICSI corpus (Shriberg et al. , 2004)",0
"The user can select characters by their frequencies (i.e. -f and -g options), the top or bottom N% (i.e. -m and -n options), their ranks (i.e. -r and -s options) and by their frequencies above two standard deviations phlS the mean (Smadja, 1993) (i.e. -z option)",0
"In their presentation of the factored SMT models, Koehn and Hoang (2007) describe experiments for translating from English to German, Spanish and Czech, using morphology tags added on the morphologically rich side, along with POS tags",0
"The most sophisticated of these techniques (such as Support Vector Machines) are unfortunately too computationally expensive to be used on large datasets like the Penn Treebank (Marcus et al. , 1993)",0
"In particular, we use a feature augmentation technique recently introduced by Daume III (2007), and active learning (Lewis and Gale, 1994) to perform domain adaptation of WSD systems",0
"2 Related Work Syntax-based translation models engaged with SCFG have been actively investigated in the literature (Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Galley et al., 2004; Satta and Peserico, 2005)",0
"154 2 Translation Models 2.1 Standard Phrase-based Model Most phrase-based translation models (Och, 2003; Koehn et al. , 2003; Vogel et al. , 2003) rely on a pre-existing set of word-based alignments from which they induce their parameters",0
"For example, extractive text summarization generates a summary by selecting a few good sentences from one or more articles on the same topic (Goldstein et al. , 2000)",0
"Much of the work in sentiment analysis in the computational linguistics domain has focused either on short segments, such as sentences (Wilson et al. , 2005), or on longer documents with an explicit polarity orientation like movie or product reviews (Turney, 2002)",0
"sister head tag X Table 4: Linguistic features in the current model compared to the models of Carroll and Rooth (1998), Collins (1997), and Charniak (2000) Negra, based on Collinss (1997) model for nonrecursive NPs in the Penn Treebank (which are also flat)",0
"73 ID Participant BBN-COMBO BBN system combination (Rosti et al., 2008) CMU-COMBO Carnegie Mellon University system combination (Jayaraman and Lavie, 2005) CMU-GIMPEL Carnegie Mellon University Gimpel (Gimpel and Smith, 2008) CMU-SMT Carnegie Mellon University SMT (Bach et al., 2008) CMU-STATXFER Carnegie Mellon University Stat-XFER (Hanneman et al., 2008) CU-TECTOMT Charles University TectoMT (Zabokrtsky et al., 2008) CU-BOJAR Charles University Bojar (Bojar and Hajic, 2008) CUED Cambridge University (Blackwood et al., 2008) DCU Dublin City University (Tinsley et al., 2008) LIMSI LIMSI (Dechelotte et al., 2008) LIU Linkoping University (Stymne et al., 2008) LIUM-SYSTRAN LIUM / Systran (Schwenk et al., 2008) MLOGIC Morphologic (Novak et al., 2008) PCT a commercial MT provider from the Czech Republic RBMT16 Babelfish, Lingenio, Lucy, OpenLogos, ProMT, SDL (ordering anonymized) SAAR University of Saarbruecken (Eisele et al., 2008) SYSTRAN Systran (Dugast et al., 2008) UCB University of California at Berkeley (Nakov, 2008) UCL University College London (Wang and Shawe-Taylor, 2008) UEDIN University of Edinburgh (Koehn et al., 2008) UEDIN-COMBO University of Edinburgh system combination (Josh Schroeder) UMD University of Maryland (Dyer, 2007) UPC Universitat Politecnica de Catalunya, Barcelona (Khalilov et al., 2008) UW University of Washington (Axelrod et al., 2008) XEROX Xerox Research Centre Europe (Nikoulina and Dymetman, 2008) Table 2: Participants in the shared translation task",0
"The algorithm is based on the Machine Learning method for word categorisation, inspired by the well known study on basic-level categories \[Rosch, 1978\], presented in \[Basili et al, 1993a\]",0
"The corresponding unlabeled figures are 73.3 and 33.4.3 This confirms the results of previous studies showing that the pseudo-projective parsing technique used by MaltParser tends to give high precision  given that non-projective dependencies are among the most difficult to parse correctly  but rather low recall (McDonald and Nivre, 2007)",0
he research presented in this paper is similar in motivation to Resnik's (1993) work on selectional restriction,0
"We compared this nonprobabilistic DOP model against tile probabilistic DOP model (which estimales the most probable parse for each sentence) on three different domains: tbe Penn ATIS treebank (Marcus et al. 1993), the Dutch OVIS treebank (Bonnema el al. 1997) and tile Penn Wall Street Journal (WSJ) treebank (Marcus el al. 1993)",0
"A common choice for the local probabilistic classifier is maximum entropy classifiers (Berger et al. , 1996)",0
"For efficiency reasons we report results on sentences of length 30 words or less.10 The syntax-based method gives a BLEU (Papineni et al., 2002) score of 25.04, a 0.46 BLEU point gain over Pharoah",0
"The model consists of a set of word-pair parameters p(t\[s) and position parameters p(j\[i,/); in model 1 (IBM1) the latter are fixed at 1/(1 + 1), as each position, including the empty position 0, is considered equally likely to contain a translation for w. Maximum likelihood estimates for these parameters can be obtained with the EM algorithm over a bilingual training corpus, as described in (Brown et al. , 1993)",0
"(Smadja, 1993:p.168) Kita & al",0
"However, in the experiments described here, we focus on alignment at the level of sentences, this for a number of reasons: First, sentence alignments have so far proven their usefulness in a number of applications, e.g. bilingual lexicography (Langlois, 1996; Klavans and Tzoukermann, 1995; Dagan and Church, 1994), automatic translation verification (Macklovitch, 1995; Macklovitch, 1996) and the automatic acquisition of knowledge about translation (Brown et al. , 1993)",0
"1 Introduction Decoding is one of the three fundamental problems in classical SMT (translation model and language model being the other two) as proposed by IBM in the early 1990s (Brown et al. , 1993)",0
"2 We illustrate the rule extraction with an example from the tree-to-tree translation model based on tree sequence alignment (Zhang et al, 2008a) without losing of generality to most syntactic tree based models",0
"The applications range from simple classification tasks such as text classification and history-based tagging (Ratnaparkhi, 1996) to more complex structured prediction tasks such as partof-speech (POS) tagging (Lafferty et al., 2001), syntactic parsing (Clark and Curran, 2004) and semantic role labeling (Toutanova et al., 2005)",0
"By no means an exhaustive list, the most commonly cited ranking and scoring algorithms are HITS (Kleinberg 1998) and PageRank (Page et al. 1998), which rank hyperlinked documents using the concepts of hubs and authorities",0
"Two major research topics in this field are Named Entity Recognition (NER) (N. Wacholder and Choi, 1997; Cucerzan and Yarowsky, 1999) and Word Sense Disambiguation (WSD) (Yarowsky, 1995; Wilks and Stevenson, 1999)",0
"However, attempts to retrofit syntactic information into the phrase-based paradigm have not met with enormous success (Koehn et al. , 2003; Och et al. , 2003)1, and purely phrase-based machine translation systems continue to outperform these syntax/phrase-based hybrids",1
"Given the parallel corpus, we tagged the English words with a publicly available maximum entropy tagger (Ratnaparkhi, 1996), and we used an implementation of the IBM translation model (AlOnaizan et al. , 1999) to align the words",0
"Proceedings of the 40th Annual Meeting of the Association for (Brown et al. , 1990; Brown et al. , 1993), a number of other algorithms have been developed",0
"lscript1-regularized log-linear models (lscript1-LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assumingaLaplacianpriorontheweights(Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007)",0
"Penn Treebank (Marcus et al., 1993) the HPSG LinGo Redwoods Treebank (Oepen et al., 2002), and a smaller dependency treebank (Buchholz and Marsi, 2006)",0
"This is similar to results in the literature (Ramshaw and Marcus, 1995)",0
"In our experiments we use a grammar with a start symbol S, a single preterminal C, and two nonterminals A and B used to ensure that only one parse can generate any given word-level alignment (ignoring insertions and deletions) (Wu, 1997; Zens and Ney, 2003)",0
"Until now, translation models have been evaluated either subjectively (e.g. White and O'Connell 1993) or using relative metrics, such as perplexity with respect to other models (Brown et al. 1993b)",0
"As reported in (Collins, 2002; McDonald et al., 2005), parameter averaging can effectively avoid overfitting",0
"In agreement with recent resuits on parsing with lexicalised probabilistic grammars (Collins, 1997; Srinivas, 1997), we find that statistics over lexical, as opposed to structural, features best correspond to human intuitive.judgments and to experimental findings",1
"To compare different clustering algorithms, results with the standard method of (Brown et al. , 1992) (SRILMs ngram-class) are also reported",0
"First, hierarchical word clusters are derived from unlabeled data using the Brown et al. clustering algorithm (Brown et al., 1992)",0
"Since word senses are often associated with domains (Yarowsky, 1995), word senses can be consequently distinguished by way of determining the domain of each description",0
"(1996), Warnke et al",0
"Automatic approaches to creating a semantic orientation lexicon and, more generally, approaches for word-level sentiment annotation can be grouped into two kinds: (1) those that rely on manually created lexical resourcesmost of which use WordNet (Strapparava and Valitutti, 2004; Hu and Liu, 2004; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2006; An1http://www.wjh.harvard.edu/ inquirer 599 dreevskaia and Bergler, 2006; Kanayama and Nasukawa, 2006); and (2) those that rely on text corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004)",0
"These problems formulations are similar to those studied in (Ramshaw and Marcus, 1995) and (Church, 1988; Argamon et al. , 1998), respectively",0
arletta (1996) says that 0.67 a10a14a11a15a10 0.8 allows just tentative conclusions to be draw,0
"(Yarowsky, 1995) and (Mihalcea and Moldovan, 2001) utilized bootstrapping for word sense disambiguation",0
"In addition, many more sophisticated parsing models are elaborations of such PCFG models, so understanding the properties of PCFGs is likely to be useful (Charniak, 1997; Collins, 1997)",0
"We report that our parsing framework achieved high accuracy (88.6%) in dependency analysis of Japanese with a combination of an underspecified HPSG-based Japanese grammar, SLUNG (Mitsuishi et al. , 1998) and the maximum entropy method (Berger et al. , 1996)",0
"5 Augmenting the corpus with an extracted dictionary Previous research (Callison-Burch et al., 2004; Fraser and Marcu, 2006) has shown that including word aligned data during training can improve translation results",0
"4 Experimental Work A part of the Wall Street Journal (WSJ) which had been processed in the Penn Treebanck Project (Marcus et al. , 1993) was used in the experiments",0
"We guess it is an acronym for the authors of (Galley et al., 2004): Michel Galley, Mark Hopkins, Kevin Knight and Daniel Marcu",0
"Introduction Many applications that process natural language can be enhanced by incorporating information about the probabilities of word strings; that is, by using statistical language model information (Church et al. 1991; Church and Mercer 1993; Gale, Church, and Yarowsky 1992; Liddy and Paik 1992)",0
"2 Architecture of the system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f. It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the translation process: e = argmaxp(e|f) = argmaxe {exp(summationdisplay i ihi(e,f))} (1) The feature functions hi are the system models and the i weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002)",0
"Alignment, whether for training a translation model using EM or for nding the Viterbi alignment of test data, is O(n6) (Wu, 1997), while translation (decoding) is O(n7) using a bigram language model, and O(n11) with trigrams",0
"2 Phrase-based SMT We use a phrase-based SMT system, Pharaoh, (Koehn et al. , 2003; Koehn, 2004), which is based on a log-linear formulation (Och and Ney, 2002)",0
"For all performance metrics, we show the 70% confidence interval with respect to the MAP baseline computed using bootstrap resampling (Press et al. , 2002; Och, 2003)",0
"Foralllanguagepairs,weusedtheMosesdecoder (Koehnetal.,2007), whichfollowsthephrase-based statistical machine translation approach (Koehn et al., 2003), with default settings as a starting point",0
"What, therefore, has to be explored are various similarity metrics, defining similarity in a concrete way and evaluate the results against human annotations (see Papineni et al. , 2002)",0
"The translation quality is evaluated using a well-established automatic measure: BLEU score (Papineni et al. , 2002)",1
"(1992b) has proved to be a simple yet powerful observation and has been successfully used in word sense disambiguation (WSD) and related tasks (e.g., Yarowsky (1995); Agirre and Rigau The author was partially funded by GALE DARPA Contract No",1
"Existing statistical NLG (i) uses corpus statistics to inform heuristic decisions in what is otherwise symbolic generation (Varges and Mellish, 2001; White, 2004; Paiva and Evans, 2005); (ii) applies n-gram models to select the overall most likely realisation after generation (HALOGEN family); or (iii) reuses an existing parsing grammar or treebank for surface realisation (Velldal et al. , 2004; Cahill and van Genabith, 2006)",0
"Standard sequence prediction models are highly effective for supertagging, including Hidden Markov Models (Bangalore and Joshi, 1999; Nielsen, 2002), Maximum Entropy Markov Models (Clark, 2002; Hockenmaier et al., 2004; Clark and Curran, 2007), and Conditional Random Fields (Blunsom and Baldwin, 2006)",0
"5.3 Related works and discussion Our two-step model essentially belongs to the same category as the works of (Mani et al. , 1999) and (Jing and McKeown, 2000)",0
"This probability is computed using IBMs Model 1 (Brown et al., 1993): P(Q|A) = productdisplay qQ P(q|A) (3) P(q|A) = (1)Pml(q|A)+Pml(q|C) (4) Pml(q|A) = summationdisplay aA (T(q|a)Pml(a|A)) (5) where the probability that the question term q is generated from answer A, P(q|A), is smoothed using the prior probability that the term q is generated from the entire collection of answers C, Pml(q|C)",0
"2.1 Conditional Maximum Entropy Model The goal of CME is to find the most uniform conditional distribution of y given observation x, ( )xyp, subject to constraints specified by a set of features ()yxf i,, where features typically take the value of either 0 or 1 (Berger et al. , 1996)",0
"Yarowsky (1995) has proposed automatically augmenting a small set of experimenter-supplied seed collocations (e.g. , manufacturing plant and plant life for two different senses of the noun plant) into a much larger set of training materials",0
"alpha 0 0.1 0.2 0.3 0.4 0.5 Freq=2 13555 13093 12235 11061 10803 10458 Freq=3 4203 3953 3616 3118 2753 2384 Freq=4 1952 1839 1649 1350 1166 960 Freq=5 1091 1019 917 743 608 511 Freq>2 2869 2699 2488 2070 1666 1307 TOTAL 23670 22603 20905 18342 16996 15620 alpha 0.6 0.7 0.8 0.9 1.0 Freq=2 10011 9631 9596 9554 9031 Freq=3 2088 1858 1730 1685 1678 Freq=4 766 617 524 485 468 Freq=5 392 276 232 202 189 Freq>2 1000 796 627 517 439 TOTAL 14257 13178 12709 12443 11805 Table 7: Number of extracted MWUs by frequency 6.2 Qualitative Analysis As many authors assess (Frank Smadja, 1993; John Justeson and Slava Katz, 1995), deciding whether a sequence of words is a multiword unit or not is a tricky problem",0
oodman (1997) and Johnson (1997) both suggest this strateg,0
"In comparison, the 2D model in Figure 2(c) used in previous work (Ding et al., 2008) can only model the interaction between adjacent questions",1
"So far, these techniques have focused on phrasebased models using contiguous phrases (Koehn et al. , 2003; Och and Ney, 2004)",0
"Following the framework of global linear models in (Collins, 2002), we cast this task as learning a mapping F from input verses x  X to a text-reuse hypothesis y  Y  {epsilon1}",0
"However, in the coarse-grained task, the sense inventory was first clustered semi-automatically with each cluster representing an equivalence class over senses (Navigli, 2006)",0
"A summary of the differences between our proposed approach and that of (Papineni et al., 2002) would include:  The reliance of BLEU on the diversity of multiple reference translations in order to capture some of the acceptable alternatives in both word choice and word ordering that we have shown above",0
"By contrast, alternative approaches, like Collins (1997), apply an additional transformation to each tree in the tree-bank, splitting each rule into small parts, which finally results in a new grammar covering many more sentences than the explicit one",0
"This leads to a good amount of work in this area (Ratnaparkhi et al. , 1994; Berger et al. , 1996; Pietra et al, 1997; Zhou et al. , 2003; Riezler and Vasserman, 2004) In the most basic approach, such as Ratnaparkhi et al",0
"We have observed in several experiments that the number of SuperARVs does not grow signi cantly as training set size increases; the moderate-sized Resource Management corpus (Price et al. , 1988) with 25,168 words produces 328 SuperARVs, compared to 538 SuperARVs for the 1 million word Wall Street Journal (WSJ) Penn Treebank set (Marcus et al. , 1993), and 791 for the 37 million word training set of the WSJ continuous speech recognition task",0
he novel idea presented in Strube & Ponzetto (2006) was to induce a semantic network from the Wikipedia categorization graph to compute measures of semantic relatednes,1
"In particular, this method has been used for word sense disambiguation (Lin, 1997) and thesaurus construction (Lin, 1998)",0
"Also, we chose to average each individual perceptron (Collins, 2002) prior to Bayesian averaging",0
in (1998) proposed a word similarity measure based on the distributio nal pattern of words which allows to construct a thesaurus using a parsed corpu,0
"2.1 Data-based Methods Data-based approaches extract their information directly from texts and are divided into supervised and unsupervised methods (Yarowsky, 1995; Stevenson, 2003)",0
"In the probabilistic LR model, probabilities are assigned to tree 696 Precision Recall F-score Time (min) Best-First Classifier-Based (this paper) 88.1 87.8 87.9 17 Deterministic (MaxEnt) (this paper) 85.4 84.8 85.1 < 1 Charniak & Johnson (2005) 91.3 90.6 91.0 Unk Bod (2003) 90.8 90.7 90.7 145* Charniak (2000) 89.5 89.6 89.5 23 Collins (1999) 88.3 88.1 88.2 39 Ratnaparkhi (1997) 87.5 86.3 86.9 Unk Tsuruoka & Tsujii (2005): deterministic 86.5 81.2 83.8 < 1* Tsuruoka & Tsujii (2005): search 86.8 85.0 85.9 2* Sagae & Lavie (2005) 86.0 86.1 86.0 11* Table 1: Summary of results on labeled precision and recall of constituents, and time required to parse the test set",0
"Of course, many applications require smoothing of the estimated distributionsthis problem also has known solutions in MapReduce (Brants et al., 2007)",0
"Obtained percent agreement of 0.988 and  coefficient (Carletta, 1996) of 0.975 suggest high convergence of both annotations",0
"(Suzuki et al. , 2006) 94.36 (+0.06) Table 8: The HySOL performance with the F-score optimization technique on Chunking (CoNLL-2000) experiments from unlabeled data appear different from each other",0
"Discriminative training with hidden variables has been handled in this probabilistic framework (Quattoni et al. , 2004; Koo and Collins, 2005), but we choose Equation 3 for efficiency",0
"It also shows that DOP's frontier lexicalization is a viable alternative to constituent lexicalization (as proposed in Charniak 1997; Collins 1997, 99; Eisner 1997)",0
"The following treebanks were used for training the parser: (Aduriz et al. , 2003; Bhmov et al. , 2003; Chen et al. , 2003; Haji et al. , 2004; Marcus et al. , 1993; Mart et al. , 2002; Montemagni et al. 2003; Oflazer et al. , 2003; Prokopidis et al. , 2005; Csendes et al. , 2005)",0
"However, their decoder is outperformed by phrase-based decoders such as (Koehn, 2004), (Och et al. , 1999), and (Tillmann and Ney, 2003)",0
.1 NP Our NP chunks are very similar to the ones of Ramshaw and Marcus (1995,0
"If one reduces the problem of entity mention detection to the detection of its head, the nature of the problem changes and the annotation of data becomes at; The [GPE Jordanian] [ORG military] [PER spokesman]  This allows us to consider the problem as a tagging/chunking problem and describe each word as beginning (B) an entity mention, inside (I) an entity mention or outside (O) an entity mention (Ramhsaw and Marcus, 1995; Sang and Veenstra, 1999)",0
"Semantic DSN: The construction of this network is inspired by (Lin, 1998)",0
"In the work of Smadja (1993) on extracting collocations, preference was given to constructions whose constituents appear in a fixed order, a similar (and more generally implemented) version of our assumption here that asymmetric constructions are more idiomatic than symmetric ones",0
"Deterministic Annealing: In this system, instead of using the regular MERT (Och, 2003) whose training objective is to minimize the onebest error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique)",0
"In practice, 7-/ is very large and the model's expectation Efj cannot be computed directly, so the following approximation(Lau et al. , 1993) is used: n E fj,~ E15(hi)p(tilhi)fj(hi,ti) i=1 where fi(hi) is the observed probability of the history hi in the training set",0
"0 10 20 30 40 5097.20 97.25 97.30 97.35 97.40 Iteration Accuracy on development data Every iterationEvery 4th iterationEvery 8th iteration Every 16th iterationOnce at the beginning No supervised data Figure 2: Dependence on the inclusion of the supervised training data (English) English Czech No supervised data 97.37 95.88 Once at the beginning 97.40 96.00 Every training iteration 97.44 96.21 Table 6: Dependence on the inclusion of the supervised training data 5.4 The morphological analyzers and the perceptron feature templates The whole experiment can be performed with the original perceptron feature set described in (Collins, 2002) instead of the feature set described in this article",0
"Bilexical CFG is at the heart of most modern statistical parsers (Collins, 1997; Charniak, 1997), because the statistics associated with word-specific rules are more informative for disambiguation purposes",0
"He then goes on to adapt the conventional noisy channel MT model of [Brown et al 1993] to NLU, where extracting a semantic representation from an input text corresponds to finding: argmax(Sem) {p(Input|Sem) p(Sem)}, where p(Sem) is a model for generating semantic representations, and p(Input|Sem) is a model for the relation between semantic representations and corresponding texts",0
"The Chinese text was tagged using the MXPOST maximum-entropy part of speech tagging tool (Ratnaparkhi, 1996) trained on the Penn Chinese Treebank 5.1; the English text was tagged using the TnT part of speech tagger (Brants, 2000) trained on the Wall Street Journal portion of the English Penn treebank",0
"For this experiment, we used sections 02 21 of the Penn Treebank (PTB) (Marcus et al. , 1993) as the training data and section 23 (2416 sentences) for evaluation, as is now standard",0
he measure of predictiveness we employed is log likelihood ratio with respect to the target variable (Dunning 1993,0
"When we trained external Chinese models, we used the same unlabeled data set as DeNero and Klein (2007), including the bilingual dictionary",0
"Aspect-based sentiment analysis summarizes sentiments with diverse attributes, so that customers may have to look more closely into analyzed sentiments (Titov and McDonald, 2008)",0
"Note that apart from previous work (Ding et al., 2008) we use complete skip-chain (contextanswer) edges in hc(x,y)",0
"It can be expected that the log-likelihood ratio produces an accurate ranking of word pairs that highly correlates with human judgment (Dunning, 1993), although there are other measures which come close in performance (e.g. Rapp, 1998)",1
"5http://cl.cs.okayama-u.ac.jp/rsc/ jacabit/ a4a6a5 which gathers the set of co-occurrence units a7 associated with the number of times that a7 and a2 occur together a8a6a9a10a9 a5 a11 . In order to identify speci c words in the lexical context and to reduce word-frequency effects, we normalize context vectors using an association score such as Mutual Information (Fano, 1961) or Log-likelihood (Dunning, 1993)",0
"The idea caught on very quickly: Suhm and Waibel (1994), Mast et aL (1996), Warnke et al",0
"While it was initially believed that lexicalization of PCFG parsers (Collins, 1997; Charniak, 2000) is crucial for obtaining good parsing results, Gildea (2001) demonstrated that the lexicalized Model-1 parser of Collins (1997) does not benefit from bilexical information when tested on a new text domain, and only marginally benefits from such information when tested on the same text domain as the training corpora",0
"Test and training materials were derived from the Brown corpus of American English, all of which has been parsed and manually verified by the Penn T~eebank project (Marcus et al. , 1993) and parts of which have been manually sense-tagged by the WordNet group (Miller et al. , 1993)",0
"(Brown et al. , 1993) defined two local search operations for their 1-to-N alignment models 3, 4 and 5",0
"Following (Lin, 2004), we computed the skip bi-gram score using both the sentence pool and the query pool",0
"Restricting phrases to syntactic constituents has been shown to harm performance (Koehn et al., 2003), so we tighten our definition of a violation to disregard cases where the only point of overlap is obscured by our phrasal resolution",0
"In (Collins and Roark, 2004; Shen and Joshi, 2005), a search stops if there is no hypothesis compatible with the gold standard in the queue of candidates",0
"1 Motivation A major component in phrase-based statistical Machine translation (PBSMT) (Zens et al., 2002; Koehn et al., 2003) is the table of conditional probabilities of phrase translation pairs",0
"During evaluation two performance metrics, BLEU (Papineni et al. , 2002) and NIST, were computed",0
"While reranking has benefited many tagging and parsing tasks (Collins, 2000; Collins, 2002c; Charniak and Johnson, 2005) including semantic role labeling (Toutanova et al. , 2005), it has not yet been applied to semantic parsing",0
"Default parameters were used for all experiments except for the numberofiterationsforGIZA++(OchandNey, 2003)",0
"Recent research [Yamamoto et al. , 2001] shows that using different clusters for predicted and conditional words can lead to cluster models that are superior to classical cluster models, which use the same clusters for both words [Brown et al. , 1992]",0
"Weights for the log-linear model are set using the 500-sentence tuning set provided for the shared task with minimum error rate training (Och, 2003) as implemented by Venugopal and Vogel (2005)",0
"The results of these studies have important applications in lexicography, to detect lexicosyntactic regularities (Church and Hanks, 19901 (Calzolari and Bindi,1990), such as, for example~ support verbs (e.g. """"make-decision"""") prepositional verbs (e.g. """"rely-upon"""") idioms, semantic relations (e.g. """"part_of"""") and fixed expressions (e.g. """"kick the bucket"""")",0
"Our evaluation metric is BLEU (Papineni et al., 2002) with caseinsensitive matching from unigram to four-gram",0
"We extract the named entities from the web pages using the Stanford Named Entity Recognizer (Finkel et al. , 2005)",0
"Automatically determining the degree of antonymy between words has many uses including detecting and generating paraphrases (The dementors caught Sirius Black / Black could not escape the dementors) and detecting contradictions (Marneffe et al., 2008; Voorhees, 2008) (Kyoto has a predominantly wet climate / It is mostly dry in Kyoto)",0
"Translation qualities are measured by uncased BLEU (Papineni et al. 2002) with 4 reference translations, sysids: ahb, ahc, ahd, ahe",0
"In particular, we have implemented an unsupervised morphological analyzer that outperforms Goldsmith s (2001) Linguistica and Creutz and Lagus s (2005) Morfessor for our English and Bengali datasets and compares favorably to the bestperforming morphological parsers in MorphoChallenge 20053 (see Dasgupta and Ng (2007))",1
"In general, previous work in opinion mining includes document level sentiment classification using supervised (Chaovalit and Zhou, 2005) and unsupervised methods (Turney, 2002), machine learning techniques and sentiment classification considering rating scales (Pang, Lee and Vaithyanathan, 2002), and scoring of features (Dave, Lawrence and Pennock, 2003)",0
"With automatic refinement it is harder to guarantee improved performance than with manual refinements (Klein and Manning, 2003) or with refinements based on direct lexicalization (Magerman (1995), Collins (1996), Charniak (1997), etc.)",0
"Several authors (for example, Krovetz and Croft \[1989\], Guthrie et al. \[1991\], Slator \[1992\], Cowie, Guthrie, and Guthrie \[1992\], Janssen \[1992\], Braden-Harder \[1993\], Liddy and Paik \[1993\]) have attempted to improve results by using supplementary fields of information in the electronic version of the Longman Dictionary of Contemporary English (LDOCE), in particular, the box codes and subject codes provided for each sense",0
"Given a manually compiled lexicon containing words and their relative frequencies Ps(fprimej), the best segmentationfJ1 is the one that maximizes the joint probability of all words in the sentence, with the assumption that words are independent of each other1: fJ1 = argmax fprimeJprime1 Pr(fprimeJprime1 |cK1 )  argmax fprimeJprime1 Jprimeproductdisplay j=1 Ps(fprimej), where the maximization is taken over Chinese word sequences whose character sequence is cK1 . 2.2 Translation system Once we have segmented the Chinese sentences into words, we train standard alignment models in both directions with GIZA++ (Och and Ney, 2002) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993)",0
"Collins (2000) and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1Dynamic programming methods (Geman and Johnson, 2002; Lafferty et al. , 2001) can sometimes be used for both training and decoding, but this requires fairly strong restrictions on the features in the model",1
"1 Introduction Text chunking has been one of the most interesting problems in natural language learning community since the first work of (Ramshaw and Marcus, 1995) using a machine learning method",0
"Adaptations to the algorithms in the presence of ngram LMs are discussed in (Chiang, 2007; Venugopal et al., 2007; Huang and Chiang, 2007)",0
"Recognizing this, Dolan (1994) proposes a method for """"ambiguating"""" dictionary senses by combining them to create grosser sense distinctions",0
"(2002) do not use a feature selection technique, employing instead an objective function which includes a Table 4 Values of Savings (a, b) for various values of a, b. ab Savings (a, b) 1100,000 2,692.7 110 48.6 11100 83.5 1011,000 280.0 1,00110,000 1,263.9 10,00150,000 2,920.2 50,001100,000 4,229.8 Collins and Koo Discriminative Reranking for NLP Gaussian prior on the parameter values, thereby penalizing parameter values which become too large: a C3  arg min a  LogLossa X k0:::m a 2 k 7 2 k  28 Closed-form updates under iterative scaling are not possible with this objective function; instead, optimization algorithms such as gradient descent or conjugate gradient methods are used to estimate parameter values",0
"2.3 Forest minimum error training To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space (Macherey et al., 2008)",0
"There are other approaches to statistical machine translation where translation is achieved through transduction of source language structure to target language structure (Alshawi et al. , 1998b; Wu, 1997)",0
.1 Likelihood Ratios in the Type-based Stage The log-likelihood ratio by Dunning (1993) tests whether the probability of a word is dependent on the occurrence of the preceding word typ,0
"Words are encoded through an automatic clustering algorithm (Brown et al. , 1992) while tags, labels and extensions are normally encoded using diagonal bits",0
"As previously observed in the literature (Mani, Gates, and Bloedorn 1999; Jing and McKeown 2000), such components include a clause in the clause conjunction, relative clauses, and some elements within a clause (such as adverbs and prepositions)",0
"Equation (10) is of interest because the ratio p(C | v, r)/p(C | r) can be interpreted as a measure of association between the verb v and class C. This ratio is similar to pointwise mutual information (Church and Hanks 1990) and also forms part of Resniks association score, which will be introduced in Section 6",0
"edu Abstract This paper reports on our experience hand tagging the senses of 25 of the most frequent verbs in 12,925 sentences of the Wall Street Journal Treebank corpus (Marcus et al. 1993)",0
"3.5 The Experiments We have ran LexTract on the one-millionword English Penn Treebank (Marcus et al. , 1993) and got two Treebank grammars",0
" Most existing work to capture labelconsistency, has attempted to create all parenleftbign2parenrightbig pairwise dependencies between the different occurrences of an entity, (Finkel et al. , 2005; Sutton and McCallum, 2004), where n is the number of occurrences of the given entity",0
"Haghighi and Klein s (2006) prototype-driven approach requires just a few prototype examples for each POS tag, exploiting these labeled words to constrain the labels of their distributionally similar words when training a generative log-linear model for POS tagging",0
"As mentioned in Section 2.2, there are words which have two or more candidate POS tags in the PTB corpus (Marcus et al., 1993)",0
"Much previous work has been done on this problem and many different methods have been used: Church's PARTS (1988) program uses a Markov model; Bourigault (1992) uses heuristics along with a grammar; Voutilainen's NPTool (1993) uses a lexicon combined with a constraint grammar; Juteson and Katz (1995) use repeated phrases; Veenstra (1998), Argamon, Dagan & Krymolowski(1998) and Daelemaus, van den Bosch & Zavrel (1999) use memory-based systems; Ramshaw & Marcus (In Press) and Cardie & Pierce (1998) use rule-based systems",0
"Parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis (Brown et al. 1990; Gale and Church 1991; Gale, Church, and Yarowsky 1992; Church 1993; Brown et al. 1993; Dagan, Church, and Gale 1993; Department of Computer Science, University of Science and Technology, Clear Water Bay, Hong Kong",0
"6 Experiments We evaluated the translation quality of the system using the BLEU metric (Papineni et al. , 2002)",0
"6 Concluding remarks Our work presents a set of improvements on previous state of the art of Grammar Association: first, by providing better language models to the original system described in (Vidal et al. , 1993); second, by setting the technique into a rigorous statistical framework, clarifying which kind of probabilities have to be estimated by association models; third, by developing a novel and especially adequate association model: Loco C. On the other hand, though experimental results are quite good, we find them particularly relevant for pointing out directions to follow for further improvement of the Grammar Association technique",0
"Another technique used was to filter sentences of the out-of-domain corpus based on their similarity to the target domain, as predicted by a classifier (Dredze et al. , 2007)",0
"Recent computational work either focuses on sentence subjectivity (Wiebe et al. 2002; Riloff et al. 2003), concentrates just on explicit statements of evaluation, such as of films (Turney 2002; Pang et al. 2002), or focuses on just one aspect of opinion, e.g., (Hatzivassiloglou and McKeown 1997) on adjectives",0
"Although, there are various manual/automatic evaluation methods for these systems, e.g., BLEU (Papineni et al. 2002), these methods are basically incapable of dealing with an MTsystem and a w/p-MT-system at the same time, as they have different output forms",1
"Labelling was carried out by three computational linguistics graduate students with 89% agreement resulting in a Kappa statistic of 0.87, which is a satisfactory indication that our corpus can be labelled with high reliability using our tag set (Carletta, 1996)",0
"To tackle this problem, we defined 2The best results of Collins and Roark (2004) (LR=88.4%, LP=89.1% and F=88.8%) are achieved when the parser utilizes the information about the final punctuation and the look-ahead",0
"In order to improve sentence-level evaluation performance, several metrics have been proposed, including ROUGE-W, ROUGE-S (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005)",0
"BLEU (Papineni et al. , 2002) is a precision metric that assesses the quality of a translation in terms of the proportion of its word n-grams (n  4 has become standard) that it shares with several reference translations",0
"Smadja (1993) also detailed techniques for collocation extraction and developed a program called XTRACT, which is capable of computing flexible collocations based on elaborated statistical calculation",0
"The mutual information of a cooccurrence pair, which measures the degree of association between the two words (Church and Hanks, 1990), is defined as (Fano, 1961): P(xly) I(x,y) -log 2 P(x,y) _ log 2 (1) P(x)P(y) P(x) = log 2 P(y\[x) P(Y) where P(x) and P(y) are the probabilities of the events x and y (occurrences of words, in our case) and P(x, y) is the probability of the joint event (a cooccurrence pair)",0
"For instance, the mutual information (Church et al. 1990) and log-likelihood ratio (Dunning 1993; Cohen 1995) have been widely used for extracting word bigrams",1
"The measure simHinate is the same as the similarity measure proposed in (Hindle, 1990), except that it does not use dependency triples with negative mutual information",0
"of ACL 1990 (Smadja, 1993), F. Smadja, Retrieving collocations fi'cma text: XTRACT, (1993)",0
"Statistical approaches, which depend on a set of unknown parameters that are learned from training data, try to describe the relationship between a bilingual sentence pair (Brown et al. , 1993; Vogel and Ney, 1996)",0
"2.3.4 Word Translation Probability Estimation Many methods are used to estimate word translation probabilities from unparallel or parallel bilingual corpora (Koehn and Knight, 2000; Brown et al. , 1993)",0
"For example, minimum entropy regularization (Grandvalet and Bengio, 2004; Jiao et al., 2006), aims to maximize the conditional likelihood of labeled data while minimizing the conditional entropy of unlabeled data: summationdisplay i logp(y(i)|x(i)) 122bardblbardbl2H(y|x) (3) This approach generally would result in sharper models which can be data-sensitive in practice",0
"The most commonly used MT evaluation metric in recent years has been IBM?s Bleu metric (Papineni et al. , 2002)",0
"Identification of Terms To-be Transliterated (TTT) must not be confused with recognition of Named Entities (NE) (Hermjakob et al., 2008)",0
"Slrs Parse Base (Black et al. , 1993a) is 1.76",0
"Statistical and information theoretic approaches (Hindle and Rooth, 1993), (Ratnaparkhi et al. , 1994),(Collins and Brooks, 1995), (Franz, 1996) Using lexical collocations to determine PPA with statistical techniques was first proposed by (Hindle and Rooth, 1993)",0
"We build a subset S C ~"""" incrementally by iterating to adjoin a feature f E ~"""" which maximizes loglikelihood of the model to S. This algorithm is called the Basic Feature Selection (Berger et al. , 1996)",0
"We use the same alignment data for the five language pairs Chinese/English, Romanian/English, Hindi/English, Spanish/English, and French/English (Wellington et al. , 2006)",0
he fact that different authors use different versions of the same gold standard to evaluate similar experiments (e.g. Goldwater & Griffiths (2007) versus Johnson (2007)) supports this clai,0
"These domains have been commonly used in prior work on summarization (Weischedel et al., 2004; Zhou et al., 2004; Filatova and Prager, 2005; DemnerFushman and Lin, 2007; Biadsy et al., 2008)",0
"To find these pairs automatically, wetrainedanon-sequentiallog-linearmodel that achieves a .902 accuracy (Galley et al. , 2004)",0
"Reliability metrics (Krippendorff 1980; Carletta 1996) are designed to give a robust measure of how well distinct sets of data agree with, or replicate, one another",0
"Models of this kind assume that an input word is generated by only one output word (Brown et al. , 1993)",0
"In the early statistical translation model work at IBM, these representations were called cepts, short for concepts (Brown et al., 1993)",0
"We decided to use the class of maximum entropy models, which are probabilistically sound, can make use of possibly many overlapping features, and can be trained efficiently (Berger et al., 1996)",1
"To be able identify that adjacent blocks (e.g., the development and and progress) can be merged into larger blocks, our model infers binary (non-linguistic) trees reminiscent of (Wu, 1997; Chiang, 2005)",0
"The first work on SMT done at IBM (Brown et al. , 1990; Brown et al. , 1992; Brown et al. , 1993; Berger et al. , 1994), used a noisy-channel model, resulting in what Brown et al",0
"(1999) proposed a summarization system based on the draft and revision. Jing and McKeown (2000) proposed a system based on extraction and cut-and-paste generation. Our abstractors performed the same cut-and-paste operations that Jing and McKeown noted in their work, and we think that our two-step model will be a reasonable starting point for our subsequent research",0
"Close to the problem studied here is Jing and McKeowns (Jing and McKeown, 2000) cut-and-paste method founded on EndresNiggemeyers observations",0
arowsky (1995) has proposed a bootstrapping method for word sense disambiguatio,0
"It turns out that while problems of coverage and ambiguity prevent straightforward lookup, injection of gazetteer matches as features in machine-learning based approaches is critical for good performance (Cohen, 2004; Kazama and Torisawa, 2007a; Toral and Munoz, 2006; Florian et al., 2003)",0
"The resulting intercoder reliability, measured with the Kappa statistic(Carletta,1996), is considered excellent (= 0.80)",0
"The feature weights i are trained in concert with the LM weight via minimum error rate (MER) training (Och, 2003)",0
"However, most of the existing models have been developed for English and trained on the Penn Treebank (Marcus et al. , 1993), which raises the question whether these models generalize to other languages, and to annotation schemes that differ from the Penn Treebank markup",0
his feature is implemented by using the IBM-1 lexical parameters (Brown et al. 1993; Och et al. 2004,0
"As an additional baseline, we compare against a phrasal SMT decoder, Pharaoh (Koehn et al. 2003)",0
"4.2 Adaptation to Chinese (Cahill et al. , 2004)s algorithm (Section 3.2) only resolves certain NLDs with known types of antecedents (TOPIC, TOPIC REL and FOCUS) at fstructures",0
"However, by examining the Algorithm 4.2: Perceptron with local and non-local features (parameters: n, Ca, Cl)   0 until no more updates do for i  1 to L do8 >> >> >> >> >> < >> >> >> >> >> : {yn} = n-bestyl(xi,y) y = argmaxy{yn}a(xi,y) y = 2nd-besty{yn}a(xi,y) if y = yi & a(xi,yi)a(xi,y)  Ca then  = + a(xi,yi)a(xi,y) (A) else if a(xi,yi)a(xi,y)  Ca then  = + a(xi,yi)a(xi,y) (A) else (B) 8> < >: if y1 = yi then (y1 represents the best in {yn})  = + l(xi,yi)l(xi,y1) else if l(xi,yi)l(xi,y2)  Cl then  = + l(xi,yi)l(xi,y2) proofs in Collins (2002a), we can see that the essential condition for convergence is that the weights are always updated using some y (= y) that satises: (xi,yi)(xi,y)  0 ( C in the case of a perceptron with a margin)",0
"Two disjoint corpora are used in steps 2 and 5, both consisting of complete articles taken from the Wall Street Journal Treebank Corpus (Marcus et al. , 1993)",0
"HockenmaierandSteedman(2007)showedthat a CCG corpus could be created by adapting the Penn Treebank (Marcus et al., 1993)",0
"Recent work on the automatic acquisition of multilingual LFG resources from treebanks for Chinese, German and Spanish (Burke et al. , 2004; Cahill et al. , 2005; ODonovan et al. , 2005) has shown that given a suitable treebank, it is possible to automatically acquire high quality LFG resources in a very short space of time",0
"57 Given a pair of English sentences to be compared (a system translation against a reference translation), we perform tokenization2, lemmatization using WordNet3, and part-of-speech (POS) tagging with the MXPOST tagger (Ratnaparkhi, 1996)",0
"Morphosyntacticinformationhas in fact been shown to significantlyimprove the extractionresults (Breidt, 1993; Smadja, 1993; Zajac et al. , 2003)",0
"Once an acceptable rate of interjudge agreement was verified on the first nine clusters (Kappa (Carletta, 1996) of 0.68), the remaining 11 clusters were annotated by one judge each",0
1993) and Och and Ney (2003,0
"5.3 Experimental setup We used the Stanford Parser (Klein and Manning, 2003) for both languages, Penn English Treebank (Marcus et al., 1993) and Penn Arabic Treebank set (Kulick et al., 2006)",0
"The part of speech tags for the development and test data were automatically assigned by MXPOST (Ratnaparkhi, 1996), where the tagger was trained on the entire training corpus; to generate part of speech tags for the training data, we used 10-way jackknifing.8 English word clusters were derived from the BLLIP corpus (Charniak et al., 2000), which contains roughly 43 million words of Wall Street Journal text.9 The Czech experiments were performed on the Prague Dependency Treebank 1.0 (Hajic, 1998; Hajic et al., 2001), which is directly annotated with dependency structures",0
arpuat and Wu (2007b) and Chan et a,0
"This includes the standard notion of phrase, popular with phrasedbased SMT (Koehn et al. , 2003; Vogel et al. , 2003) aswellassequencesofwordsthatcontaingaps(possibly of arbitrary size)",0
"On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression phrase-based models",0
"Although the BLEU (Papineni et al. , 2002) score from Finnish to English is 21.8, the score in the reverse direction is reported as 13.0 which is one of the lowest scores in 11 European languages scores (Koehn, 2005)",0
ur study also shows that the simulated-annealing algorithm (Kirkpatrick et al. 1983) is more effective 1552 than the perceptron algorithm (Collins 2002) for feature weight tunin,1
"Given training data consisting of parallel sentences: }1),,{( )()( Sief ii =, our Model-1 training for t(f|e) is as follows:  =  = S s ss e efefceft 1 )()(1 ),;|()|(  Where 1 e  is a normalization factor such that 0.1)|( =  j j eft ),;|( )()( ss efefc denotes the expected number of times that word e connects to word f.   == = = l i i m j jl k k ss eeff eft eft efefc 11 1 )()( ),(),( )|( )|( ),;|(  With the conditional probability t(f|e), the probability for an alignment of foreign string F given English string E is in (1):  = = + = m j n i ijm eft l EFP 1 0 )|( )1( 1 )|( (1) The probability of alignment F given E: )|( EFP is shown to achieve the global maximum under this EM framework as stated in (Brown et al. ,1993)",0
"(In our experiments, we use maximum entropy classification (MaxEnt) (Berger et al. , 1996) to train this probability model)",0
"Many of the previous studies of Bio-NER tasks have been based on machine learning techniques including Hidden Markov Models (HMMs) (Bikel et al. , 1997), the dictionary HMM model (Kou et al. , 2005) and Maximum Entropy Markov Models (MEMMs) (Finkel et al. , 2004)",0
"Automatic Evaluation Measures A variety of automatic evaluation methods have been recently proposed in the machine translation community (NIST, 2002; Melamed et al. , 2003; Papineni et al. , 2002)",0
"One possible conclusion from the POS tagging literature is that accuracy is approaching the limit, and any remaining improvement is within the noise of the Penn Treebank training data (Ratnaparkhi, 1996; Toutanova et al. , 2003)",0
"This set of 800 sentences was used for Minimum Error Rate Training (Och, 2003) to tune the weights of our system with respect to BLEU score",0
"Collocations were extracted according to the method described in (Church and Hanks, 1990) by moving a window on texts",0
"4.2 Binarization Schemes Besides the baseline (Zhang et al., 2006) and iterative cost reduction binarization methods, we also perform right-heavy and random synchronous binarizations for comparison",0
"6 Bracketing of Compound Nouns The first analysis task we consider is the syntactic disambiguation of compound nouns, which has received a fair amount of attention in the NLP literature (Pustejovsky et al. , 1993; Resnik, 1993; Lauer, 1995)",0
"We use the same preprocessing steps as Turian and Melamed (2005): during both training and testing, the parser is given text POS-tagged by the tagger of Ratnaparkhi (1996), with capitalization stripped and outermost punctuation removed",0
"Some stem from work on graphical models,includingloopybeliefpropagation(Suttonand McCallum, 2004; Smith and Eisner, 2008), Gibbs sampling (Finkel et al., 2005), sequential Monte Carlo methods such as particle filtering (Levy et al., 2008), and variational inference (Jordan et al., 1999; MacKay, 1997; Kurihara and Sato, 2006)",0
"Recently some researchers have pointed out the importance of the lexicon and proposed lexicalized models (Jelinek et al. , 1994; Collins, 1997)",0
"(Marcus, et al. 1993; Santorini 1990) The syntactic annotation task consists of marking constituent boundaries, inserting empty categories (traces of movement, PRO, pro), showing the relationships between constituents (argument/adjunct structures), and specifying a particular subset of adverbial roles",0
"For each feature function, there is a model parameter  i . The best word segmentation W * is determined by the decision rule as  = == M i ii W M W WSfWSScoreW 0 0 * ),(maxarg),,(maxarg  (2) Below we describe how to optimize  s. Our method is a discriminative approach inspired by the Minimum Error Rate Training method proposed in Och (2003)",0
"Daume III (Daume III, 2007) divided features into three classes: domainindependent features, source-domain features and target-domain features",0
"Document level sentiment classification is mostly applied to reviews, where systems assign a positive or negative sentiment for a whole review document (Pang et al. , 2002; Turney, 2002)",0
"Recently, Ponzetto and Strube (2006) suggest to mine semantic relatedness from Wikipedia, which can deal with the data sparseness problem suffered by using WordNet",1
"6 The Experiments To investigate the e ects of lookahead on our family of deterministic parsers, we ran empirical experiments on the standard the Penn Treebank (Marcus et al. , 1993) datasets",0
"7.3 EM algorithm The only other application of the EM algorithm to word-sense disambiguation is described in (Gale, Church, and Yarowsky, 1995)",0
"Also, attribute classi cation is a hard problem and there is no existing classi cation scheme that can be used for open domains like newswire; for example, WordNet (Miller et al. , 1993) organises adjectives as concepts that are related by the non-hierarchical relations of synonymy and antonymy (unlike nouns that are related through hierarchical links such as hyponymy, hypernymy and metonymy)",0
OS tag the text using Ratnaparkhi (1996,0
"All conditions were optimized using BLEU (Papineni et al., 2002) and evaluated using both BLEU and Translation Edit Rate (TER) (Snover et al., 2006)",0
2005) applied the distributional similarity proposed by Lin (1998) to coordination disambiguatio,0
"We adopted log-likelihood ratio (Danning 1993), which gave the best pertbrmance among crude non-iterative methods in our test experiments 6 ",0
"Mathematical details are fully described in (Brown et al. , 1993)",0
"The baseline we measure against in all of these experiments is the state-of-the-art grow-diag-final (gdf ) alignment refinement heuristic commonly used in phrase-based SMT (Koehn et al. , 2003)",0
"In this paper, sentence pairs are extracted by a simple model that is based on the so-called IBM Model1 (Brown et al., 1993)",0
"Attempts to alleviate this tagbottleneck i~lude tmotstr~ias (Te~ ot ill,, 1996; Hearst, 1991) and unsupervised algorith~ (Yarowsky, 199s) Dictionary-based approaches rely on linguistic knowledge sources such as ma~l~i,~e-readable dictionaries (Luk, 1995; Veronis and Ide, 1990) and WordNet (Agirre and Rigau, 1996; Resnik, 1995) and e0(ploit these for word sense disaznbiguation",0
"It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) instead of the original word-based approach",0
"We use Entropy Regularization (ER) (Jiao et al., 2006) to leverage unlabeled instances.7 We weight the ER term by choosing the best8 weight in {103,102,101,1,10} multiplied by #labeled#unlabeled for each data set and query selection method",0
"3.2 Translation performance For the experiments reported in this section, we used feature weights trained with minimum error rate training (MERT; Och, 2003) . Because MERT ignores the denominator in Equation 1, it is invariant with respect to the scale of the weight vector   the Moses implementation simply normalises the weight vector it finds by its lscript1-norm",0
"BLEU (Papineni et al., 2002), NIST (Doddington, 2002)",0
"The third baseline, COMP is the document compression system developed by Daume III and Marcu (2002), which compresses documents by cutting out constituents in a combined syntax and discourse tree",0
"Automatic measures like BLEU (PAPINENI et al. , 2001) or NIST (DODDINGTON, 2002) do so by counting sequences of words in such paraphrases",0
"For the extraction problem, there have been various methods proposed to date, which are quite adequate (Hindle and Rooth 1991; Grishman and Sterling 1992; Manning 1992; Utsuro, Matsumoto, and Nagao 1992; Brent 1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997)",1
"This operation does not change the collection of phrases or rules extracted from a hypothesized alignment, see, for instance, (Koehn et al. , 2003)",0
"From this point of view, some of the measures used in the evaluation of Machine Translation systems, such as BLEU (Papineni et al. , 2002), have been imported into the summarization task",0
"One kind is the Penn Treebank (Marcus et al. , 1993)",0
"The most commonly used MT evaluation metric in recent years has been IBMs Bleu metric (Papineni et al., 2002)",0
opez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based system,0
"From this data, we use the the GHKM minimal-rule extraction algorithm of (Galley et al., 2004) to yield rules like: NP-C(x0:NPB PP(IN(of x1:NPB))$x1 de x0 Though this rule can be used in either direction, here we use it right-to-left (Chinese to English)",0
"Thus, it may not suffer from the issues of non-isomorphic structure alignment and non-syntactic phrase usage heavily (Wellington et al., 2006)",0
"The prior probability P0 is the prior distribution for the phrase probability which is estimated using the phrase normalized counts commonly used in conventional Phrasebased SMT systems, e.g., (Koehn et al., 2003)",0
"Feature weight tuning was carried out using minimum error rate training, maximizing BLEU scores on a held-out development set (Och, 2003)",0
"To generate word alignments we use GIZA++ (Och and Ney 2003), which implements both the IBM Models of Brown et al",0
"The model cleanly incorporates both syntax and lexical semantics using quasi-synchronous dependency grammars (Smith and Eisner, 2006)",0
"A maximum entropy approach has been applied to partof-speech tagging before (Ratnaparkhi 1996), but the approach's ability to incorporate nonlocal and non-HMM-tagger-type evidence has not been fully explored",1
"We estimated the probabilities P(c I Pi) and P(c) similarly to Resnik (1993) by using relative frequencies from the BNC, together with WordNet (Miller et al. , 1990) as a source of taxonomic semantic class information",0
"3.4 Feature Representation Ranking Models Following previous work on sentiment classi cation (Pang et al. , 2002), we represent each review as a vector of lexical features",0
"For every class the weights of the active features are combined and the best scoring class is chosen (Berger et al. , 1996)",0
"2 IBM Model 4 Various statistical alignment models of the form Pr(fJ1 ;aJ1jeI1) have been introduced in (Brown et al. , 1993; Vogel et al. , 1996; Och and Ney, 2000a)",0
"1 Introduction By exploiting information encoded in human-produced syntactic trees (Marcus et al. , 1993), research on probabilistic models of syntax has driven the performance of syntactic parsers to about 90% accuracy (Charniak, 2000; Collins, 2000)",1
he prevalent use of this criterion despite repeated advice that it is unlikely to be suitable for all studies (Carletta 1996; Di Eugenio and Glass 2004; Krippendorff 2004a) is probably due to a desire for a simple system that can be easily applied to a schem,0
"Alignment performance is measured by the Alignment Error Rate (AER) (Och and Ney, 2003) AER(B;B) = 12|B B|/(|B|+|B|) where B is a set reference word links, and B are the word links generated automatically",0
"1 Introduction Over the past five years progress in machine translation, and to a lesser extent progress in natural language generation tasks such as summarization, has been driven by optimizing against n-grambased evaluation metrics such as Bleu (Papineni et al. , 2002)",0
"To solve the problem, Cahill and van Genabith (2006) apply an automatic generation grammar transformation to their training data: they automatically label CFG nodes with additional case information and the model now learns the new improved generation rules of Tables 4 and 5",0
"This difference was highlighted in the 3http://w3.msi.vxu.se/jha/maltparser/ studyofMcDonaldandNivre(2007), whichshowed that the difference is reflected directly in the error distributions of the parsers",0
"(Fleischman et al. , 2003; Jijkoun et al. , 2003)",0
"The parser implementation in (Bikel, 2002) was used in this experiment and it was run in a mode which emulated the Collins (1997) parser",0
"These algorithms are usually applied to sequential labeling or chunking, but have also been applied to parsing (Taskar et al. , 2004; McDonald et al. , 2005), machine translation (Liang et al. , 2006) and summarization (Daume III et al. , 2006)",0
"Artificial ungrammaticalities have been used in various NLP tasks (Smith and Eisner, 2005; Okanohara and Tsujii, 2007) The idea of an automatically generated ungrammatical treebank was proposed by Foster (2007)",0
"203 Estimating the parameters for these models is more difficult (and more computationally expensive) than with the models considered in the previous section: rather than simply being able to count the word pairs and alignment relationships and estimate the models directly, we must use an existing model to compute the expected counts for all possible alignments, and then use these counts to update the new model.7 This training strategy is referred to as expectationmaximization (EM) and is guaranteed to always improve the quality of the prior model at each iteration (Brown et al., 1993; Dempster et al., 1977)",0
"For example, the sentence I went to California last May would be marked for base NPs as: I went to California last May I 0 0 I B I indicating that the NPs are I, California and last May. This approach has been studied in (Ramshaw and Marcus, 1995)",0
"Incremental top-down and left-corner parsers have been shown to effectively (and efficiently) make use of non-local features from the left-context to yield very high accuracy syntactic parses (Roark, 2001; Henderson, 2003; Collins and Roark, 2004), and we will use such rich models to derive our scores",1
"The loglinear model weights are learned using Chiangs implementation of the maximum BLEU training algorithm (Och, 2003), both for the baseline, and the WSD-augmented system",0
"An example set of tags can be found in the Penn Treebank project (Marcus et al. , 1993)",0
"More details about the re-ranking algorithm are presented in (Ji et al. , 2006)",0
"They reported that their method is superior to BLEU (Papineni et al. , 2002) in terms of the correlation between human assessment and automatic evaluation",1
"Rather than explicit annotation, we could use latent annotations to split the POS tags, similarly to the introduction of latent annotations to PCFG grammars (Matsuzaki et al., 2005; Petrov et al., 2006)",0
"Minimum error rate training (MERT) with respect to BLEU score was used to tune the decoders parameters, and performed using the technique proposed in (Och, 2003)",0
"Furthermore, good results have been produced in other areas of NLP research using maximum entropy techniques (Berger et al. , 1996; Koeling, 2001; Ratnaparkhi, 1997a)",1
"As described in Section 4, we define the problem of term variation identifica1484 tion as a binary classification task, and build two types of classifiers according to the maximum entropy model (Berger et al., 1996) and the MART algorithm (Friedman, 2001), where all term similarity metrics are incorporated as features and are jointly optimized",0
"Self-training (Yarowsky, 1995) is a form of semi-supervised learning",0
"6.4 Feature Selection Methods A number of previous papers (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1998; Della Pietra, Della Pietra, and Lafferty 1997; McCallum 2003; Zhou et al. 2003; Riezler and Vasserman 2004) describe feature selection approaches for log-linear models applied to NLP problems",0
"In the field of statistical analysis of natural language data, it is common to use measures of lexical association, such as the informationtheoretic measure of mutual information, to extract useful relationships between words (e.g. Church and Hanks (1990))",0
"For the efficiency of minimum-error-rate training (Och, 2003), we built our development set (580 sentences) using sentences not exceeding 50 characters from the NIST MT-02 evaluation test data",0
"For a comparison, we also include the ROUGE-1 Fscores (Lin, 2004) of each system output against the human compressed sentences",0
 Motivation Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation researc,1
"Firstly, (Shen et al., 2008) resorted to heuristics to extract the Stringto-Dependency trees, whereas our approach employs the well formalized CCG grammatical theory",0
"We will briefly review the perceptron algorithm, and its convergence properties  see Collins (2002) for a full description",0
"Translation performance is measured using the automatic BLEU (Papineni et al. , 2002) metric, on one reference translation",0
"Barzilay & Lee (2003) employ Multiple Sequence Alignment (MSA, e.g., Durbin et al. , 1998) to align strings extracted from closely related news articles",0
"Unlike minimum error rate training (Och, 2003), our system is able to exploit large numbers of specific features in the same manner as static reranking systems (Shen et al. , 2004; Och et al. , 2004)",1
"Among recent top performing methods are Hidden Markov Models (Brants 2000), maximum entropy approaches (Ratnaparkhi 1996), and transformation-based learning (Brill 1994)",1
"P(ctd|C1,,Ct1) =producttextiP(Did|Dbtdd ,,Di1d ,C1,,Ct1) (3) The actions are also sometimes split into a sequence of elementary decisions Di = di1,,din, as discussed in (Titov and Henderson, 2007a)",0
"Our evaluation metric is case-insensitive BLEU-4 (Papineni et al. 2002), as defined by NIST, that is, using the shortest (as opposed to closest) reference sentence length for the brevity penalty",0
e also tested the flat syntactic feature set proposed in Luo and Zitouni (2005)s wor,0
"Discriminative methods such as Conditional Random Fields (CRFs) (Lafferty et al. , 2001), Semi-Markov Random Fields (Sarawagi and Cohen, 2004), and perceptrons (Collins, 2002a) have been popular approaches for sequence labeling because of their excellent performance, which is mainly due to their ability to incorporate many kinds of overlapping and non-independent features",1
ing and McKeown (2000) and Jing (2000) propose a cut-and-paste strategy as a computational process of automatic abstracting and a sentence reduction strategy to produce concise sentence,0
"Since it loosely links the two sentences syntactic structures, QG is well suited for problems like word alignment for MT (Smith and Eisner, 2006) and question answering (Wang et al., 2007)",0
"2 Learning algorithm The translation model is a standard linear model (Och and Ney, 2002), which we train using MIRA (Crammer and Singer, 2003; Crammer et al., 2006), following Watanabe et al",0
"We also implemented an averaged perceptron system (Collins, 2002) (another online learning algorithm) for comparison",0
"Such a coding procedure covers, for example, how segmentation of a corpus is performed, if multiple tagging is allowed and if so, is it unlimited or are there just certain combinations of tags not allowed, is look ahead permitted, etc For further information on coding procedures we want to refer to \[Dybkjmr et al.1998\] and for good examples of coding books see, for example, \[Carletta et al.1996\], \[Alexandersson et al.1998\], or \[Thym~-Gobbel and Levin1998\]",0
"5.3 Comparison with System Combination We re-implemented a state-of-the-art system combination method (Rosti et al., 2007)",0
"Given a sentence-pair (f,e), the most likely (Viterbi) word alignment is found as (Brown et al. , 1993): a = argmaxa P(f,a|e)",0
"Systems which are able to acquire a small number of verbal subcategorisation classes automatically from corpus text have been described by Brent (1991, 1993), and Ushioda et al",0
"Some of them use human reference translations, e.g., the BLEU method (Papineni et al. , 2002), which is based on comparison of N-gram models in MT output and in a set of human reference translations",0
ilingual Bracketing [Wu 1997] is one of the bilingual shallow parsing approaches studied for Chinese-English word alignmen,0
2002) and Turney (2002) classified sentiment polarity of reviews at the document leve,0
"By analyzing rhetorical discourse structure of aim, background, solution, etc. or citation context, we can obtain appropriate abstracts and the most influential contents from scientific articles (Teufel and Moens, 2002; Mei and Zhai, 2008)",0
"According to our experience, the best performance is achieved when the union of the source-to-target and target-to-source alignment sets (IBM models; Brown et al. [1993]) is used for tuple extraction (some experimental results regarding this issue are presented in Section 4.2.2)",1
"(1993) found that direct annotation takes twice as long as automatic tagging plus correction, for partof-speech annotation); and the output quality reflects the difficulty of the task (inter-annotator disagreement is on the order of 10%, as contrasted with the approximately 3% error rate reported for part-of-speech annotation by Marcus et al.)",0
"This is a particularly exciting area in computational linguistics as evidenced by the large number of contributions in these special issues: Biber (1993), Brent (1993), Hindle and Rooth (this issue), Pustejovsky et al",0
"Not only is this beneficial in terms of parsing complexity, but smaller rules can also improve a translation models ability to generalize to new data (Zhang et al., 2006)",0
"Inter-annotator agreement was determined for six pairs of two annotators each, resulting in kappa values (Carletta (1996)) ranging from 0.62 to 0.82 for the whole database (Carlson et al",0
"1 Introduction In this paper, we present an approach for extracting the named entities (NE) of natural language inputs which uses the maximum entropy (ME) framework (Berger et al. , 1996)",0
"Though taggers based on dependency networks (Toutanova et al. , 2003), SVM (Gimenez and M`arquez, 2003), MaxEnt (Ratnaparkhi, 1996), CRF (Smith et al. , 2005), and other methods may reach slightly better results, their train/test cycle is orders of magnitude longer",1
"4.1 Applications to phrase-based SMT Aphrase-basedtranslationmodelcanbeestimated in two stages: first a parallel corpus is aligned at the word-level and then phrase pairs are extracted (Koehn et al. , 2003)",0
"3.1 Paraphrase Identification A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay & Lee, 2003; Dolan & Brockett, 2004)",0
"3.2 Mapping Mapping the identified units (tokens or sequences) to their equivalents in the other language was achieved by training a new translation model (IBM 2) using the EM algorithm as described in (Brown et al. , 1993)",0
olan (1994) described a heuristic approach to forming unlabeled clusters of closely related senses in a MR,0
"We chose nouns that occur a minimum of 10 times in the corpus, have no undetermined translations and at least five different translations in the six nonEnglish languages, and have the log likelihood score of at least 18; that is: LL(T T, T S ) =  = 2 1 ij n* j * j*i ij n log  18 where n ij stands for the number of times T T and T S have been seen together in aligned sentences, n i* and n *j stand for the number occurrences of T T and T S, respectively, and n ** represents the total 4 We computed raw percentages only; common measures of annotator agreement such as the Kappa statistic (Carletta, 1996) proved to be inappropriate for our two-category (yesno) classification scheme",0
"In contrast, more recent research has focused on stochastic approaches that model discourse coherence at the local lexical (Lapata, 2003) and global levels (Barzilay and Lee, 2004), while preserving regularities recognized by classic discourse theories (Barzilay and Lapata, 2005)",0
"These methods often involve using a statistic such as 2 (Gale and Church, 1991) or the log likelihood ratio (Dunning, 1993) to create a score to measure the strength of correlation between source and target words",0
"Further, we can learn the channel probabilities in an unsupervised manner using a variant of the EM algorithm similar to machine translation (Brown et al. , 1993), and statistical language understanding (Epstein, 1996)",0
"Unlike our technique, in most cases researchers have focused on the scenario where labeled training data is available in both the source and the target domain (e.g., (Daume III, 2007; Chelba and Acero, 2004; Daume III and Marcu, 2006))",0
"3 Incremental Parsing Method Based on Adjoining Operation In order to avoid the problem of infinite local ambiguity, the previous works have adopted the following approaches: (1) a beam search strategy (Collins and Roark, 2004; Roark, 2001; Roark, 2004), (2) limiting the allowable chains to those actually observed in the treebank (Collins and Roark, 2004), and (3) transforming the parse trees with a selective left-corner transformation (Johnson and Roark, 2000) before inducing the allowable chains and allowable triples (Collins and Roark, 2004)",0
"5 Comparison with Previous Top Systems and Related Work In POS tagging, the previous best performance was reported by (Shen et al., 2007) as summarized in Table 7",1
"This latter point is a critical difference that contrasts to the major weakness of the work of (Liang et al. , 2006) which uses a top-N list of translations to select the maximum BLEU sentence as a target for training (so called local update)",1
"This can be done by smoothing the observed frequencies (Church and Mercer, 1993), or by class-based methods (Brown et al. , 1991; Pereira and Tishby, 1992; Pereira et ah, 1993; Hirschman, 1986; Resnik, 1992; Brill et ah, 1990; Dagan et al. , 1993)",0
"For each co-occurring pair of word types u and v, these likelihoods are initially set proportional to their co-occurrence frequency n(u,v) and inversely proportional to their marginal frequencies n(u) and n(v) z, following (Dunning, 1993) 2",0
"5 Bidirectional Sequence Classification Bidirectional POS tagging (Shen et al., 2007), the current state of the art for English, has some properties that make it appropriate for Icelandic",0
2006) and Blitzer et a,0
"In particular, this holds for the SCFG implementing Inversion 3For two sequences of numbers, the notation y < z stands for y  y,z  z : y < z. Transduction Grammar (Wu, 1997)",0
"1 Introduction Most of the current work in statistical machine translation builds on word replacement models developed at IBM in the early 1990s (Brown et al. , 1990, 1993; Berger et al. , 1994, 1996)",0
"Instead of analyzing sentences directly, AUCONTRAIRE relies on the TEXTRUNNER Open Information Extraction system (Banko et al., 2007; Banko and Etzioni, 2008) to map each sentence to one or more tuples that represent the entities in the sentences and the relationships between them (e.g., was born in(Mozart,Salzburg))",0
"2 Related Work Given its potential usefulness in coreference resolution, anaphoricity determination has been studied fairly extensively in the literature and can be classified into three categories: heuristic rule-based (e.g. Paice and Husk 1987; Lappin and Leass 1994; Kennedy and Boguraev 1996; Denber 1998; Vieira and Poesio 2000), statistics-based (e.g., Bean and Riloff 1999; Cherry and Bergsma 2005; Bergsma et al 2008) and learning-based (e.g. Evans 2001; Ng and Cardie 2002a; Ng 2004; Yang et al 2005; Denis and Balbridge 2007)",0
"Dirichlet priors can be used to bias HMMs toward more skewed distributions (Goldwater and Griffiths, 2007; Johnson, 2007), which is especially useful in the weakly supervised setting consideredhere",0
"The method thereby retains the full set of lexical entries of phrase-based systems (e.g., (Koehn et al., 2003)).1  The model allows a straightforward integration of lexicalized syntactic language modelsfor example the models of (Charniak, 2001)in addition to a surface language model",0
"Using linguistic principles to recover empty categories Richard CAMPBELL Microsoft Research One Microsoft Way Redmond, WA 98052 USA richcamp@microsoft.com Abstract This paper describes an algorithm for detecting empty nodes in the Penn Treebank (Marcus et al. , 1993), finding their antecedents, and assigning them function tags, without access to lexical information such as valency",0
"For instance, the HALOGEN statistical realizer [LangkildeGeary, 2002] underwent the most comprehensive evaluation of any surface realizer, which was conducted by measuring sentences extracted from the Penn TreeBank [Marcus et al. , 1993], converting them into its input formalism, and then producing output strings",0
"The terms graph-based and transition-based were used by McDonald and Nivre (2007) to describe the difference between MSTParser (McDonald and Pereira, 2006), which is a graph-based parser with an exhaustive search decoder, and MaltParser (Nivre et al., 2006), which is a transition-based parser with a greedy search decoder",0
"We compare semisupervised LEAF with a previous state of the art semi-supervised system (Fraser and Marcu, 2006b)",1
"We set all feature weights by optimizing Bleu (Papineni et al., 2002) directly using minimum error rate training (MERT) (Och, 2003) on the tuning part of the development set (dev-test2009a)",0
"Both systems rely on the OpenNlp maximum-entropy part-of-speech tagger and chunker (Ratnaparkhi, 1996), but KNOWITALL applies them to pages downloaded from the Web based on the results of Google queries, whereas KNOWITNOW applies them once to crawled and indexed pages.6 Overall, each of the above elements of KNOWITALL and KNOWITNOW are the same to allow for controlled experiments",0
"This corpus contains annotations of semantic PASs superimposed on the Penn Treebank (PTB) (Marcus et al. , 1993; Marcus et al. , 1994)",0
"A monotonic segmentation copes with monotonic alignments, that is, j < k ??aj < ak following the notation of (Brown et al. , 1993)",0
"The value of Dist(D(T)) can be defined in various ways, and they found that using log-likelihood ratio (see Dunning 1993) worked best which is represented as follows: 0 # log )(# log D K k TD k k i M ii i i M ii i  == , where k i and K i are the frequency of a word w i in D(W) and D 0 respectively, and {w 1,,w M } is the set of all words in D 0 . As stated in introduction, Dist(D(T)) is normalized by the baseline function, which is referred as B Dist () here",0
"To this end, the translational correspondence is described within a translation rule, i.e., (Galley et al. , 2004) (or a synchronous production), rather than a translational phrase pair; and the training data will be derivation forests, instead of the phrase-aligned bilingual corpus",0
"For evaluation we use a state-of-the-art baseline system (Moses) (Hoang and Koehn, 2008) which works with a log-linear interpolation of feature functions optimized by MERT (Och, 2003)",0
"For mutual information (MI), we use two different equations: one for two-element compound nouns (Church and Hanks, 1990) and the other for three-element compound nouns (Suet al. , 1994)",0
"The concept of these alignments is similar to the ones introduced by (Brown et al. , 1993), but we will use another type of dependence in the probability distributions",0
"Collins (1997) reports 88% labeled precision and recall on individual parse constituents on data from the Penn Treebank, roughly consistent with our finding of at least 13% error",0
"Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including """"crummy"""" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g",0
"There has thus been a trend recently towards robust wide-coverage semantic construction (e.g., (Bos et al., 2004; Zettlemoyer and Collins, 2007))",0
2002) and Turney (2002,0
"Also, we used Adwait Ratnaparkhis part-of-speech tagger (Ratnaparkhi, 1996) to tag unknown words in the test data",0
"In the nal step, we score our translations with 4-gram BLEU (Papineni et al. , 2002)",0
"4.2 Translation Results The evaluation metrics used in our experiments are WER (Word Error Rate), PER (Positionindependent word Error Rate) and BLEU (BiLingual Evaluation Understudy) (Papineni et al. , 2002)",0
"1 Introduction In latent variable approaches to parsing (Matsuzaki et al., 2005; Petrov et al., 2006), one models an observed treebank of coarse parse trees using a grammar over more refined, but unobserved, derivation trees",0
"Previous attempts have used, for instance, the similarities between case frames (Lin and Pan57 tel, 2001), anchor words (Barzilay and Lee, 2003; Shinyama et al. , 2002; Szepektor et al. , 2004), and a web-based method(Szepektor et al. , 2004;Geffet and Dagan, 2005)",0
"The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleubased minimum error rate training (Och, 2003) to optimize the weights of their log linear models feature functions (Och and Ney, 2002)",1
"All our experiments used the standard BIO encoding (Ramshaw and Marcus, 1995) with different feature sets and learning procedures",0
"One of our goals was to use for this study only information that could be annotated reliably (Passonneau and Litman 1993; Carletta 1996), as we believe this will make our results easier to replicate",0
"In the following experiments, we run two machine learning classifiers: Bayes Point Machines (BPM) (Herbrich et al., 2001), and the maximum entropy model (ME) (Berger et al., 1996)",0
illmann and Zhang (2006) avoided the problem by precomputing the oracle translations in advanc,0
"Examples of this are bilexical grammars--such as Eisner and Satta (1999), Charniak (1997), Collins (1997)--where the lexical heads of each constituent are annotated on both the rightand left-hand sides of the context-free rules, under the constraint that every constituent inherits the lexical head from exactly one of its children, and the lexical head of a POS is its terminal item",0
"Kupiec (1992) has proposed an estimation method for the N-gram language model using the Baum-Welch reestimation algorithm (Rabiner et al. , 1994) from an untagged corpus and Cutting et al",0
"Phrases of up to 10 in length on the French side were extracted from the parallel text, and minimum-error-rate training (Och 2003) was 8 We can train on the full training data shown if tighter constraints are placed on rule extraction for the United Nations data",0
"Our method is a natural extension of those proposed in (Brown et al. , 1992) and (Li and Abe, 1996), and overcomes their drawbacks while retaining their advantages",1
"Since in these LVCs the complement is a predicative noun in stem form identical to a verb, we form development and test expressions by combining give or take with verbs from selected semantic classes of Levin (1993), taken from Stevenson et al",0
"2 Inversion transduction grammars Inversion transduction grammars (ITGs) (Wu, 1997) are a notational variant of binary syntax-directed translation schemas (Aho and Ullman, 1972) and are usually presented with a normal form: A  [BC] A  BC A  e|f A  e| A  |f where A,B,C  N and e,f  T. The first production rule, intuitively, says that the subtree [[]B[]C]A in the source language translates into 62 a subtree [[]B[]C]A, whereas the second production rule inverts the order in the target language, i.e. [[]C[]B]A. The universal recognition problem of ITGs can be solved in time O(n6|G|) by a CYKstyle parsing algorithm with two charts",0
"Some researchers (Hindle, 1990; Grefenstette, 1994; Lin, 1998) classify terms by similarities based on their distributional syntactic patterns",0
"Example of such algorithms are (Pereira et al., 1993) and (Lin, 1998) that use syntactic features in the vector definition",0
"available): SCISSOR (Ge and Mooney, 2005), an integrated syntactic-semantic parser; KRISP (Kate and Mooney, 2006), an SVM-based parser using string kernels; WASP (Wong and Mooney, 2006; Wong and Mooney, 2007), a system based on synchronous grammars; Z&C (Zettlemoyer and Collins, 2007)3, a probabilistic parser based on relaxed CCG grammars; and LU (Lu et al., 2008), a generative model with discriminative reranking",0
"Even the 3 A demo of the parser can be found at http://lfgdemo.computing.dcu.ie/lfgparser.html creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level (Papineni et al. , 2002)",1
"This can be done in a supervised (Yarowsky, 1994), a semi-supervised (Yarowsky, 1995) or a fully unsupervised way (Pantel & Lin, 2002)",0
"Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrase-based approach (Koehn et al. , 2003)",1
"In the thriving area of research on automatic analysis and processing of product reviews (Hu and Liu 2004; Turney 2002; Pang and Lee 2005), little attention has been paid to the important task studied here  assessing review helpfulness",1
"However, it seems unrealistic to expect a one-size-fits-all approach to be achieve uniformly high performance across varied languages, and, in fact, it doesnt. Though the system presented in (Dasgupta and Ng, 2007) outperforms the best systems in the 2006 PASCAL challenge for Turkish and Finnish, it still does significantly worse on these languages than English (F-scores of 66.2 and 66.5, compared to 79.4)",1
"The publicly available Moses4 decoder is used for training and decoding (Koehn and Hoang, 2007)",0
"We carried out automatic evaluation of our summaries using ROUGE (Lin, 2004) toolkit, which has been widely adopted by DUC for automatic summarization evaluation",1
ur POS tagger is essentially the maximum entropy tagger by Ratnaparkhi (1996) retrained on the CTB-I dat,0
"The most obvious comparison takes on the form of a keyword analysis, which looks for the words that are significantly more frequent in the one corpus as compared to the other (Dunning, 1993; Scott, 1997; Rayson et al., 2004)",0
"The release has implementations for BLEU (Papineni et al. , 2002), WER and PER error criteria and it has decoding interfaces for Phramer and Pharaoh",0
"The mixture coefficients are trained in the usual way (minimum error-rate training, Och, 2003), so that the additional context is exploited when it is useful and ignored when it isnt. The paper proceeds as follows",0
"Moses uses standard external tools for some of these tasks, such as GIZA++ (Och and Ney, 2003) for word alignments and SRILM (Stolcke, 2002) for language modeling",0
"The corpus used for training our models was on the order of 100,000 words, whereas that used by (Brown et al., 1992) was around 1,000 times this size",0
"Table 2 shows the dependency accuracy, root accuracy and complete match scores for our best parser (Model 2 with label set B) in comparison with Collins (1997) (Model 3), Charniak (2000), and Yamada and Matsumoto (2003).5 It is clear that, with respect to unlabeled accuracy, our parser does not quite reach state-of-the-art performance, even if we limit the competition to deterministic methods such as that of Yamada and Matsumoto (2003)",0
"We build sentencespecific zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram language models, estimated using 4.7B words of English newswire text, and apply them to rescore either 10000-best lists generated by HCP or word lattices generated by HiFST",0
"Firstly, there is also H(RB) A(ADVP) declined H(VBD) H(VP) the dollar A(DT) H(NN) C(NP-SBJ) H(VP) H(S) Figure 2: A tree with constituents marked the top-down method, which is a version of the algorithm described by Hockenmaier et al (Hockenmaier et al. , 2000), but used for translating into simple (AB) CG rather than the Steedmans Combinatory Categorial Grammar (CCG) (Steedman, 1993)",0
"(Emami et al., 2007), (Brants et al., 2007), (Church et al., 2007)",0
"It is an implementation of Models 1-4 of Brown et al. \[1993\], where each of these models produces a Viterbi alignment",0
"Johnson (2007) compared two Bayesian inference algorithms, Variational Bayes and what we call here a point-wise collapsed Gibbs sampler, and found that Variational Bayes produced the best solution, and that the Gibbs sampler was extremely slow to converge and produced a worse solution than EM",0
ean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolutio,0
"The XEROX tagger comes with a list of built-in ending guessing rules (Cutting et al. ,1992)",0
"We held out 300 sentences for minimum error rate training (MERT) (Och, 2003) and optimised the parameters of the feature functions of the decoder for each experimental run",0
"Appendix A: Derivation of the Probability of RWE We take a noisy channel approach, which is a common technique in NLP (for example (Brown et al. , 1993)), including spellchecking (Kernighan et al. , 1990)",0
"The use of such relations (mainly relations between verbs or nouns and their arguments and modifiers) for various purposes has received growing attention in recent research (Church and Hanks, 1990; Zernik and Jacobs, 1990; Hindle, 1990)",0
"Bilingual bracketing methods were used to produce a word alignment in (Wu, 1997)",0
"The k-best list is very important for the minimum error rate training (Och, 2003a) which is used for tuning the weights  for our model",0
everal sentiment information retrieval models were proposed in the framework of probabilistic language models by Eguchi and Lavrenko (2006,0
"Collins and Roark (2004) saw a LFMS improvement of 0.8% over their baseline discriminative parser after adding punctuation features, one of which encoded the sentence-final punctuation",1
"This is analogous, and in a certain sense equivalent, to empirical risk minimization, which has been used successfully in related areas, such as speech recognition (Rahim and Lee, 1997), language modeling (Paciorek and Rosenfeld, 2000), and machine translation (Och, 2003)",0
"Motivated by the fact that non-syntactic phrases make non-trivial contribution to phrase-based SMT, the tree sequencebased translation model is proposed (Liu et al., 2007; Zhang et al., 2008a) that uses tree sequence as the basic translation unit, rather than using single sub-tree as in the STSG",0
"Their weights are optimized w.r.t. BLEU score using the algorithm described in (Och, 2003)",0
"(Krenn, 2000b; Smadja, 1993))",0
"However, the only known work which automates part of a customer service center using natural language dialogue is the one by Chu-Carroll and Carpenter (1999)",1
"Other recent work has incorporated constituent and dependency subtrees into the translation rules used by phrase-based systems (Galley et al., 2004; Quirk et al., 2005)",0
"This time the chunker achieved a F~=l score of 93.81 which is half a point better than the results obtained by (Ramshaw and Marcus, 1995): 93.3 (other chunker rates for this data: accuracy: 98.04%; precision: 93.71%; recalh 93.90%)",1
"Specifically, Kim and Hovy (2007) identify which political candidate is predicted to win by an opinion posted on a message board and aggregate opinions to correctly predict an election result",0
"Uses Maximum Entropy (Berger et al., 1996) classification, trained on JNLPBA (Kim et al., 2004) (NER)",0
"Generative and discriminative models have been comparedanddiscussedagreatdeal(NgandJordan, 2002), including for NLP models (Johnson, 2001; Klein and Manning, 2002)",0
"(2001)) and unsupervised approaches (e.g. , Cardie and Wagstaff (1999), Bean and Riloff (2004))",0
"Most of the reported work on paraphrase generation from arbitrary input sentences uses machine learning techniques trained on sentences that are known or can be inferred to be paraphrases of each other (Bannard and Callison-Burch, 2005; Barzilay and Lee, 2003; Barzilay and McKeown, 2001; Callison-Burch et al. , 2006; Dolan et al. , 2004; Ibrahim et al. , 2003; Lin and Pantel, 2001; Pang et al. , 2003; Quirk et al. , 2004; Shinyama et al. , 2002)",0
 Background The natural language generator used in our experiments is the WSJ-trained system described in Cahill and van Genabith (2006) and Hogan et a,0
"This is based on the idea from (Ratnaparkhi, 1996) that rare words in the training set are similar to unknown words in the test set, and can be used to learn how to tag the unknown words that will be encountered during testing",0
"In the last decade or so research on lexical semantics has focused more on sub-problems like word sense disambiguation (Yarowsky, 1995; Stevenson and Wilks, 2001), named entity recognition (Collins and Singer, 1999), and vocabulary construction for information extraction (Riloff, 1996)",0
"The boosting approach to ranking has been applied to named entity segmentation (Collins 2002a) and natural language generation (Walker, Rambow, and Rogati 2001)",0
"Lacking an automatic method, recent WSD works (Bruce and Wiebe 1995; Luk 1995; Yarowsky 1995) still resort to human intervention to identify and group closely related senses in an MRD",0
"We split the treebank into training (sections 0-18), development (sections 1921) and test (sections 22-24) as in (Collins, 2002)",0
"We have been using the output of word_align, a robust alignment program that proved useful for bilingual concordancing of noisy texts (Dagan et al. , 1993)",0
"There are also automatic methods for summary evaluation, such as ROUGE (Lin, 2004), which gives a score based on the similarity in the sequences of words between a human-written model summary  and  the  machine  summary",0
"Both models are based on IBM translation model 2 (Brown et al. , 1993) which has the 49 property that it generates tokens independently",0
"Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar",0
"Introduction Recently, there has been an increased interest in approaches to automatically learning to recognize shallow linguistic patterns in text \[Ramshaw and Marcus, 1995, Vilain and Day, 1996, Argamon et al. , 1998, Buchholz, 1998, Cardie and Pierce, 1998, Veenstra, 1998, Daelemans et aI",0
"(Koehn et al., 2003)), in which translation and language models are trainable separately too",0
"Our learning algorithm stems from Perceptron training in (Collins, 2002)",0
"The table also shows the -score, which is another commonly used measure for inter-annotator agreement [Carletta, 1996]",0
"4.2 A ROUGE Based Approach ROUGE (Lin, 2004) is the standard automatic evaluation metric in the Summarization community",0
"(General grammars with infinite numbers of nonterminals were studied by (Liang et al., 2007b))",0
"Here, ppicker shows the accuracy when phrases are extracted by using the N-best phrase alignment method described in Section 4.1, while growdiag-final shows the accuracy when phrases are extracted using the standard phrase extraction algorithm described in (Koehn et al. , 2003)",0
"(Pang & Lee, 2004; Aue & Gamon, 2005)",0
"Automatically creating or extending taxonomies for specific domains is then a very interesting area of research (OSullivan et al., 1995; Magnini and Speranza, 2001; Snow et al., 2006)",1
"(Cahill et al. , 2004b) provide four sets of annotation principles, one for non-coordinate configurations, one for coordinate configurations, one for traces (long distance dependencies) and a final catch all and clean up phase",0
"3 Baseline MT System The phrase-based SMT system used in our experiments is Moses, phrase translation pro ing probabilities, and languag ties are combined in the log-linear model to obtain the best translation best e  of the source sentence f :  =  = M p | )(maxarg fee ebest  (2) m mm h 1 ,(maxarg f)e e  The weights are set by a discriminative training method using a held-out data set as describ in (Och, 2003)",0
ne such model is the IBM Model 1 (Brown et al. 1993,0
"1 Introduction Maximum Entropy (ME) modeling has received a lot of attention in language modeling and natural language processing for the past few years (e.g. , Rosenfeld, 1994; Berger et al 1996; Ratnaparkhi, 1998; Koeling, 2000)",0
"The output of a contextfree parser, such as that of Collins (1997) or Charniak (2000), can be transformed into a sequence of shallow constituents for comparison with the output of a shallow parser",0
"For a detailed description for Model 4 the reader is referred to (Brown et al. , 1993)",0
"Only recently have robust knowledge-based methods for some of these tasks begun to appear, and their performance is still not very good, as seen above in our discussion of using WordNet as a semantic network; 33 as for checking the plausibility of a hypothesis on the basis of causal knowledge about the world, we now have a much better theoretical grasp of how such inferences could be made (see, for example, Hobbs et al. \[1993\] and Lascarides and Asher \[1993\]), but we are still quite a long way from a general inference engine",0
"(2005) and compare with results reported by HK06 (Haghighi and Klein, 2006) and CRR07 (Chang et al., 2007)",0
"Volume 17, Number 1 March 1991 References Lakoff, George and Johnson, Mark Metaphors We Live 8y University of Chicago Press 1980 MADCOW Committee (Hirschman, Lynette et al) Multi-Site Data Collection for a Spoken Language Corpus in Proceedings Speech and Natural Language Workshop February 1992 Grice, H. P. Logic and Conversation in P. Cole and J. L. Morgan, Speech Acts, New York: Academic Press, 1975 Pustejovsky, James The Generative Lexicon Computational Linguistics Volume 17, Number 4 December 1991 Hobbs, Jerry R. and Stickel, Mark Interpretation as Abduction in Proceedings of the 26th ACL June 1988 Bobrow, R. , Ingria, R. and Stallard, D. The Mapping Unit Approach to Subcategorization in Proceedings Speech and Natural Language Workshop February 1991 Hobbs, Jerry R. , and Martin, Paul Local Pragmatics in Proceedings, 10th International Joint Conference on Artificial Intelligence (IJCAI-87)",0
"108 To follow related work and to focus on the effects of the language model, we present translation resultsunderaninversiontransductiongrammar(ITG) translation model (Wu, 1997) trained on the Europarl corpus (Koehn, 2005), described in detail in Section 3, and using a trigram language model",0
"Some are the result of inconsistency in labeling in the training data (Ratnaparkhi 1996), which usually reflects a lack of linguistic clarity or determination of the correct part of speech in context",1
"In the training phase, bilingual parallel sentences are preprocessed and aligned using alignment algorithms or tools such as GIZA++ (Och and Ney, 2003)",0
"For example, both Haghighi and Klein (2006) and Mann and McCallum (2008) have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate",0
"(Haghighi and Klein, 2006) also worked on one of our data sets",0
"Meanwhile, some learning algorithms, like maximum likelihood for conditional log-linear models (Lafferty et al., 2001), unsupervised models (Pereira and Schabes, 1992), and models with hidden variables (Koo and Collins, 2005; Wang et al., 2007; Blunsom et al., 2008), require summing over the scores of many structures to calculate marginals",0
his is similartothegraphconstructionmethodofHughes and Ramage (2007) and Rao et a,0
"When labeled training data is available, we can use the Maximum Entropy principle (Berger et al. , 1996) to optimize the  weights",0
", 1989), e.g, lexicography (Church and Hanks, 1990), information retrieval (Salton, 1986a), text input (Yamashina and Obashi, 1988), etc. This paper will touch on its feasibility in topic identification",0
"3 Maximum Entropy ME models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence, but otherwise is as uniform as possible (Berger et al. 1996)",0
.9595 0.9590 0.9611 0.9085 0.9134 0.9152 Table 8: Comparison of F1 results of our baseline model with Nakagawa and Uchimoto (2007) and Zhang and Clark (2008) on CTB 3.,0
"Previous studies called the class of algorithms illustrated in Figure 2 cautious or sequential because in each iteration they acquire 1 or a small set of rules (Abney, 2004; Collins and Singer, 1999)",0
"Recently, some generic methods were proposed to handle context-sensitive inference (Dagan et al., 2006; Pantel et al., 2007; Downey et al., 2007; Connor and Roth, 2007), but these usually treat only a single aspect of context matching (see Section 6)",0
"5 Results We present results that compare our system against the baseline Pharaoh implementation (Koehn et al. , 2003a) and MER training scripts provided for this workshop",0
"(Dave et al. , 2003; Pang and Lee, 2004; Turney, 2002))",0
"In order to incorporate a new dependency which contains extra information other than the bilingual sentence pair, we modify Eq.2 by adding a new variable v: Pr(a|e,f,v) = exp[ summationtextM m=1 mhm(a,e,f,v)]summationtext aprime exp[ summationtextM m=1 mhm(aprime,e,f,v)](4) Accordingly, we get a new decision rule: a = argmax a braceleftbigg Msummationdisplay m=1 mhm(a,e,f,v) bracerightbigg (5) Note that our log-linear models are different from Model 6 proposed by Och and Ney (2003), which defines the alignment problem as finding the alignment a that maximizes Pr(f, a|e) given e. 3 Feature Functions In this paper, we use IBM translation Model 3 as the base feature of our log-linear models",0
"1 Introduction Base noun phrases (baseNPs), broadly the initial portions of non-recursive noun phrases up to the head (Ramshaw and Marcus, 1995), are valuable pieces of linguistic structure which minimally extend beyond the scope of named entities",0
"Because of its central role in building machine translation systems and because of the complexity of the task, sub-sentential alignment of parallel corpora continues to be an active area of research (e.g. , Moore et al. , 2006; Fraser and Marcu, 2006), and this implies a continuing demand for manually created or human-verified gold standard alignments for development and evaluation purposes",0
"During training each example is broken into elementary trees using head rules and argument/adjunct rules similar to those of (Collins, 1997)",0
"As a first step, SemEval2007 Task 4 offered many useful insights into the performance of different approaches to semantic relation classification; it has also motivated followup research (Davidov and Rappoport, 2008; KatrenkoandAdriaans, 2008; NakovandHearst, 2008; O Seaghdha and Copestake, 2008)",0
"The models are based on a maximum entropy framework (Ratnaparkhi, 1996; Xue and Shen, 2003)",0
"In a next step, chunk information was added by a rule-based language-independent chunker (Macken et al., 2008) that contains distituency rules, which implies that chunk boundaries are added between two PoS codes that cannot occur in the same constituent",0
"The model presented above is based on our previous work (Jiang and Zhai, 2007c), which bears the same spirit of some other recent work on multitask learning (Ando and Zhang, 2005; Evgeniou and Pontil, 2004; Daume III, 2007)",0
"Since then, supervised learning from sense-tagged corpora has since been used by several researchers: Zernik (1990, 1991), Hearst (1991), Leacock, Towell, and Voorhees (1993), Gale, Church, and Yarowsky (1992d, 1993), Bruce and Wiebe (1994), Miller et al",0
 The Log-Likelihood-Ratio Association Measure We base all our association-based word-alignment methods on the log-likelihood-ratio (LLR) statistic introduced to the NLP community by Dunning (1993,0
"The IBM source-channel model for statistical machine translation (P. Brown et al. , 1993) plays a central role in our system",1
"In a different work, Banerjee and Lavie (2005) argued that the measured reliability of metrics can be due to averaging effects but might not be robust across translations",0
"Thus, we used the five taggers, MBL (Daelemans et al., 1996), MXPOST (Ratnaparkhi, 1996), fnTBL (Ngai and Florian, 2001), TnT, and IceTagger3, in the same manner as described in (Loftsson, 2006), but with the following minor changes",0
"As with the graph-based parser, we use the discriminative perceptron (Collins, 2002) to train the transition-based model (see Figure 5)",0
"Moreover, it was (without imposing determinism) the inference technique employed in (Vidal et al. , 1993)",0
"For extracting simple noun phrases we first used Ramshaw and Marcuss base NP chunker (Ramshaw and Marcus, 1995)",0
"The first, Powells method, was advocated by Och (2003) when MERT was first introduced for statistical machine translation",0
"Variations of SCFGs go back to Aho and Ullman (1972)s Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu (1997), which restrict grammar rules to be binary, the synchronous grammars in Chiang (2005), which use only a single nonterminal symbol, and the Multitext Grammars in Melamed (2003), which allow independent rewriting, as well as other tree-based models such as Yamada and Knight (2001) and Galley et al",0
"A large database of human judgments might also be useful as an objective function for minimum error rate training (Och, 2003) or in other system development tasks",0
"This score measures the precision of unigrams, bigrams, trigrams and fourgrams with respect to a reference translation with a penalty for too short sentences (Papineni et al. , 2002)",0
"It has been shown that the methods can be ported to other languages and treebanks (Burke et al. , 2004; Cahill et al. , 2003), including Cast3LB (ODonovan et al. , 2005)",0
"We can then use this newly identified set to: (1) use Turneys method to find the orientation for the terms and employ the terms and their scores in a classifier, and (2) use Turneys method to find the orientation for the terms and add the new terms as additional seed terms for a second iteration As opposed to Turney (2002), we do not use the web as a resource to find associations, rather we apply the method directly to in-domain data",0
"We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust nonlexical syntax/alignment model with a(n optional)lexical-semantics-drivenlog-linear model",0
"We conclude with some challenges that still remain in applying proactive learning for MT. 2 Syntax Based Machine Translation In recent years, corpus based approaches to machine translation have become predominant, with Phrase Based Statistical Machine Translation (PBSMT) (Koehn et al., 2003) being the most actively progressing area",1
"Since we also adopt a linear scoring function in Equation (3), the feature weights of our combination model can also be tuned on a development data set to optimize the specified evaluation metrics using the standard Minimum Error Rate Training (MERT) algorithm (Och 2003)",0
"3 Phrase-Based SMT According to the translation model presented in (Koehn et al. , 2003), given a source sentence f, the best target translation best e can be obtained according to the following model )( )()|(maxarg )|(maxarg e e e eef fee length LM best pp p = = (1) Where the translation model )|( efp can be decomposed into  =  = I i i iii i i II aefpbadef efp 1 1 1 1 ),|()()|( )|(   w (2) Where )|( i i ef and )( 1  ii bad denote phrase translation probability and distortion probability, respectively",0
"Preparing an aligned abbreviation corpus, we obtain the optimal combination of the features by using the maximum entropy framework (Berger et al., 1996)",0
"Apart from this, the module is a straightforward implementation of (Ramshaw and Marcus, 1995), which in turn adapts (Brill, 1993) for syntactic chunking",0
"On a separate note, previous research has explicitly studied sentiment analysis as an application of transfer learning (Blitzer et al., 2007)",0
"Full discriminative parser training faces signi cant algorithmic challenges in the relationship between parsing alternatives and feature values (Geman and Johnson, 2002) and in computing feature expectations",0
"However, in yet unpublished work we found that at least for the computation of synonyms and related words neither syntactical analysis nor singular value decomposition lead to significantly better results than the approach described here when applied to the monolingual case (see also Grefenstette, 1993), so we did not try to include these methods in our system",0
"section 20 Majority voting (Mufioz et al. , 1999) (Tjong Kim Sang and Veenstra~ 1999) (Ramshaw and Marcus, 1995) (Argarnon et al. , 1998) accuracy precision O:98.10% C:98.29% 93.63% O:98.1% C:98.2% 93.1% 97.58% 92.50% 97.37% 91.80% 91.6% recall FZ=I 92.89% 93.26 92.4% 92.8 92.25% 92.37 92.27% 92.03 91.6% 91.6 section 00 accuracy precision Majority voting 0:98.59% C:98.65% 95.04% r (Tjong Kim Sang and Veenstra, 1999) 98.04% 93.71% (Ramshaw and Marcus, 1995) 97.8% 93.1% recall FB=I 94.75% 94.90 93.90% 93.81 93.5% 93.3 Table 3: The results of majority voting of different data representations applied to the two standard data sets put forward by (Ramshaw and Marcus, 1995) compared with earlier work",0
"Previously published approaches to reducing the rule set include: enforcing a minimum span of two words per non-terminal (Lopez, 2008), which would reduce our set to 115M rules; or a minimum count (mincount) threshold (Zollmann et al., 2008), which would reduce our set to 78M (mincount=2) or 57M (mincount=3) rules",0
"Probably the most widely used feature weighting function is (point-wise) Mutual Information (MI) (Church and Patrick 1990; Hindle 1990; Luk 1995; Lin 1998; Gauch, Wang, and Rachakonda 1999; Dagan 2000; Baroni and Vegnaduzzo 2004; Chklovski and Pantel 2004; Pantel and Ravichandran 2004; Pantel, Ravichandran, and Hovy 2004; Weeds, Weir, and McCarthy 2004), dened by: weight MI (w,f)=log 2 P(w,f) P(w)P(f) (1) We calculate the MI weights by the following statistics in the space of co-occurrence instances S: weight MI (w,f)=log 2 count(w,f) nrels count(w) count(f) (2) where count(w,f) is the frequency of the co-occurrence pair w,f  in S, count(w)and count(f) are the independent frequencies of w and f in S,andnrels is the size of S.High MI weights are assumed to correspond to strong wordfeature associations",1
"The classical Bayes relation is used to introduce a target language model (Brown et al. , 1993): e = argmaxe Pr(e|f) = argmaxe Pr(f|e)Pr(e) where Pr(f|e) is the translation model and Pr(e) is the target language model",0
"With hand-labeled data, {m} can be learnt via generalized iterative scaling algorithm (GIS) (Darroch and Ratcliff, 1972) or improved iterative scaling (IIS) (Berger 367 et al. , 1996)",0
"While the need for annotation by multiple raters has been well established in NLP tasks (Carletta, 1996), most previous work in error detection has surprisingly relied on only one rater to either create an annotated corpus of learner errors, or to check the systems output",0
"For the efficiency of minimum-errorrate training (Och, 2003), we built our development set (580 sentences) using sentences not exceeding 50 characters from the NIST MT-02 evaluation test data",0
"For example, 10 million words of the American National Corpus (Ide et al. , 2002) will have manually corrected POS tags, a tenfold increase over the Penn Treebank (Marcus et al. , 1993), currently used for training POS taggers",1
"Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences",0
"Research on the automatic classification of movie or product reviews as positive or negative (e.g. , (Pang et al. , 2002; Morinaga et al. , 2002; Turney and Littman, 2003; Nasukawa and Yi, 2003; Mullen and Collier, 2004; Beineke et al. , 2004; Hu and Liu, 2004)) is perhaps the most similar to our work",0
"1 Introduction Word compositions have long been a concern in lexicography(Benson et al. 1986; Miller et al. 1995), and now as a specific kind of lexical knowledge, it has been shown that they have an important role in many areas in natural language processing, e.g., parsing, generation, lexicon building, word sense disambiguation, and information retrieving, etc.(e.g. , Abney 1989, 1990; Benson et al. 1986; Yarowsky 1995; Church and Hanks 1989; Church, Gale, Hans, and Hindle 1989)",0
"This analysis depends on the SPECIALIST Lexicon and the Xerox part-of-speech tagger (Cutting et al. , 1992) and provides simple noun phrases that are mapped to concepts in the UMLS Metathesaurus using MetaMap (Aronson, 2001)",0
"In addition, we developed a word clustering procedure (based on a standard approach (Brown et al. , 1992)) that optimizes conditional word clusters",0
"In one experiment, it has to be performed on the basis of the gold-standard, assumed-perfect POS taken directly from the training data, the Penn Treebank (Marcus et al. , 1993), so as to abstract from a particular POS tagger and to provide an upper bound",0
"Jing and McKeown (2000) have proposed a rule-based algorithm for sentence combination, but no results have been reported",1
"This includes the automatic generation of sense-tagged data using monosemous relatives (Leacock et al. , 1998; Mihalcea and Moldovan, 1999; Agirre and Martinez, 2004), automatically bootstrapped disambiguation patterns (Yarowsky, 1995; Mihalcea, 2002), parallel texts as a way to point out word senses bearing different translations in a second language (Diab and Resnik, 2002; Ng et al. , 2003; Diab, 2004), and the use of volunteer contributions over the Web (Chklovski and Mihalcea, 2002)",0
"For example, when applying their approach to a different domain with somewhat less rigid syntax, Zettlemoyer and Collins (2007) need to introduce new combinators and new forms of candidate lexical entries",0
"The closest work is that of Jing and McKeown (1999) and Daume III and Marcu (2005), in which multiple sentences are processed, with fragments within them being recycled to generate the novel generated text",0
"Additionally, some research has explored cutting and pasting segments of text from the full document to generate a summary (Jing and McKeown 2000)",0
"7 Discussion As we mentioned, there are some algorithms similar to ours (Collins and Roark, 2004; Daume III and Marcu, 2005; McDonald and Pereira, 2006; Liang et al. , 2006)",0
"(Ramshaw and Marcus, 1995) approached chucking by using Transformation Based Learning(TBL)",0
"Following initial work by (Sparck Jones, 1964) and (Grefenstette, 1994), an early, online distributional thesaurus presented in (Lin, 1998) has been widely used and cited, and numerous authors since have explored thesaurus properties and parameters: see survey component of (Weeds and Weir, 2005)",0
"The frequency counts of dependency relationships are filtered with the loglikelihood ratio (Dunning, 1993)",0
"Among these measures, the most important are Wu & Palmers (Wu and Palmer, 1994), Resniks (Resnik, 1995) and Lins (Lin, 1998)",1
"In our experiments, the class assignment is performed by maximizing the mutual information between adjacent phrases, following the line described in (Brown 301 et al. , 1992), with only the modification that candidates to clustering are phrases instead of words",0
"Our systems use both corpus-based and knowledge-based approaches: Maximum Entropy(ME) (Lau et al. , 1993; Berger et al. , 1996; Ratnaparkhi, 1998) is a corpus-based and supervised method based on linguistic features; ME is the core of a bootstrapping algorithm that we call re-training inspired  This paper has been partially supported by the Spanish Government (CICyT) under project number TIC-2003-7180 and the Valencia Government (OCyT) under project number CTIDIB-2002-151 by co-training (Blum and Mitchell, 1998); Relevant Domains (RD) (Montoyo et al. , 2003) is a resource built from WordNet Domains (Magnini and Cavaglia, 2000) that is used in an unsupervised method that assigns domain and sense labels; Specification Marks(SP) (Montoyo and Palomar, 2000) exploits the relations between synsets stored in WordNet (Miller et al. , 1993) and does not need any training corpora; Commutative Test (CT) (Nica et al. , 2003), based on the Sense Discriminators device derived from EWN (Vossen, 1998), disambiguates nouns inside their syntactic patterns, with the help of information extracted from raw corpus",0
"Correspondences between MALTUS and other tagsets (Klein and Soria, 1998) were also provided (Popescu-Belis, 2003)",0
ntroduction The Penn Treebank (Marcus et al. 1993) initiated a new paradigm in corpus-based researc,1
"The observation that shallow syntactic information can be extracted using local information by examining the pattern itself, its nearby context and the local part-of-speech information has motivated the use of learning methods to recognize these patterns (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al. , 1998; Cardie and Pierce, 1998)",0
"The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al. , 1993; Ney et al. , 2000)",0
"85 Recently some alignment evaluation metrics have been proposed which are more informative when the alignments are used to extract translation units (Fraser and Marcu, 2006; Ayan and Dorr, 2006)",0
"The leader of the pack is the MXPOST tagger (Ratnaparkhi, 1996)",0
"To set the weights, m, we performed minimum error rate training (Och, 2003) on the development set using Bleu (Papineni et al. , 2002) as the objective function",0
"The tree is produced by a state-of-the-art dependency parser (McDonald et al. , 2005) trained on the Wall Street Journal Penn Treebank (Marcus et al. , 1993)",0
"We also plan to employ this evaluation metric as feedback in building dialogue coherence models as is done in machine translation (Och, 2003)",0
"Also relevant is previous work that applied machine learning approaches to MT evaluation, both with human references (Corston-Oliver et al. , 2001; Kulesza and Shieber, 2004; Albrecht and Hwa, 2007; Liu and Gildea, 2007) and without (Gamon et al. , 2005)",0
"5 Combining In-Domain and Out-of-Domain Data for Training In this section, we will first introduce the AUGMENT technique of Daume III (2007), before showing the performance of our WSD system with and without using this technique",0
"The published F score for voted perceptron is 93.53% with a different feature set (Collins, 2002)",0
"In (Liang et al., 2006) a standard phrase-based model is augmented with more than a million features whose weights are trained discriminatively by a variant of the perceptron algorithm",0
"We estimate loss gradients (Equation 13) using a sample of the inference set, which gives a 100-fold increase in training speed (Turian & Melamed, 2006)",0
his upper bound is consistent with the upper limit of 50% found by Daume III and Marcu (2005) which takes into account stemming difference,0
"We prepare the corpus by passing it through Adwait Ratnaparkhis part-of-speech tagger (Ratnaparkhi, 1996) (trained on the Penn Treebank WSJ corpus) and then running Steve Abneys chunker (Abney, 1997) over the entire text",0
"As Carletta (1996) notes, many tasks in computational linguistics are simply more difficult than the content analysis classifications addressed by Krippendorff, and according to Fleiss (1981), kappa values between .4 and .75 indicate fair to good agreement anyhow",0
"The parser is coupled with an on-line averaged perceptron (Collins, 2002) as the learning method",0
"2.2 Learning Algorithm For learning coreference decisions, we used a Maximum Entropy (Berger et al. , 1996) model",0
"However, (Fraser and Marcu, 2007a) show that, in phrase-based translation, improvements in AER or f-measure do not necessarily correlate with improvements in BLEU score",0
"The intuition is that the produced clusters will be less sense-conflating than those produced by other graph-based approaches, since collocations provide strong and consistent clues to the senses of a target word (Yarowsky, 1995)",0
"Thus, the WSJ+NANC model has better oracle rates than the WSJ model (McClosky et al. , 2006) for both the WSJ and BROWN domains",0
"Feature-based methods (Jiang and Zhai, 2007; Kambhatla, 2004; Zhou et al., 2005) use pre-defined feature sets to extract features to train classification models",0
"String alignment with synchronous grammars is quite expensive even for simple synchronous formalisms like ITG (Wu, 1997)but Duchi et al",1
"They may rely only on this information (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003)), or they may combine it with additional information as well (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Wilson et al., 2005a))",0
" The piecewise linearity observation made in (Papineni et al. , 2002) is no longer applicable since we cannot move the log operation into the expected value",0
"In some recent grammar induction and MT work (Haghighi and Klein, 2006; Quirk et al., 2005) it has been shown that even a small amount of knowledge about a language, in the form of grammar fragments, treelets or prototypes, can go a long way in helping with the induction of a grammar from raw text or with alignment of parallel corpora",0
"However, this is not unprecedented: discriminatively weighted generative models have been shown to outperform purely discriminative competitors in various NLP classification tasks (Raina et al., 2004; Toutanova, 2006), and remain the standard approach in statistical translation modeling (Och, 2003)",1
"A later study (Pang and Lee, 2004) found that performance increased to 87.2% when considering only those portions of the text deemed to be subjective",1
"(Wick et al. , 2006) report extracting database records by learning record field compatibility",0
"Word alignment was carried out by running Giza++ implementation of IBM Model 4 initialized with 5 iterations of Model 1, 5 of the HMM aligner, and 3 iterations of Model 4 (Och and Ney, 2003) in both directions and then symmetrizing using the grow-diag-final-and heuristic (Koehn et al., 2003)",0
"In particular, ROUGE-2 is the recall in bigrams with a set of human-written abstractive summaries (Lin, 2004)",0
"1 Introduction Empty categories (also called null elements) are used in the annotation of the PENN treebank (Marcus et al. , 1993) in order to represent syntactic phenomena like constituent movement (e.g. whextraction), discontinuous constituents, and missing elements (PRO elements, empty complementizers and relative pronouns)",0
