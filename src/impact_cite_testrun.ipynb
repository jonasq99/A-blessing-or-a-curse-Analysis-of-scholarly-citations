{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import torch - might be needed\n",
    "#import torch.nn as nn\n",
    "\n",
    "from datasets import DatasetDict, Dataset, load_dataset, concatenate_datasets\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "\n",
    "from utils import create_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('Velkymoss/impact_cite_v0.11',num_labels=2, use_auth_token=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_seqs(examples):\n",
    "    return tokenizer(examples['citation'], padding = True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Use create_data function to create a dictionary of pandas dataframes \n",
    "# TODO The function creates to new colums named 'context' and 'footenote_text' to the dataframes.\n",
    "# Add a new column to each dataframe named 'citation' which is the concatenates the strings of the 'context' and 'footnote_text' columns\n",
    "# TODO Use Dataset.from_pandas() on each dataframe to convert them to HuggingFace Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Use tokenize_seqs function to tokenize each dataset in the dictionary and use map() with batched=True to apply it to all the datasets\n",
    "# TODO Make sure the column labels is named 'labels' otherwise use tokenized_dataset.rename_column('label', 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Loop over all rows in the dataset, get the prediction and store it to later calculate the F1 score and accuracy per class\n",
    "# TODO Calculate the F1 score and accuracy per class \n",
    "# TODO Plot the accuracy per class for each class\n",
    "\n",
    "# TODO Repeat this process for a number of different configurations of numbers of tokens as context around the citation\n",
    "# TODO Store the calculated results in a file. Store the plots as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If stuck on using the Transformers library, check out the documentation here: https://huggingface.co/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
