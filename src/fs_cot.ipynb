{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "#from text_extraction import file_finder, TextExtraction\n",
    "from data_creator import create_data\n",
    "from utils import get_context, few_shot_cot, get_fewshot_cot_examples, get_label\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = create_data(500, 30)\n",
    "\n",
    "def filter_label(dataframes_dict: dict[pd.DataFrame], label: int) -> pd.DataFrame:\n",
    "    # Create an empty list to store filtered DataFrames\n",
    "    filtered_dataframes = []\n",
    "    \n",
    "    for key, df in dataframes_dict.items():\n",
    "        if 'Label' in df.columns:\n",
    "            filtered_df = df[df['Label'] == label]\n",
    "            \n",
    "            filtered_dataframes.append(filtered_df)\n",
    "    \n",
    "    result_df = pd.concat(filtered_dataframes, ignore_index=True)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def sample_data(dataframe):\n",
    "    sampled_df = dataframe.sample(n=100, random_state=42) \n",
    "    return sampled_df\n",
    "\n",
    "\n",
    "\n",
    "opinionated_data = filter_label(df_dict, 1)\n",
    "neutral_data = sample_data(filter_label(df_dict, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([opinionated_data, neutral_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199/199 [10:50<00:00,  3.27s/it]\n"
     ]
    }
   ],
   "source": [
    "def get_precictions(df):\n",
    "    predictions = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        #name = df[\"Authors\"].iloc[i]\n",
    "        title = df[\"Title\"].iloc[i]\n",
    "        context = get_context(df[\"context\"].iloc[i])\n",
    "        footnote = df[\"footnote_text\"].iloc[i]\n",
    "        #pred = context_sentiment(context)\n",
    "        pred = few_shot_cot(examples = get_fewshot_cot_examples(), citation=context, title=title, footnote=footnote)\n",
    "        \n",
    "        #while pred != \"0\" and pred != \"1\":\n",
    "            #print(\"Retrying prediction...\")\n",
    "            #pred = few_shot_cot(examples = get_fewshot_cot_examples(), citation=context, title=title, footnote=footnote)\n",
    "        pred = get_label(pred)\n",
    "        predictions.append(pred)\n",
    "        #predictions = [int(i) for i in predictions]           \n",
    "    return predictions\n",
    "\n",
    "predictions = get_precictions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6282722513089004\n",
      "Accuracy for label 0: 0.68\n",
      "Accuracy for label 1: 0.6\n"
     ]
    }
   ],
   "source": [
    "labels = df[\"Label\"].tolist()\n",
    "\n",
    "def calculate_accuracy_per_label(predictions, labels, label_value):\n",
    "    # Create a boolean array indicating whether the label matches the specified value\n",
    "    label_matches = [l == label_value for l in labels]\n",
    "  \n",
    "    # Extract predictions for instances where the label matches the specified value\n",
    "    matched_predictions = [p for i, p in enumerate(predictions) if label_matches[i]]\n",
    "        \n",
    "    return sum(matched_predictions)/100 if label_value == 1 else (len(matched_predictions) - sum(matched_predictions))/100\n",
    "   \n",
    "f1 = f1_score(predictions, labels)\n",
    "accuracy_label_0 = calculate_accuracy_per_label(predictions, labels, label_value=0)\n",
    "accuracy_label_1 = calculate_accuracy_per_label(predictions, labels, label_value=1)\n",
    "\n",
    "print(\"F1 score:\", f1)\n",
    "print(\"Accuracy for label 0:\", accuracy_label_0)\n",
    "print(\"Accuracy for label 1:\", accuracy_label_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
