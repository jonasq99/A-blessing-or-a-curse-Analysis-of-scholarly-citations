{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-30T18:44:23.233580127Z",
     "start_time": "2023-12-30T18:44:23.193010132Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import bibtexparser\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from flair.data import Sentence\n",
    "from flair.nn import Classifier\n",
    "from difflib import SequenceMatcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cba3fc57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:44:23.253002409Z",
     "start_time": "2023-12-30T18:44:23.233954831Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO: write a function that normalizes the names of the authors from our annotations\n",
    "# to fit the format generated by anystyle.io check below for examples and look for the pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c6c72d0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:44:23.273140203Z",
     "start_time": "2023-12-30T18:44:23.240216377Z"
    }
   },
   "outputs": [],
   "source": [
    "def file_finder(file_str: str) -> str:\n",
    "    \"\"\"\n",
    "    This function takes a file name and returns the path to the file in the all_data_articles.\n",
    "    \"\"\"\n",
    "    title_doi = \"../data/titles_doi.csv\"\n",
    "    folder_path = \"../all_data_articles\"\n",
    "    \n",
    "    #extract the doi from the file name\n",
    "    doi = file_str.split(\"_\")[-1].split(\".\")[0]\n",
    "\n",
    "    # find the row in the csv file where the doi column ends with the doi\n",
    "    df = pd.read_csv(title_doi)\n",
    "    doi_row = df[df[\"DOI\"].str.endswith(doi)]\n",
    "\n",
    "    # extract the title from the row\n",
    "    title_json = doi_row[\"Title\"].values[0].replace(\" \", \"_\") + \".json\"\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".json\") and filename.startswith(title_json[:int(len(title_json)/3)]):\n",
    "            return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8ab1f76a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:44:23.274080178Z",
     "start_time": "2023-12-30T18:44:23.255581428Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_annotations(file_str: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function takes a file name and returns the annotations from the file.\n",
    "    And also replaces missing values with None.\n",
    "    \"\"\"\n",
    "    folder_path = \"../data/annotated\"\n",
    "\n",
    "    file_path = os.path.join(folder_path, file_str)\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # replace missing values with None\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "\n",
    "    # replace values marked with nan with None\n",
    "    df = df.replace(\"nan\", None)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c1b86509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:44:23.288635076Z",
     "start_time": "2023-12-30T18:44:23.273477Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_author_name(name):\n",
    "    # TODO: probably need to handle more cases\n",
    "    if name is None:\n",
    "        return None\n",
    "    if ' and ' in name:\n",
    "        # Handle multiple authors\n",
    "        authors = name.split(' and ')\n",
    "        formatted_authors = [format_author_name(author) for author in authors]\n",
    "        return ' and '.join(formatted_authors)\n",
    "    else:\n",
    "        parts = name.split()\n",
    "        # Handle case where there is a middle initial\n",
    "        if len(parts) == 3:\n",
    "            return f\"{parts[1]}, {parts[0]} {parts[2]}\"\n",
    "        # Handle case where there is no middle initial\n",
    "        elif len(parts) == 2:\n",
    "            return f\"{parts[1]}, {parts[0]}\"\n",
    "        else:\n",
    "            return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "accc3a5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:44:23.303774044Z",
     "start_time": "2023-12-30T18:44:23.279172837Z"
    }
   },
   "outputs": [],
   "source": [
    "def df_to_triplets(df: pd.DataFrame, format_author = True) -> set:\n",
    "    \"\"\"\n",
    "    This function takes a dataframe and returns a set of triplets.\n",
    "    \"\"\"\n",
    "    triplets = set()\n",
    "    for i in range(len(df)):\n",
    "        if format_author:\n",
    "            triplet = (df.iloc[i][\"Footnote\"], format_author_name(df.iloc[i][\"Authors\"]), df.iloc[i][\"Title\"])\n",
    "        else:\n",
    "            triplet = (df.iloc[i][\"Footnote\"], df.iloc[i][\"Authors\"], df.iloc[i][\"Title\"])\n",
    "        triplets.add(triplet)\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "952fb35a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:44:23.304747227Z",
     "start_time": "2023-12-30T18:44:23.292317764Z"
    }
   },
   "outputs": [],
   "source": [
    "def dict_to_triplets(extraction: dict) -> set:\n",
    "    \"\"\"\n",
    "    Converts a dictionary of footnotes to a set of triplets\n",
    "    \"\"\"\n",
    "    triplets = set()\n",
    "\n",
    "    for number, references in extraction.items():\n",
    "        for reference in references:\n",
    "            author = reference[0]\n",
    "            title = reference[1]\n",
    "\n",
    "            if author == \"\":\n",
    "                author = None\n",
    "\n",
    "            if title == \"\":\n",
    "                title = None\n",
    "            \n",
    "            triplets.add((int(number), author, title))\n",
    "\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "split_pattern = re.compile('; see |; | . See also | .See also |. See |, see')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c7e30437ee62674"
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cc101939",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:44:23.325118167Z",
     "start_time": "2023-12-30T18:44:23.304084386Z"
    }
   },
   "outputs": [],
   "source": [
    "def information_extraction(file_path: str) -> set:\n",
    "    \"\"\"\n",
    "    This function takes a file path and returns a set of triplets.\n",
    "    \"\"\"\n",
    "    path = \"../all_data_articles\"\n",
    "    file_path = os.path.join(path, file_path)\n",
    "    article = json.load(open(file_path, \"r\"))\n",
    "    extraction = {}\n",
    "\n",
    "    prev_footnote = None\n",
    "\n",
    "    for number, footnote in tqdm(article[\"footnotes\"].items()):\n",
    "\n",
    "        # If the footnote is ibid, use the previous footnote\n",
    "        if footnote.startswith(\"Ibid\"):\n",
    "            # do not fully replace ibid with previous footnote but rather prepend it since there might be other references after ibid\n",
    "            footnote = prev_footnote + \"; \" + footnote.lstrip(\"Ibid. \")\n",
    "\n",
    "        # Store the footnote for the next iteration\n",
    "        prev_footnote = footnote\n",
    "        \n",
    "        references = split_pattern.split(footnote)\n",
    "\n",
    "        author_title_list = []\n",
    "\n",
    "        for reference in references:\n",
    "\n",
    "            command = ['ruby', 'anystyle.rb', str(reference).strip()]\n",
    "            bibtex = subprocess.run(command, stdout=subprocess.PIPE, text=True).stdout\n",
    "            parsed_bibtex = bibtexparser.loads(bibtex).entries\n",
    "            \n",
    "            if parsed_bibtex:\n",
    "                parsed_bibtex = parsed_bibtex[0]\n",
    "            else:\n",
    "                #print(f\"No valid BibTeX entry found in: {bibtex}, set to empty dict\")\n",
    "                parsed_bibtex = {}\n",
    "                \n",
    "            if \"note\" in parsed_bibtex:\n",
    "                continue\n",
    "    \n",
    "            # Extract title and author\n",
    "            title = parsed_bibtex.get('title', parsed_bibtex.get('booktitle', None))\n",
    "            author = parsed_bibtex.get('author', parsed_bibtex.get(\"editor\", None))\n",
    "    \n",
    "            if author is not None or title is not None:\n",
    "                # Append author and title pair to the list\n",
    "                author_title_list.append([author, title])\n",
    "        \n",
    "        # Store the list in the extraction dictionary with the footnote number as the key\n",
    "        extraction[number] = author_title_list\n",
    "\n",
    "    return dict_to_triplets(extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "25ea4e66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:44:23.341026538Z",
     "start_time": "2023-12-30T18:44:23.324785839Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_scores(triplets, extractions):\n",
    "    TP = len(triplets & extractions)  # Intersection of triplets and extractions\n",
    "    FP = len(extractions - triplets)  # Elements in extractions but not in triplets\n",
    "    FN = len(triplets - extractions)  # Elements in triplets but not in extractions\n",
    "\n",
    "    recall = TP / (TP + FN) if TP + FN != 0 else 0\n",
    "    precision = TP / (TP + FP) if TP + FP != 0 else 0\n",
    "    f_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n",
    "\n",
    "    return recall, precision, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7180727a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:44:23.345005022Z",
     "start_time": "2023-12-30T18:44:23.340627953Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: add some how to check for similarity between the authors and titles from the two sets. if for footnote number x \n",
    "# the authors and titles are very similar then we can assume that the extraction is correct.\n",
    "\n",
    "def calculate_similarity(str1, str2):\n",
    "    return SequenceMatcher(None, str1, str2).ratio()\n",
    "\n",
    "def evaluate_extraction(set1, set2, threshold=0.95):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for triplet1 in set1:\n",
    "        footnote1, author1, title1 = triplet1\n",
    "        author1 = author1 if author1 is not None else \"\"\n",
    "        title1 = title1 if title1 is not None else \"\"\n",
    "        concat_str1 = str(author1) + \" \" + str(title1)\n",
    "        found_match = False\n",
    "\n",
    "        for triplet2 in set2:\n",
    "            footnote2, author2, title2 = triplet2\n",
    "            author2 = author2 if author2 is not None else \"\"\n",
    "            title2 = title2 if title2 is not None else \"\"\n",
    "            concat_str2 = str(author2) + \" \" + str(title2)\n",
    "\n",
    "            # Check for footnote number and similarity\n",
    "            if footnote1 == footnote2 and calculate_similarity(concat_str1, concat_str2) >= threshold:\n",
    "                found_match = True\n",
    "                break\n",
    "\n",
    "        if found_match:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_negatives += 1\n",
    "\n",
    "    false_positives = len(set2) - true_positives\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "475026d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T19:01:20.171540038Z",
     "start_time": "2023-12-30T18:44:23.344465637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels - https___doi.org_10.1093_ehr_ceab280.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:18<01:08,  1.16it/s]Entry type patent not standard. Not considered.\n",
      "100%|██████████| 100/100 [01:28<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.5294117647058824\n",
      "Precision: 0.2680851063829787\n",
      "F-Score: 0.3559322033898305\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_cead103.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 6/115 [00:07<03:19,  1.83s/it]Entry type webpage not standard. Not considered.\n",
      " 21%|██        | 24/115 [00:32<01:54,  1.26s/it]Entry type thesis not standard. Not considered.\n",
      "Entry type thesis not standard. Not considered.\n",
      " 26%|██▌       | 30/115 [00:45<01:59,  1.40s/it]Entry type thesis not standard. Not considered.\n",
      "100%|██████████| 115/115 [02:15<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.5685279187817259\n",
      "Precision: 0.5572139303482587\n",
      "F-Score: 0.5628140703517589\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_ceac260.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 12/175 [00:15<03:32,  1.31s/it]Entry type thesis not standard. Not considered.\n",
      " 99%|█████████▉| 174/175 [02:36<00:00,  1.61it/s]Entry type webpage not standard. Not considered.\n",
      "100%|██████████| 175/175 [02:37<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.47555555555555556\n",
      "Precision: 0.4798206278026906\n",
      "F-Score: 0.47767857142857145\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_cead065.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [02:37<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.3181818181818182\n",
      "Precision: 0.3452914798206278\n",
      "F-Score: 0.3311827956989247\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_cead080.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [01:54<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.7650273224043715\n",
      "Precision: 0.7692307692307693\n",
      "F-Score: 0.767123287671233\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_cew052.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [01:58<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.6042780748663101\n",
      "Precision: 0.5824742268041238\n",
      "F-Score: 0.5931758530183727\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_cead107.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 120/124 [02:03<00:03,  1.21it/s]Entry type thesis not standard. Not considered.\n",
      "100%|██████████| 124/124 [02:07<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.5463917525773195\n",
      "Precision: 0.5408163265306123\n",
      "F-Score: 0.5435897435897437\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_cead004.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [01:56<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.599009900990099\n",
      "Precision: 0.5576036866359447\n",
      "F-Score: 0.5775656324582339\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path_annotations = \"../data/annotated\"\n",
    "\n",
    "for filename in os.listdir(path_annotations):\n",
    "    print(filename)\n",
    "    title_json = file_finder(filename)\n",
    "    df = load_annotations(filename)\n",
    "    triplets = df_to_triplets(df, format_author=True)\n",
    "    extraction = information_extraction(title_json)\n",
    "    recall, precision, f_score = evaluate_extraction(triplets, extraction, threshold=0.9)\n",
    "    print(\"\\n\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"F-Score: {f_score}\")\n",
    "    print(\"-\"* 50)\n",
    "    print(\"\\n\")\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6420835b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T19:01:20.176415399Z",
     "start_time": "2023-12-30T19:01:20.174938727Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: implement a simple approach with a just regrex to extract the author and title, and a simple split with \";\"\n",
    "\n",
    "def extract_citations(file_path: str) -> set:\n",
    "    path = \"../all_data_articles\"\n",
    "    file_path = os.path.join(path, file_path)\n",
    "    article = json.load(open(file_path, \"r\"))\n",
    "    \n",
    "    citations = set()\n",
    "    prev_footnote = None\n",
    "\n",
    "    for footnote_number, footnote_text in tqdm(article[\"footnotes\"].items()):\n",
    "        # If the footnote is ibid, use the previous footnote\n",
    "        if footnote_text.startswith(\"Ibid\"):\n",
    "            footnote_text = prev_footnote\n",
    "        \n",
    "        prev_footnote = footnote_text\n",
    "\n",
    "        # Split the footnote into individual citations\n",
    "        individual_citations = split_pattern.split(footnote_text)\n",
    "        \n",
    "        for citation_text in individual_citations:\n",
    "            # Regular expression to extract authors and titles\n",
    "            # TODO: try a better pattern\n",
    "            pattern = re.compile(r'^(.+?),\\s+(.+?)[,|(]')\n",
    "\n",
    "            match = pattern.match(citation_text)\n",
    "            \n",
    "            if match:\n",
    "                author = match.group(1)\n",
    "                title = match.group(2)\n",
    "                citations.add((int(footnote_number), author, title))\n",
    "\n",
    "\n",
    "    return citations\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels - https___doi.org_10.1093_ehr_ceab280.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 107961.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.49193548387096775\n",
      "Precision: 0.25957446808510637\n",
      "F-Score: 0.3398328690807799\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_cead103.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:00<00:00, 74127.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.41899441340782123\n",
      "Precision: 0.373134328358209\n",
      "F-Score: 0.3947368421052632\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_ceac260.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:00<00:00, 124915.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.5677966101694916\n",
      "Precision: 0.600896860986547\n",
      "F-Score: 0.5838779956427016\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_cead065.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:00<00:00, 144736.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.32558139534883723\n",
      "Precision: 0.31390134529147984\n",
      "F-Score: 0.31963470319634707\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_cead080.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [00:00<00:00, 145574.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.4887640449438202\n",
      "Precision: 0.47802197802197804\n",
      "F-Score: 0.48333333333333334\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_cew052.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:00<00:00, 160901.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.5113636363636364\n",
      "Precision: 0.4639175257731959\n",
      "F-Score: 0.4864864864864865\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_cead107.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124/124 [00:00<00:00, 96761.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.4911242603550296\n",
      "Precision: 0.42346938775510207\n",
      "F-Score: 0.4547945205479452\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_cead004.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:00<00:00, 152698.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.5968586387434555\n",
      "Precision: 0.5253456221198156\n",
      "F-Score: 0.5588235294117647\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path_annotations = \"../data/annotated\"\n",
    "\n",
    "for filename in os.listdir(path_annotations):\n",
    "    print(filename)\n",
    "    title_json = file_finder(filename)\n",
    "    df = load_annotations(filename)\n",
    "    triplets = df_to_triplets(df, format_author=False)\n",
    "    extraction = extract_citations(title_json)\n",
    "    recall, precision, f_score = evaluate_extraction(triplets, extraction, threshold=0.9)\n",
    "    print(\"\\n\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"F-Score: {f_score}\")\n",
    "    print(\"-\"* 50)\n",
    "    print(\"\\n\")\n",
    "    #break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-30T19:01:21.213986017Z",
     "start_time": "2023-12-30T19:01:20.178799094Z"
    }
   },
   "id": "818aa90ace8233db"
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "37f1b11b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T19:01:21.795881176Z",
     "start_time": "2023-12-30T19:01:21.108163294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels - https___doi.org_10.1093_ehr_ceab280.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 129975.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.49193548387096775\n",
      "Precision: 0.25957446808510637\n",
      "F-Score: 0.3398328690807799\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_cead103.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:00<00:00, 100845.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.41899441340782123\n",
      "Precision: 0.373134328358209\n",
      "F-Score: 0.3947368421052632\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_ceac260.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:00<00:00, 127986.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.5677966101694916\n",
      "Precision: 0.600896860986547\n",
      "F-Score: 0.5838779956427016\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_cead065.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142/142 [00:00<00:00, 136135.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.32558139534883723\n",
      "Precision: 0.31390134529147984\n",
      "F-Score: 0.31963470319634707\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_cead080.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [00:00<00:00, 145955.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.4887640449438202\n",
      "Precision: 0.47802197802197804\n",
      "F-Score: 0.48333333333333334\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_cew052.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:00<00:00, 155034.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.5113636363636364\n",
      "Precision: 0.4639175257731959\n",
      "F-Score: 0.4864864864864865\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_cead107.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124/124 [00:00<00:00, 97909.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.4911242603550296\n",
      "Precision: 0.42346938775510207\n",
      "F-Score: 0.4547945205479452\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Labels - https___doi.org_10.1093_ehr_cead004.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:00<00:00, 151858.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recall: 0.5968586387434555\n",
      "Precision: 0.5253456221198156\n",
      "F-Score: 0.5588235294117647\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path_annotations = \"../data/annotated\"\n",
    "\n",
    "for filename in os.listdir(path_annotations):\n",
    "    print(filename)\n",
    "    title_json = file_finder(filename)\n",
    "    df = load_annotations(filename)\n",
    "    triplets = df_to_triplets(df, format_author=False)\n",
    "    extraction = extract_citations(title_json)\n",
    "    recall, precision, f_score = evaluate_extraction(triplets, extraction, threshold=0.9)\n",
    "    print(\"\\n\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"F-Score: {f_score}\")\n",
    "    print(\"-\"* 50)\n",
    "    print(\"\\n\")\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c87751be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T19:01:21.843109025Z",
     "start_time": "2023-12-30T19:01:21.799702550Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: write a function that extracts the author and title from the footnote using a tagger (i.e. flair)\n",
    "\n",
    "def tagger_information_extraction(file_path: str, tagger) -> set:\n",
    "    path = \"../all_data_articles\"\n",
    "    file_path = os.path.join(path, file_path)\n",
    "    article = json.load(open(file_path, \"r\"))\n",
    "    \n",
    "    citations = set()\n",
    "    prev_footnote = None\n",
    "\n",
    "    for footnote_number, footnote_text in tqdm(article[\"footnotes\"].items()):\n",
    "        # If the footnote is ibid, use the previous footnote\n",
    "        if footnote_text.startswith(\"Ibid\"):\n",
    "            footnote_text = prev_footnote\n",
    "        \n",
    "        prev_footnote = footnote_text\n",
    "\n",
    "        # Split the footnote into individual citations\n",
    "        individual_citations = split_pattern.split(footnote_text)\n",
    "\n",
    "        for citation_text in individual_citations:\n",
    "            \n",
    "            author = None\n",
    "\n",
    "            sentence = Sentence(citation_text)\n",
    "            tagger.predict(sentence)\n",
    "            for span in sentence.get_spans('ner'):\n",
    "                if span.tag == \"PERSON\" or span.tag == \"ORG\":\n",
    "                    if author is None:\n",
    "                        author = span.text\n",
    "                    else:\n",
    "                        author += (\"and \" + span.text)\n",
    "                if span.tag == \"WORK_OF_ART\":\n",
    "                    citations.add((int(footnote_number), author, span.text))\n",
    "                    author = None\n",
    "\n",
    "                \n",
    "    return citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ac22b8ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T19:01:30.889467233Z",
     "start_time": "2023-12-30T19:01:21.843065829Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ner-ontonotes-large'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[140], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m path_annotations \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../data/annotated\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# load the NER tagger\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m tagger \u001B[38;5;241m=\u001B[39m Classifier\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mner-ontonotes-large\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39mlistdir(path_annotations):\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28mprint\u001B[39m(filename)\n",
      "File \u001B[0;32m~/anaconda3/envs/data-science/lib/python3.11/site-packages/flair/nn/model.py:555\u001B[0m, in \u001B[0;36mClassifier.load\u001B[0;34m(cls, model_path)\u001B[0m\n\u001B[1;32m    551\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    552\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\u001B[38;5;28mcls\u001B[39m, model_path: Union[\u001B[38;5;28mstr\u001B[39m, Path, Dict[\u001B[38;5;28mstr\u001B[39m, Any]]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mClassifier\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    553\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cast\n\u001B[0;32m--> 555\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mClassifier\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mload(model_path\u001B[38;5;241m=\u001B[39mmodel_path))\n",
      "File \u001B[0;32m~/anaconda3/envs/data-science/lib/python3.11/site-packages/flair/nn/model.py:153\u001B[0m, in \u001B[0;36mModel.load\u001B[0;34m(cls, model_path)\u001B[0m\n\u001B[1;32m    150\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;66;03m# if the model cannot be fetched, load as a file\u001B[39;00m\n\u001B[0;32m--> 153\u001B[0m state \u001B[38;5;241m=\u001B[39m model_path \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model_path, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m load_torch_state(\u001B[38;5;28mstr\u001B[39m(model_path))\n\u001B[1;32m    155\u001B[0m \u001B[38;5;66;03m# try to get model class from state\u001B[39;00m\n\u001B[1;32m    156\u001B[0m cls_name \u001B[38;5;241m=\u001B[39m state\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__cls__\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m~/anaconda3/envs/data-science/lib/python3.11/site-packages/flair/file_utils.py:351\u001B[0m, in \u001B[0;36mload_torch_state\u001B[0;34m(model_file)\u001B[0m\n\u001B[1;32m    347\u001B[0m warnings\u001B[38;5;241m.\u001B[39mfilterwarnings(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    348\u001B[0m \u001B[38;5;66;03m# load_big_file is a workaround byhttps://github.com/highway11git\u001B[39;00m\n\u001B[1;32m    349\u001B[0m \u001B[38;5;66;03m# to load models on some Mac/Windows setups\u001B[39;00m\n\u001B[1;32m    350\u001B[0m \u001B[38;5;66;03m# see https://github.com/zalandoresearch/flair/issues/351\u001B[39;00m\n\u001B[0;32m--> 351\u001B[0m f \u001B[38;5;241m=\u001B[39m load_big_file(model_file)\n\u001B[1;32m    352\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mload(f, map_location\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/data-science/lib/python3.11/site-packages/flair/file_utils.py:49\u001B[0m, in \u001B[0;36mload_big_file\u001B[0;34m(f)\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_big_file\u001B[39m(f: \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m     45\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Workaround for loading a big pickle file.\u001B[39;00m\n\u001B[1;32m     46\u001B[0m \n\u001B[1;32m     47\u001B[0m \u001B[38;5;124;03m    Files over 2GB cause pickle errors on certain Mac and Windows distributions.\u001B[39;00m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 49\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(f, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f_in:\n\u001B[1;32m     50\u001B[0m         \u001B[38;5;66;03m# mmap seems to be much more memory efficient\u001B[39;00m\n\u001B[1;32m     51\u001B[0m         bf \u001B[38;5;241m=\u001B[39m mmap\u001B[38;5;241m.\u001B[39mmmap(f_in\u001B[38;5;241m.\u001B[39mfileno(), \u001B[38;5;241m0\u001B[39m, access\u001B[38;5;241m=\u001B[39mmmap\u001B[38;5;241m.\u001B[39mACCESS_READ)\n\u001B[1;32m     52\u001B[0m         f_in\u001B[38;5;241m.\u001B[39mclose()\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'ner-ontonotes-large'"
     ]
    }
   ],
   "source": [
    "path_annotations = \"../data/annotated\"\n",
    "# load the NER tagger\n",
    "tagger = Classifier.load('ner-ontonotes-large')\n",
    "\n",
    "for filename in os.listdir(path_annotations):\n",
    "    print(filename)\n",
    "    title_json = file_finder(filename)\n",
    "    df = load_annotations(filename)\n",
    "    triplets = df_to_triplets(df, format_author=False)\n",
    "    extraction = tagger_information_extraction(title_json, tagger=tagger)\n",
    "    recall, precision, f_score = evaluate_extraction(triplets, extraction, threshold=0.90)\n",
    "    print(\"\\n\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"F-Score: {f_score}\")\n",
    "    print(\"-\"* 50)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b979b4e6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T19:01:30.892032867Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_triplets(triplets):\n",
    "    return list(filter(lambda triplet: 10 < triplet[0] < 20, triplets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f60ff8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T19:01:30.893397935Z"
    }
   },
   "outputs": [],
   "source": [
    "extract_triplets(extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4641d7e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T19:01:30.935013971Z"
    }
   },
   "outputs": [],
   "source": [
    "extract_triplets(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c6fb2b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T19:01:30.935157563Z"
    }
   },
   "outputs": [],
   "source": [
    "#(113, 'M. Young and P. Willmott', 'Family and Kinship in East London'),\n",
    "#('113', 'Young, M. and Willmott, P.', 'Family and Kinship in East London'),"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
