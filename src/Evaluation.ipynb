{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation True for opinionated, False for others\n",
    "\n",
    "# annotated labels dict\n",
    "# labels =  {triplet: label}\n",
    "\n",
    "# prediction dict\n",
    "# predictions = {tiplet: pred}\n",
    "\n",
    "# maybe develop a function where we get the labels (annotated and prediction)\n",
    "# just through the doi\n",
    "\n",
    "# evaluation\n",
    "# take the predicted labels {tiplet: label}\n",
    "# take the actual labels {tiplet: pred}\n",
    "# give precicion, recall and f-score\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible fumction\n",
    "\n",
    "def get_files (doi):\n",
    "    ann_file = #read annotated file and put it in dict (triplet, label)\n",
    "    prediction = #get  dict (triplet, prediction)\n",
    "    return ann_file, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just possible if triplets are the same, \n",
    "# or combine it with functions of information extraction\n",
    "\n",
    "def evaluation_A (ann_file, prediction):\n",
    "\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for triplet in ann_file:\n",
    "        if ann_file[triplet] == True and prediction[triplet] == True:\n",
    "            true_positives += 1\n",
    "        elif ann_file[triplet] == True and prediction[triplet] == False:\n",
    "            false_negatives += 1\n",
    "        elif ann_file[triplet] == False and prediction[triplet] == True:\n",
    "            false_positives += 1\n",
    "\n",
    "    prec = true_positives/(true_positives+false_positives)\n",
    "    rec = true_positives/(true_positives+false_negatives)\n",
    "    f_measure = 2*(prec*rec)/(prec+rec)\n",
    "    \n",
    "    return prec, rec, f_measure\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just possible if the length of annotated and predicted labels are the same\n",
    "# no guarante that the labels are for the same triplet?\n",
    "\n",
    "def evaluation_B (ann_file, prediction):\n",
    "\n",
    "    y_labels = np.array(list(ann_file.values()), dtype = float)\n",
    "    y_pred = np.array(list(prediction.values()), dtype = float)\n",
    "\n",
    "    precision = precision_score(y_labels, y_pred)\n",
    "    recall = recall_score(y_labels, y_pred)\n",
    "    fscore = f1_score(y_labels, y_pred)\n",
    "    \n",
    "    return precision, recall, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {1: True, 2: False, 3: False, 4: True, 5: True}\n",
    "b = {1: True, 2: True, 3: False, 4: False, 5: False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5, 0.3333333333333333, 0.4)\n"
     ]
    }
   ],
   "source": [
    "print(evaluation_A(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5, 0.3333333333333333, 0.4)\n"
     ]
    }
   ],
   "source": [
    "print(evaluation_B(a,b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
