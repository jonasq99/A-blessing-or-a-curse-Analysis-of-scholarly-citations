{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_doi_df = pd.read_csv('data/titles_doi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sleep():\n",
    "    return random.uniform(1, 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_footnotes(driver):\n",
    "    footnotes = {}\n",
    "    citations = driver.find_elements(By.CSS_SELECTOR, 'a.link-ref')\n",
    "    for c in citations:\n",
    "        label = c.text\n",
    "        if label.isdigit():\n",
    "            rev_id = \"fn\" + label.zfill(4)\n",
    "            try:\n",
    "                # Find the element with reveal-id=\"fn0001\"\n",
    "                element = driver.find_element(By.CSS_SELECTOR, f\"[reveal-id={rev_id}]\")\n",
    "\n",
    "                if element:\n",
    "                    # Scroll to the element using JavaScript\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView({ behavior: 'smooth' });\", element)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "            \n",
    "            c.click()\n",
    "            time.sleep(random_sleep())\n",
    "            \n",
    "            # Wait for the citation to be visible\n",
    "            wait = WebDriverWait(driver, 10)\n",
    "            wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, 'p.footnote-compatibility')))\n",
    "            \n",
    "            ref = driver.find_element(By.CSS_SELECTOR, \"div#revealContent\").text.split('\\n')[-1]\n",
    "            footnotes[label] = ref\n",
    "\n",
    "            # Use JavaScript to click the \"Close\" button forcefully\n",
    "            close_button = driver.find_element(By.CSS_SELECTOR, \"a.close-reveal-modal\")\n",
    "            driver.execute_script(\"arguments[0].click();\", close_button)\n",
    "\n",
    "            time.sleep(random_sleep())\n",
    "    return footnotes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article(soup, url):\n",
    "    paragraphs = soup.find_all('p', class_='chapter-para')\n",
    "\n",
    "    # Initialize a variable to store the extracted text and citations\n",
    "    result = \"\"\n",
    "\n",
    "    # Iterate through paragraphs\n",
    "    for paragraph in paragraphs:\n",
    "        # Extract the text within the paragraph\n",
    "        text = ''\n",
    "        citations = []\n",
    "        for element in paragraph.contents:\n",
    "            if element.name == 'a':\n",
    "                # Extract the citation number from the sup tag\n",
    "                try:\n",
    "                    citation_number = int(element.find('sup').get_text())\n",
    "                    # Add the citation to the list\n",
    "                    citations.append(citation_number)\n",
    "\n",
    "                except:\n",
    "                    try:\n",
    "                        citation_number = int(element.get_text())\n",
    "                        citations.append(citation_number)\n",
    "\n",
    "                    except:\n",
    "                        citation_number = \"\"\n",
    "                        print(element, url)\n",
    "                \n",
    "                if citation_number != \"\":\n",
    "                    text += f'[CITATION-{citation_number}] '\n",
    "\n",
    "            elif element and hasattr(element, 'strip'):\n",
    "                # Add the text content (if not None and has a strip method)\n",
    "                try:\n",
    "                    text += element.strip()  \n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "        result += text\n",
    "\n",
    "    return result, citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all(url):\n",
    "    firefox_options = Options()\n",
    "    firefox_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/100.0')\n",
    "\n",
    "    # Create a Firefox WebDriver instance\n",
    "    driver = webdriver.Firefox(options=firefox_options)\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(random_sleep())\n",
    "    accept_button = driver.find_element(By.XPATH, \"//button[@id='accept-button']\")\n",
    "    accept_button.click()\n",
    "\n",
    "    publication_data = {}\n",
    "\n",
    "    # Get the page source after it has loaded\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Extract the title\n",
    "    title_element = soup.find('h1', class_='wi-article-title')\n",
    "    title = title_element.get_text(strip=True)\n",
    "    publication_data['title'] = title\n",
    "\n",
    "    # Extract the author\n",
    "    author_element = soup.find('button', class_='linked-name')\n",
    "    author = author_element.get_text(strip=True)\n",
    "    publication_data['author'] = author\n",
    "\n",
    "    # Extract the publication date\n",
    "    date_element = soup.find('div', class_='citation-date')\n",
    "    date = date_element.get_text(strip=True)\n",
    "    publication_data['date'] = date\n",
    "\n",
    "    article, footnotes_numbers = scrape_article(soup, url)\n",
    "    publication_data['article'] = article\n",
    "\n",
    "    footnotes = scrape_footnotes(driver)\n",
    "    publication_data['footnotes'] = footnotes\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    return publication_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data):\n",
    "    path = \"all_data_articles/\" + data['title'].replace(' ', '_') + \".json\"\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    \n",
    "    print(f\"Saved {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://doi.org/10.1093/ehr/cead151\n",
      "Saved all_data_articles/Catholic_Intellectuals_and_Transnational_Anti-Communism:_Pax_Romana_from_the_Spanish_Civil_War_to_the_post-1945_World_Order*.json\n",
      "Scraping https://doi.org/10.1093/ehr/cead103\n",
      "Saved all_data_articles/Au_Nom_de_la_Patrie:_Southern_Identities_and_Patriotic_Mobilisation_in_First_World_War_France.json\n"
     ]
    }
   ],
   "source": [
    "for index, row in title_doi_df[:2].iterrows():\n",
    "    url = row['DOI']\n",
    "    print(f\"Scraping {url}\")\n",
    "    data = scrape_all(url)\n",
    "    save_data(data)\n",
    "    time.sleep(random_sleep())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
